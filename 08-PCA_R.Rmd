# (PART) Chapters 8-10 {-}
# Chapter 8: Principle Component Analysis

```{r, include=FALSE, cache=TRUE}
require(Matrix)
```

***

## Example 8.3, Exercise 8.11

A census provided information, by tract, on five socioeconomic variables for the
Madison, Wisconsin, area.

* total population (thousands)
* professional degree (percent)
* employed age over 16 (percent)
* government employment (percent)
* median home value

```{r, cache=TRUE}
X <-  read.table("dataset/T8-5.dat",header=FALSE)
X

# The function princomp() uses the spectral decomposition approach. 
# The functions prcomp() use the singular value decomposition (SVD).

X.pc <- prcomp(X)
summary(X.pc)

# The rotation shows the estimated eigenvector loadings
X.pc$rotation

screeplot(X.pc, col = "red", pch = 16,
          type = "lines", cex = 2, lwd = 2, main = "")

# Another useful graphical method to help interpret the first two principal components is called the biplot.
# Biplots are a graphical method for simultaneously displaying the variables and sample units described by a multivariate data matrix.

biplot(X.pc, col = c(2, 3), cex = c(.75, 1.5),
       xlim = c( -.45, .45),
       xlab = "First principal component",
       ylab = "Second principal component",
       main = "Biplot")

# use the factoextra package to create a ggplot2-based elegant visualization.

# (1) Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component.
library(factoextra)
fviz_eig(X.pc)


# (2) Graph of individuals. Individuals with a similar profile are grouped together.
fviz_pca_ind(X.pc,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
)

## The quality of representation shows the importance of a principal component for a given observation
## Some details about computing cos2: the quality of representation
# a. Coordinates of individuals on the principal components
ind.coord <- X.pc$x
ind.coord
# b. Calculate the square distance between each individual and the PCA center of gravity
n <- nrow(X)
d2 <- rowSums(ind.coord^2)
cos2 <- round(apply(ind.coord, 2, function(x)  x^2/d2), 2)
cos2
rowSums(cos2[,1:2])

# (3) Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.
fviz_pca_var(X.pc,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
)

# (4) Biplot of individuals and variables
fviz_pca_biplot(X.pc, 
                repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
)
```

## Example 8.10, Example 8.11

```{r, cache=TRUE}
X <-  read.table("dataset/T8-2.dat",header=FALSE)
X
n <- nrow(X)
p <- ncol(X)

X.pc <- prcomp(X)
# values of the principal components
Y <- X.pc$x
Y

library(factoextra)
fviz_eig(X.pc)

# covariance matrix of first two sample PCs: y1 and y2
Y12 <- Y[, 1:2]
S <- diag(as.vector(X.pc$sdev^2)[1:2])
S

## Two-part procedure
# (i) ellipse format chart for the first two PCs (y1, y2)
library(car)
alpha <- 0.05
radius <- sqrt(qchisq(1-alpha, df = 2))
par(pty = "s")
plot(0, xlim=c(-5000, 5000), ylim=c(-4000, 4000), 
     xlab = expression(hat(y)[1]), ylab = expression(hat(y)[2]), type="n")
ellipse(center = colMeans(Y12), shape = S, radius = radius)
points(Y12[, 1], Y12[, 2])

# (ii) T^2 chart for the last p-2 PCs
T2 <- Y[, 3]^2/X.pc$sdev[3]^2 +
  Y[, 4]^2/X.pc$sdev[4]^2 +
  Y[, 5]^2/X.pc$sdev[5]^2

UCL <- qchisq(1-alpha, df = p - 2)

plot(1:n, T2, ylim = range(T2, UCL*1.05), type = 'p',
     xlab = "Period", ylab = expression(T^2),
     main = expression(paste(T^2,"-chart")))
abline(h = UCL, lty = 1, col = "red")
lines(1:n, T2)

```

## Homework
### Exercise 8.1, 8.2

```{r, cache=TRUE}
Sigma <- matrix(c(5, 2,
                  2, 2), nrow = 2, byrow = T)

eigen(Sigma)

V <- diag(diag(Sigma))
rho <- sqrt(solve(V)) %*% Sigma %*% sqrt(solve(V))

eigen(rho)

# the correlation coefficients between the PC Y_i and the standardized variable Z_k
# Y_i, Z_k
i <- 1
k <- 1
ed <- eigen(rho)
ed$vector[k, i] * sqrt(ed$values[i])
```

### Exercise 8.22

```{r, cache=TRUE}
library(factoextra)
mydata <-  read.table("dataset/T1-10.dat",header=FALSE)
X <- mydata[, 3:9]
n <- nrow(X)
p <- ncol(X)

# use covariance matrix
X.pc <- prcomp(X)
# proportion of variance
round(X.pc$sdev^2/sum(X.pc$sdev^2),4)
fviz_eig(X.pc)

# use correlation matrix
Z.pc <- prcomp(X, scale = T)
# proportion of variance
round(Z.pc$sdev^2/sum(Z.pc$sdev^2),4)
fviz_eig(Z.pc)

pc1 <- X.pc$x[, 1]
pc2 <- X.pc$x[, 2]
plot(pc1,pc2,type='n')
text(pc1,pc2,mydata[, 1])

qqnorm(pc1, pch = 1)
qqline(pc1, col = "red", lwd = 2)

```


## Reference
[1] Johnson, R. A., and Wichern, D. W. (2007). Applied multivariate statistical analysis (6th ed.). Upper Saddle River, N.J.: Pearson Prentice Hall.

[2] Zelterman, D. (2015). Applied Multivariate Statistics with R (1st ed.).

[3] Principal Component Analysis in R: prcomp vs princomp, http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/

