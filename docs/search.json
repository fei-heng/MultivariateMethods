[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"collection lecture notes STA 6707 Multivariate Methods.","code":""},{"path":"chapter-8-principle-component-analysis.html","id":"chapter-8-principle-component-analysis","chapter":"1 Chapter 8: Principle Component Analysis","heading":"1 Chapter 8: Principle Component Analysis","text":"","code":""},{"path":"chapter-8-principle-component-analysis.html","id":"example-8.3-exercise-8.11","chapter":"1 Chapter 8: Principle Component Analysis","heading":"1.1 Example 8.3, Exercise 8.11","text":"census provided information, tract, five socioeconomic variables \nMadison, Wisconsin, area.total population (thousands)professional degree (percent)employed age 16 (percent)government employment (percent)median home value","code":"\nX <-  read.table(\"dataset/T8-5.dat\",header=FALSE)\nX\n#>      V1    V2    V3   V4   V5\n#> 1  2.67  5.71 69.02 30.3 1.48\n#> 2  2.25  4.37 72.98 43.3 1.44\n#> 3  3.12 10.27 64.94 32.0 2.11\n#> 4  5.14  7.44 71.29 24.5 1.85\n#> 5  5.54  9.25 74.94 31.0 2.23\n#> 6  5.04  4.84 53.61 48.2 1.60\n#> 7  3.14  4.82 67.00 37.6 1.52\n#> 8  2.43  2.40 67.20 36.8 1.40\n#> 9  5.38  4.30 83.03 19.7 2.07\n#> 10 7.34  2.73 72.60 24.5 1.42\n#> 11 4.94  4.66 64.32 27.7 1.42\n#> 12 4.82  4.26 82.64 20.3 1.46\n#> 13 5.02  4.17 84.25 20.6 1.42\n#> 14 3.37  1.00 69.93 16.4 1.17\n#> 15 3.63  6.40 70.31 29.0 2.00\n#> 16 7.43  6.00 70.53 37.7 1.44\n#> 17 2.20 10.59 69.85 41.7 2.01\n#> 18 7.16  4.71 79.44 33.0 1.55\n#> 19 6.33  2.88 66.24 38.1 1.73\n#> 20 2.57  1.85 67.25 33.4 1.18\n#> 21 6.38  1.56 63.00 18.2 0.93\n#> 22 5.34  3.41 72.57 20.1 1.66\n#> 23 4.87  5.20 75.13 16.5 3.64\n#> 24 2.04  4.83 67.78 17.4 1.49\n#> 25 5.48  1.34 77.43 21.6 1.32\n#> 26 7.77  5.32 58.57 31.2 3.21\n#> 27 6.29  2.60 64.32 27.4 1.78\n#> 28 6.38  3.71 78.61 34.1 1.30\n#> 29 5.76  4.06 83.77 31.4 1.52\n#> 30 6.03  3.10 76.04 25.0 1.08\n#> 31 5.09  1.85 74.65 24.1 0.97\n#> 32 4.36  1.67 65.43 23.7 1.07\n#> 33 3.07  2.00 68.03 26.2 1.19\n#> 34 1.82  1.13 49.50 21.9 1.62\n#> 35 3.31  0.94 74.75 26.5 1.12\n#> 36 3.45  0.72 65.99 22.0 1.20\n#> 37 1.74  0.97 60.24 22.0 1.17\n#> 38 1.81  1.54 70.05 24.4 1.00\n#> 39 5.59  1.66 77.96 17.1 1.30\n#> 40 3.72  1.69 82.40 16.3 1.52\n#> 41 3.39  1.24 67.17 27.7 1.03\n#> 42 2.25  2.80 70.81 23.4 1.14\n#> 43 3.31  1.30 71.30 19.2 1.21\n#> 44 5.27  1.20 73.08 30.3 1.35\n#> 45 3.26  1.02 74.36 16.5 1.23\n#> 46 6.76  1.53 78.37 22.6 1.33\n#> 47 2.92  4.42 58.50 68.5 2.25\n#> 48 1.64 16.70 64.61 49.4 3.13\n#> 49 1.36 14.26 66.42 22.5 2.80\n#> 50 3.58  3.38 65.57 26.1 1.31\n#> 51 3.38  2.17 66.10 22.6 1.44\n#> 52 7.25  1.16 78.52 23.6 1.50\n#> 53 5.44  2.93 73.59 22.3 1.65\n#> 54 5.83  4.47 77.33 26.2 2.16\n#> 55 3.74  2.26 79.70 20.2 1.58\n#> 56 9.21  2.36 74.58 21.8 1.72\n#> 57 2.14  6.30 86.54 17.4 2.80\n#> 58 6.62  4.79 78.84 20.0 2.33\n#> 59 4.24  5.82 71.39 27.1 1.69\n#> 60 4.72  4.71 78.01 20.6 1.55\n#> 61 6.48  4.93 74.23 20.9 1.98\n\n# The function princomp() uses the spectral decomposition approach. \n# The functions prcomp() use the singular value decomposition (SVD).\n\nX.pc <- prcomp(X)\nsummary(X.pc)\n#> Importance of components:\n#>                           PC1    PC2     PC3     PC4\n#> Standard deviation     10.345 6.2986 2.89324 1.69348\n#> Proportion of Variance  0.677 0.2510 0.05295 0.01814\n#> Cumulative Proportion   0.677 0.9279 0.98088 0.99902\n#>                            PC5\n#> Standard deviation     0.39331\n#> Proportion of Variance 0.00098\n#> Cumulative Proportion  1.00000\n\n# The rotation shows the estimated eigenvector loadings\nX.pc$rotation\n#>             PC1         PC2         PC3         PC4\n#> V1  0.038887287 -0.07114494  0.18789258  0.97713524\n#> V2 -0.105321969 -0.12975236 -0.96099580  0.17135181\n#> V3  0.492363944 -0.86438807  0.04579737 -0.09104368\n#> V4 -0.863069865 -0.48033178  0.15318538 -0.02968577\n#> V5 -0.009122262 -0.01474342 -0.12498114  0.08170118\n#>             PC5\n#> V1 -0.057699864\n#> V2 -0.138554092\n#> V3  0.004966048\n#> V4  0.006691800\n#> V5  0.988637470\n\nscreeplot(X.pc, col = \"red\", pch = 16,\n          type = \"lines\", cex = 2, lwd = 2, main = \"\")\n\n# Another useful graphical method to help interpret the first two principal components is called the biplot.\n# Biplots are a graphical method for simultaneously displaying the variables and sample units described by a multivariate data matrix.\n\nbiplot(X.pc, col = c(2, 3), cex = c(.75, 1.5),\n       xlim = c( -.45, .45),\n       xlab = \"First principal component\",\n       ylab = \"Second principal component\",\n       main = \"Biplot\")\n\n# use the factoextra package to create a ggplot2-based elegant visualization.\n\n# (1) Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component.\nlibrary(factoextra)\n#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_eig(X.pc)\n\n\n# (2) Graph of individuals. Individuals with a similar profile are grouped together.\nfviz_pca_ind(X.pc,\n             col.ind = \"cos2\", # Color by the quality of representation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n)\n\n## The quality of representation shows the importance of a principal component for a given observation\n## Some details about computing cos2: the quality of representation\n# a. Coordinates of individuals on the principal components\nind.coord <- X.pc$x\nind.coord\n#>                PC1          PC2          PC3         PC4\n#>  [1,]  -4.35590788   0.35186425  -1.58945113 -1.35312421\n#>  [2,] -13.50089124  -9.11108690   1.79713519 -2.74284848\n#>  [3,]  -8.30048747   2.42902922  -5.89221657  0.24041504\n#>  [4,]   1.67803285   0.76997301  -3.61863740  1.35257624\n#>  [5,]  -2.31333719  -5.77611226  -4.16751019  1.55935662\n#>  [6,] -27.20948854   5.01664733   1.71320375  1.69502211\n#>  [7,] -11.53924438  -1.32704212   0.37488801 -1.07590362\n#>  [8,] -10.52195183  -0.74937149   2.46870301 -2.18860529\n#>  [9,]  11.93915794  -6.68524631  -0.78114090  0.14065720\n#> [10,]   2.90857069   0.09857903   1.43475296  2.54080801\n#> [11,]  -4.22662722   5.63897636  -0.75992016  1.18523960\n#> [12,]  11.21729466  -6.58230929  -0.69763216 -0.44553475\n#> [13,]  11.76870098  -8.12003515  -0.44887539 -0.42428346\n#> [14,]   8.61492991   6.80778546   1.01950682 -1.17174146\n#> [15,]  -2.63885151  -0.30425990  -2.27721394 -0.33321186\n#> [16,]  -9.84423032  -4.88350530   0.23395391  2.98731285\n#> [17,] -14.32332530  -6.44892755  -4.64933497 -1.34686336\n#> [18,]  -1.27647689 -10.14267573   1.09724286  1.83975359\n#> [19,] -12.01851652  -0.88860200   2.85413785  1.78024283\n#> [20,]  -7.49751789   0.90518422   2.53251108 -2.06764467\n#> [21,]   3.70958180  11.65012840   1.03525923  2.42328264\n#> [22,]   6.53972433   2.28849038  -0.29989444  0.85601375\n#> [23,]  10.68236211   1.57684077  -2.79007521  0.73904522\n#> [24,]   6.23525521   7.77884139  -2.89627716 -1.62285138\n#> [25,]   7.86457056  -2.36929347   2.21051873  0.12333510\n#> [26,] -10.05425476   8.61467906  -0.81334367  4.62947057\n#> [27,]  -3.70152919   5.94901244   1.38243735  2.18970560\n#> [28,]  -2.55724536  -9.76466732   2.07341983  0.92872290\n#> [29,]   2.25066152 -12.93256096   1.41579595  0.01121266\n#> [30,]   4.08395782  -3.06487755   1.10967447  0.96834948\n#> [31,]   4.27243668  -1.20039106   1.94652293 -0.02006664\n#> [32,]   0.06772707   7.03521654   1.48631653  0.09524844\n#> [33,]  -0.89581686   3.63416759   1.41384538 -1.40983377\n#> [34,]  -6.26902189  21.91218124   0.45398163 -0.93050925\n#> [35,]   2.27556068  -2.19712502   2.84005779 -1.98339256\n#> [36,]   1.87415219   7.55381324   1.97726412 -0.94662635\n#> [37,]  -1.04949457  12.61370671   1.15613339 -2.05363955\n#> [38,]   1.65346742   2.90483086   1.45968221 -2.86584307\n#> [39,]  11.98009486  -0.71497795   1.26110626  0.36935136\n#> [40,]  14.77876088  -4.04269064   0.93421343 -1.81526203\n#> [41,]  -2.51990646   3.73524802   2.41471713 -1.20668116\n#> [42,]   2.77386149   2.53137189   0.19562350 -2.24806954\n#> [43,]   6.83857818   4.24339807   1.20659676 -1.38354598\n#> [44,]  -1.77961535  -2.75542840   3.43534553  0.03437226\n#> [45,]  10.70286382   2.93485943   1.19032076 -1.67718929\n#> [46,]   7.49399613  -3.77801594   2.46341714  1.29217524\n#> [47,] -42.36628241  -8.76520545   4.97086441 -1.44319135\n#> [48,] -24.22446138  -6.38754714  -9.82666882 -0.50710578\n#> [49,]   0.13240425   5.31021704 -11.53099879 -0.59200525\n#> [50,]  -2.14733165   5.59148404   0.04051858 -0.43828914\n#> [51,]   1.25284200   6.98383229   0.63762120 -0.77478367\n#> [52,]   6.76125397  -4.37736497   3.04986115  1.67811821\n#> [53,]   5.19771634   0.40539870   0.56514378  0.71248814\n#> [54,]   3.52150287  -4.93579093  -0.13654685  0.94284241\n#> [55,]  10.02360265  -3.65840314   0.85647490 -1.56310331\n#> [56,]   6.32269161  -0.40546923   1.78126446  4.22904619\n#> [57,]  15.30911808  -8.65424313  -3.59471831 -2.87420152\n#> [58,]   9.61147274  -3.36319151  -1.19747251  1.83007687\n#> [59,]  -0.37962981  -0.28874022  -1.80806881  0.09620502\n#> [60,]   8.62662402  -2.77689303  -1.32620405 -0.04616036\n#> [61,]   6.54791557   0.18629405  -1.38783253  2.08166593\n#>                PC5\n#>  [1,] -0.281419035\n#>  [2,] -0.004409154\n#>  [3,] -0.325234441\n#>  [4,] -0.325379930\n#>  [5,] -0.161937766\n#>  [6,] -0.135732724\n#>  [7,] -0.106860604\n#>  [8,]  0.146410475\n#>  [9,]  0.339512953\n#> [10,] -0.218338448\n#> [11,] -0.366973284\n#> [12,] -0.223623495\n#> [13,] -0.252236221\n#> [14,] -0.064193706\n#> [15,]  0.079385117\n#> [16,] -0.578778520\n#> [17,] -0.326057864\n#> [18,] -0.262918633\n#> [19,]  0.185057340\n#> [20,] -0.025466818\n#> [21,] -0.575103051\n#> [22,]  0.010524588\n#> [23,]  1.735756491\n#> [24,] -0.205736270\n#> [25,] -0.012710470\n#> [26,]  1.142817998\n#> [27,]  0.194455278\n#> [28,] -0.323278854\n#> [29,] -0.110941682\n#> [30,] -0.509724275\n#> [31,] -0.403969336\n#> [32,] -0.286508631\n#> [33,] -0.109520935\n#> [34,]  0.387464462\n#> [35,] -0.010326808\n#> [36,]  0.017552430\n#> [37,]  0.023366776\n#> [38,] -0.162939168\n#> [39,] -0.110648611\n#> [40,]  0.227289567\n#> [41,] -0.175098878\n#> [42,] -0.227413622\n#> [43,] -0.037211916\n#> [44,]  0.085079555\n#> [45,]  0.021369217\n#> [46,] -0.091645314\n#> [47,]  0.847525584\n#> [48,] -0.007532702\n#> [49,] -0.150576006\n#> [50,] -0.224401674\n#> [51,]  0.062522325\n#> [52,]  0.106851844\n#> [53,]  0.081161521\n#> [54,]  0.394161422\n#> [55,]  0.219167679\n#> [56,]  0.013383974\n#> [57,]  0.973097369\n#> [58,]  0.438319159\n#> [59,] -0.189279132\n#> [60,] -0.212210738\n#> [61,]  0.064105593\n# b. Calculate the square distance between each individual and the PCA center of gravity\nn <- nrow(X)\nd2 <- rowSums(ind.coord^2)\ncos2 <- round(apply(ind.coord, 2, function(x)  x^2/d2), 2)\ncos2\n#>        PC1  PC2  PC3  PC4  PC5\n#>  [1,] 0.81 0.01 0.11 0.08 0.00\n#>  [2,] 0.66 0.30 0.01 0.03 0.00\n#>  [3,] 0.63 0.05 0.32 0.00 0.00\n#>  [4,] 0.15 0.03 0.71 0.10 0.01\n#>  [5,] 0.09 0.57 0.30 0.04 0.00\n#>  [6,] 0.96 0.03 0.00 0.00 0.00\n#>  [7,] 0.98 0.01 0.00 0.01 0.00\n#>  [8,] 0.91 0.00 0.05 0.04 0.00\n#>  [9,] 0.76 0.24 0.00 0.00 0.00\n#> [10,] 0.50 0.00 0.12 0.38 0.00\n#> [11,] 0.35 0.61 0.01 0.03 0.00\n#> [12,] 0.74 0.26 0.00 0.00 0.00\n#> [13,] 0.68 0.32 0.00 0.00 0.00\n#> [14,] 0.60 0.38 0.01 0.01 0.00\n#> [15,] 0.56 0.01 0.42 0.01 0.00\n#> [16,] 0.75 0.18 0.00 0.07 0.00\n#> [17,] 0.76 0.15 0.08 0.01 0.00\n#> [18,] 0.01 0.94 0.01 0.03 0.00\n#> [19,] 0.92 0.01 0.05 0.02 0.00\n#> [20,] 0.83 0.01 0.09 0.06 0.00\n#> [21,] 0.09 0.87 0.01 0.04 0.00\n#> [22,] 0.88 0.11 0.00 0.02 0.00\n#> [23,] 0.89 0.02 0.06 0.00 0.02\n#> [24,] 0.35 0.55 0.08 0.02 0.00\n#> [25,] 0.85 0.08 0.07 0.00 0.00\n#> [26,] 0.51 0.37 0.00 0.11 0.01\n#> [27,] 0.25 0.63 0.03 0.09 0.00\n#> [28,] 0.06 0.89 0.04 0.01 0.00\n#> [29,] 0.03 0.96 0.01 0.00 0.00\n#> [30,] 0.59 0.33 0.04 0.03 0.01\n#> [31,] 0.77 0.06 0.16 0.00 0.01\n#> [32,] 0.00 0.96 0.04 0.00 0.00\n#> [33,] 0.04 0.73 0.11 0.11 0.00\n#> [34,] 0.08 0.92 0.00 0.00 0.00\n#> [35,] 0.24 0.22 0.37 0.18 0.00\n#> [36,] 0.05 0.87 0.06 0.01 0.00\n#> [37,] 0.01 0.96 0.01 0.03 0.00\n#> [38,] 0.13 0.39 0.10 0.38 0.00\n#> [39,] 0.98 0.00 0.01 0.00 0.00\n#> [40,] 0.91 0.07 0.00 0.01 0.00\n#> [41,] 0.23 0.51 0.21 0.05 0.00\n#> [42,] 0.40 0.33 0.00 0.26 0.00\n#> [43,] 0.69 0.26 0.02 0.03 0.00\n#> [44,] 0.14 0.34 0.52 0.00 0.00\n#> [45,] 0.90 0.07 0.01 0.02 0.00\n#> [46,] 0.72 0.18 0.08 0.02 0.00\n#> [47,] 0.95 0.04 0.01 0.00 0.00\n#> [48,] 0.81 0.06 0.13 0.00 0.00\n#> [49,] 0.00 0.17 0.82 0.00 0.00\n#> [50,] 0.13 0.87 0.00 0.01 0.00\n#> [51,] 0.03 0.95 0.01 0.01 0.00\n#> [52,] 0.59 0.25 0.12 0.04 0.00\n#> [53,] 0.96 0.01 0.01 0.02 0.00\n#> [54,] 0.33 0.64 0.00 0.02 0.00\n#> [55,] 0.86 0.11 0.01 0.02 0.00\n#> [56,] 0.65 0.00 0.05 0.29 0.00\n#> [57,] 0.71 0.23 0.04 0.02 0.00\n#> [58,] 0.85 0.10 0.01 0.03 0.00\n#> [59,] 0.04 0.02 0.92 0.00 0.01\n#> [60,] 0.89 0.09 0.02 0.00 0.00\n#> [61,] 0.87 0.00 0.04 0.09 0.00\nrowSums(cos2[,1:2])\n#>  [1] 0.82 0.96 0.68 0.18 0.66 0.99 0.99 0.91 1.00 0.50 0.96\n#> [12] 1.00 1.00 0.98 0.57 0.93 0.91 0.95 0.93 0.84 0.96 0.99\n#> [23] 0.91 0.90 0.93 0.88 0.88 0.95 0.99 0.92 0.83 0.96 0.77\n#> [34] 1.00 0.46 0.92 0.97 0.52 0.98 0.98 0.74 0.73 0.95 0.48\n#> [45] 0.97 0.90 0.99 0.87 0.17 1.00 0.98 0.84 0.97 0.97 0.97\n#> [56] 0.65 0.94 0.95 0.06 0.98 0.87\n\n# (3) Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.\nfviz_pca_var(X.pc,\n             col.var = \"contrib\", # Color by contributions to the PC\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n)\n\n# (4) Biplot of individuals and variables\nfviz_pca_biplot(X.pc, \n                repel = TRUE,\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n)"},{"path":"chapter-8-principle-component-analysis.html","id":"example-8.10-example-8.11","chapter":"1 Chapter 8: Principle Component Analysis","heading":"1.2 Example 8.10, Example 8.11","text":"","code":"\nX <-  read.table(\"dataset/T8-2.dat\",header=FALSE)\nX\n#>         V1      V2      V3      V4     V5\n#> 1   2044.9   588.2   425.8  -189.1 -209.8\n#> 2  -2143.7  -686.2   883.6  -565.9 -441.5\n#> 3   -177.8  -464.6   707.5   736.3   38.2\n#> 4  -2186.2   450.5  -184.0   443.7 -325.3\n#> 5   -878.6  -545.8   115.7   296.4  437.5\n#> 6    563.2 -1045.4   281.2   620.5  142.7\n#> 7    403.1    66.8   340.6  -135.5  521.2\n#> 8  -1988.9  -801.8 -1437.3  -148.8   61.6\n#> 9    132.8   563.7   125.3    68.2  611.5\n#> 10 -2787.3  -213.4     7.8   169.4 -202.3\n#> 11   283.4  3936.9    -0.9   276.2 -159.6\n#> 12   761.6   256.0 -2153.6  -418.8   28.2\n#> 13  -498.3   244.7   966.5 -1142.3  182.6\n#> 14  2366.2 -1193.7  -165.5   270.6 -344.9\n#> 15  1917.8  -782.0   -82.9  -196.8  -89.9\n#> 16  2187.7  -373.9   170.1   -84.1 -250.2\nn <- nrow(X)\np <- ncol(X)\n\nX.pc <- prcomp(X)\n# values of the principal components\nY <- X.pc$x\nY\n#>              PC1         PC2           PC3         PC4\n#>  [1,] -2044.9108  -588.18998   425.8014318  -189.11860\n#>  [2,]  2143.6969   686.18028   883.6071505  -565.91372\n#>  [3,]   177.7921   464.60169   707.5038436   736.30458\n#>  [4,]  2186.1878  -450.51761  -183.9938258   443.69157\n#>  [5,]   878.5975   545.80132   115.7107748   296.42832\n#>  [6,]  -563.2024  1045.40708   281.2031894   620.50453\n#>  [7,]  -403.1044   -66.79005   340.6113523  -135.47071\n#>  [8,]  1988.9026   801.78641 -1437.2896124  -148.79412\n#>  [9,]  -132.8076  -563.69033   125.3126666    68.23589\n#> [10,]  2787.2934   213.38002     7.8088146   169.40070\n#> [11,]  -283.4329 -3936.89991    -0.8923354   276.19734\n#> [12,]  -761.6016  -255.99816 -2153.5931791  -418.80896\n#> [13,]   498.2963  -244.70169   966.5134292 -1142.28340\n#> [14,] -2366.2012  1193.71037  -165.5042865   270.56662\n#> [15,] -1917.8007   782.01030   -82.8986585  -196.81567\n#> [16,] -2187.7050   373.91026   170.0992449   -84.12436\n#>              PC5\n#>  [1,]  209.77628\n#>  [2,]  441.49580\n#>  [3,]  -38.14340\n#>  [4,]  325.32683\n#>  [5,] -437.47036\n#>  [6,] -142.64953\n#>  [7,] -521.20697\n#>  [8,]  -61.60321\n#>  [9,] -611.50329\n#> [10,]  202.32507\n#> [11,]  159.55944\n#> [12,]  -28.25294\n#> [13,] -182.65658\n#> [14,]  344.92000\n#> [15,]   89.89036\n#> [16,]  250.19249\n\nlibrary(factoextra)\nfviz_eig(X.pc)\n\n# covariance matrix of first two sample PCs: y1 and y2\nY12 <- Y[, 1:2]\nS <- diag(as.vector(X.pc$sdev^2)[1:2])\nS\n#>         [,1]    [,2]\n#> [1,] 2770248       0\n#> [2,]       0 1429193\n\n## Two-part procedure\n# (i) ellipse format chart for the first two PCs (y1, y2)\nlibrary(car)\n#> Loading required package: carData\nalpha <- 0.05\nradius <- sqrt(qchisq(1-alpha, df = 2))\npar(pty = \"s\")\nplot(0, xlim=c(-5000, 5000), ylim=c(-4000, 4000), \n     xlab = expression(hat(y)[1]), ylab = expression(hat(y)[2]), type=\"n\")\nellipse(center = colMeans(Y12), shape = S, radius = radius)\npoints(Y12[, 1], Y12[, 2])\n\n# (ii) T^2 chart for the last p-2 PCs\nT2 <- Y[, 3]^2/X.pc$sdev[3]^2 +\n  Y[, 4]^2/X.pc$sdev[4]^2 +\n  Y[, 5]^2/X.pc$sdev[5]^2\n\nUCL <- qchisq(1-alpha, df = p - 2)\n\nplot(1:n, T2, ylim = range(T2, UCL*1.05), type = 'p',\n     xlab = \"Period\", ylab = expression(T^2),\n     main = expression(paste(T^2,\"-chart\")))\nabline(h = UCL, lty = 1, col = \"red\")\nlines(1:n, T2)"},{"path":"chapter-8-principle-component-analysis.html","id":"homework","chapter":"1 Chapter 8: Principle Component Analysis","heading":"1.3 Homework","text":"","code":""},{"path":"chapter-8-principle-component-analysis.html","id":"exercise-8.1-8.2","chapter":"1 Chapter 8: Principle Component Analysis","heading":"1.3.1 Exercise 8.1, 8.2","text":"","code":"\nSigma <- matrix(c(5, 2,\n                  2, 2), nrow = 2, byrow = T)\n\neigen(Sigma)\n#> eigen() decomposition\n#> $values\n#> [1] 6 1\n#> \n#> $vectors\n#>            [,1]       [,2]\n#> [1,] -0.8944272  0.4472136\n#> [2,] -0.4472136 -0.8944272\n\nV <- diag(diag(Sigma))\nrho <- sqrt(solve(V)) %*% Sigma %*% sqrt(solve(V))\n\neigen(rho)\n#> eigen() decomposition\n#> $values\n#> [1] 1.6324555 0.3675445\n#> \n#> $vectors\n#>           [,1]       [,2]\n#> [1,] 0.7071068 -0.7071068\n#> [2,] 0.7071068  0.7071068\n\n# the correlation coefficients between the PC Y_i and the standardized variable Z_k\n# Y_i, Z_k\ni <- 1\nk <- 1\ned <- eigen(rho)\ned$vector[k, i] * sqrt(ed$values[i])\n#> [1] 0.9034532"},{"path":"chapter-8-principle-component-analysis.html","id":"exercise-8.22","chapter":"1 Chapter 8: Principle Component Analysis","heading":"1.3.2 Exercise 8.22","text":"","code":"\nlibrary(factoextra)\nmydata <-  read.table(\"dataset/T1-10.dat\",header=FALSE)\nX <- mydata[, 3:9]\nn <- nrow(X)\np <- ncol(X)\n\n# use covariance matrix\nX.pc <- prcomp(X)\n# proportion of variance\nround(X.pc$sdev^2/sum(X.pc$sdev^2),4)\n#> [1] 0.8082 0.1914 0.0002 0.0001 0.0000 0.0000 0.0000\nfviz_eig(X.pc)\n\n# use correlation matrix\nZ.pc <- prcomp(X, scale = T)\n# proportion of variance\nround(Z.pc$sdev^2/sum(Z.pc$sdev^2),4)\n#> [1] 0.5887 0.1910 0.1059 0.0602 0.0265 0.0209 0.0067\nfviz_eig(Z.pc)\n\npc1 <- X.pc$x[, 1]\npc2 <- X.pc$x[, 2]\nplot(pc1,pc2,type='n')\ntext(pc1,pc2,mydata[, 1])\n\nqqnorm(pc1, pch = 1)\nqqline(pc1, col = \"red\", lwd = 2)"},{"path":"chapter-8-principle-component-analysis.html","id":"reference","chapter":"1 Chapter 8: Principle Component Analysis","heading":"1.4 Reference","text":"[1] Johnson, R. ., Wichern, D. W. (2007). Applied multivariate statistical analysis (6th ed.). Upper Saddle River, N.J.: Pearson Prentice Hall.[2] Zelterman, D. (2015). Applied Multivariate Statistics R (1st ed.).[3] Principal Component Analysis R: prcomp vs princomp, http://www.sthda.com/english/articles/31-principal-component-methods--r-practical-guide/118-principal-component-analysis--r-prcomp-vs-princomp/","code":""},{"path":"chapter-9-factor-analysis.html","id":"chapter-9-factor-analysis","chapter":"2 Chapter 9: Factor Analysis","heading":"2 Chapter 9: Factor Analysis","text":"","code":""},{"path":"chapter-9-factor-analysis.html","id":"overview-of-fa","chapter":"2 Chapter 9: Factor Analysis","heading":"2.1 Overview of FA","text":"essential purpose FA: describe covariance relationships among many variables terms understanding, unobservable, random quantities called factors.essential purpose FA: describe covariance relationships among many variables terms understanding, unobservable, random quantities called factors.Basic objective 1: determine whether \\(p\\) variables exhibit patterns relationship \nvariables can partitioned , say, \\(m\\) subsets (\\(m<p\\)),\nsubset consists group variables tending highly related others within subset subsets.\nBasic objective 1: determine whether \\(p\\) variables exhibit patterns relationship thatthe variables can partitioned , say, \\(m\\) subsets (\\(m<p\\)),subset consists group variables tending highly related others within subset subsets.Example:\nCorrelations group test scores classics, French, English, mathematics, music collected Spearman suggested underlying “intelligence” factor.\nsecond group variables: physical-fitness scores, available, might correspond another factor.\nFA seeks confirm type structure.\nExample:Correlations group test scores classics, French, English, mathematics, music collected Spearman suggested underlying “intelligence” factor.second group variables: physical-fitness scores, available, might correspond another factor.FA seeks confirm type structure.Basic objective 2: derive, create, develop new set uncorrelated variables\nnew variables called underlying factors underlying characteristics\nhope new variables give better understanding data analyzed\nnew variables can used future analyses data\nBasic objective 2: derive, create, develop new set uncorrelated variablesthe new variables called underlying factors underlying characteristicswe hope new variables give better understanding data analyzedthe new variables can used future analyses dataThe factor analysis model assumes smaller set uncorrelated variables , sense, drives controls values variables actually measured.factor analysis model assumes smaller set uncorrelated variables , sense, drives controls values variables actually measured.difference PCA FA: PCA linear combination variables; Factor Analysis measurement model latent variable.difference PCA FA: PCA linear combination variables; Factor Analysis measurement model latent variable.","code":""},{"path":"chapter-9-factor-analysis.html","id":"warnings","chapter":"2 Chapter 9: Factor Analysis","heading":"2.2 Warnings","text":"original variables already uncorrelated, little reason consider performing FA.Criticism FA:\nunique solution FA.\nSubjective decisions made determining\nnumber underlying factors\ncreated\ninterpreted\nindividuals sample evaluated \nnew variables\n\nunique solution FA.Subjective decisions made determining\nnumber underlying factors\ncreated\ninterpreted\nindividuals sample evaluated \nnew variables\nnumber underlying factorshow createdhow interpretedhow individuals sample evaluated thenew variables","code":""},{"path":"chapter-9-factor-analysis.html","id":"the-orthogonal-factor-model","chapter":"2 Chapter 9: Factor Analysis","heading":"2.3 The Orthogonal Factor Model","text":"\\(\\vec X\\) \\(p\\)-dimensional random vector \\(\\sim (\\vec \\mu, \\mathbf{\\Sigma})\\).factor analysis model \n\\[X_i - \\mu_i=l_{i1}F_1+l_{i2}F_2+\\dots+l_{im}F_m+\\epsilon_i,\\]\\(X_i\\) \\(\\)-th component \\(\\vec X\\).\\(F_1, F_2, \\dots, F_m\\) called common factors\\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p\\) called errors specific factors\\(l_{ij}\\) called loading \\(\\)th variable \\(j\\)th factoror matrix notation:\\[\\underset{(p\\times 1)}{\\vec X - \\vec \\mu}=\\underset{(p\\times m)}{\\mathbf{L}}\\underset{(m\\times 1)}{\\vec F}+\\underset{(p\\times 1)}{\\vec \\epsilon}\\]\\(\\mathbf{L}\\): matrix factor loadings","code":""},{"path":"chapter-9-factor-analysis.html","id":"orthogonal-factor-model-with-m-common-factors","chapter":"2 Chapter 9: Factor Analysis","heading":"2.3.1 Orthogonal Factor Model with \\(m\\) Common Factors","text":"\\[\\underset{(p\\times 1)}{\\vec X}=\\underset{(p\\times 1)}{\\vec \\mu}+\\underset{(p\\times m)}{\\mathbf{L}}\\underset{(m\\times 1)}{\\vec F}+\\underset{(p\\times 1)}{\\vec \\epsilon}\\]\\(\\mu_i\\): mean variable \\(\\)\\(\\epsilon_i\\): \\(\\)th specific factor\\(F_j\\): \\(j\\)th common factor\\(l_{ij}\\): loading \\(\\)th variable \\(j\\)th factorAssumptions:\\(\\vec F\\) \\(\\vec \\epsilon\\) independent\\(E(\\vec F)=\\vec 0, Cov(\\vec F)=\\mathbf{}\\)\\(E(\\vec \\epsilon)=\\vec 0, Cov(\\vec \\epsilon)=\\mathbf{\\Psi}\\), \\(\\mathbf{\\Psi}\\) diagonal matrix","code":""},{"path":"chapter-9-factor-analysis.html","id":"covariance-structure-of-vec-x-under-the-orthogonal-factor-model","chapter":"2 Chapter 9: Factor Analysis","heading":"2.3.2 Covariance Structure of \\(\\vec X\\) under the Orthogonal Factor Model","text":"\\(\\mathbf{\\Sigma}=Cov(\\vec X) = \\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\) (factor analysis equation)\\(Cov(\\vec X, \\vec F) = \\mathbf{L}\\), can decompose variance \\(\\)th variable two parts:\\(\\)th communality: \\(h_i^2=l_{i1}^2+l_{i2}^2+\\dots+l_{im}^2\\)\\(\\)th uniqueness specific variance: \\(\\psi_i\\)\\[Var(X_i)=\\sigma_{ii}=h_i^2+\\psi_i, =1,2,\\dots,p\\]determine \\(\\vec F, \\mathbf{L}\\) \\(\\vec \\epsilon\\) exist \\(\\vec X- \\vec \\mu= \\mathbf{L}\\vec F+\\vec \\epsilon\\), try find \\(\\mathbf{L}\\) \\(\\mathbf{\\Psi}\\) factor analysis equation holds: \\(\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\).determine \\(\\vec F, \\mathbf{L}\\) \\(\\vec \\epsilon\\) exist \\(\\vec X- \\vec \\mu= \\mathbf{L}\\vec F+\\vec \\epsilon\\), try find \\(\\mathbf{L}\\) \\(\\mathbf{\\Psi}\\) factor analysis equation holds: \\(\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\).Remark: practice, FA procedures almost always applied \\(Z\\) scores correlation matrix \\(\\mathbf{\\rho}\\).Remark: practice, FA procedures almost always applied \\(Z\\) scores correlation matrix \\(\\mathbf{\\rho}\\).Example 9.1: p. 484. Verifying \\(\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\) two factorsExample 9.1: p. 484. Verifying \\(\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\) two factorsExample 9.2: p. 486. Nonexistence proper solution FA equationExample 9.2: p. 486. Nonexistence proper solution FA equation","code":""},{"path":"chapter-9-factor-analysis.html","id":"nonuniqueness-of-the-factors","chapter":"2 Chapter 9: Factor Analysis","heading":"2.3.3 Nonuniqueness of the Factors","text":"\\(m \\times m\\) orthogonal matrix \\(\\mathbf{T}\\), \\[\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}=\\mathbf{LTT'L'}+\\mathbf{\\Psi}=\\mathbf{(LT)}\\mathbf{(LT)}'+\\mathbf{\\Psi}\\]\n\n\\[Cov(\\mathbf{T}'\\vec F)=\\mathbf{T}'\\mathbf{T}=\\mathbf{}.\\], \\(\\mathbf{L}\\) loading matrix, \\(\\mathbf{L}^*=\\mathbf{LT}\\) also \nloading matrix factors \\(\\vec F^*=\\mathbf{T}'\\vec F\\), satisfies\\[\\mathbf{L}^*\\vec F^*=\\mathbf{L}\\vec F.\\]Hence, factor loading matrix \\(\\mathbf{L}\\) unique.","code":""},{"path":"chapter-9-factor-analysis.html","id":"methods-of-estimation","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4 9.3 Methods of Estimation","text":"Question: FA model small number factors, adequately represent data?Solution: verify covariance relationship:\\(\\mathbf{\\Sigma}=Cov(\\vec X) = \\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\)\\(Cov(\\vec X, \\vec F) = \\mathbf{L}\\)Parameters FA model: factor loadings \\(l_{ij}\\) specific variances \\(\\psi_{}\\).Parameters FA model: factor loadings \\(l_{ij}\\) specific variances \\(\\psi_{}\\).Two popular methods parameter estimation\nprincipal component (related principal factor) method\nmaximum likelihood method.\nTwo popular methods parameter estimationthe principal component (related principal factor) methodthe maximum likelihood method.","code":""},{"path":"chapter-9-factor-analysis.html","id":"the-principal-component-and-principal-factor-method","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4.1 The Principal Component (and Principal Factor) Method","text":"spectral decomposition:\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} & = \\lambda_1\\vec e_1\\vec e_1'+\\dots+\\lambda_p\\vec e_p\\vec e_p'\\\\\n&=\\begin{bmatrix}\n    \\sqrt{\\lambda_1}\\vec e_1&\n    \\dots&\n    \\sqrt{\\lambda_p}\\vec e_p\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\sqrt{\\lambda_1}\\vec e_1\\\\\n    \\dots\\\\\n    \\sqrt{\\lambda_p}\\vec e_p\n\\end{bmatrix}\n\\end{aligned}\n\\]\\(\\lambda_1\\ge \\lambda_2\\ge\\dots\\lambda_p\\ge0\\).Neglecting last \\(p-m\\) small eigenvalues allowing specific factors, following approximation:\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} & \\approx \\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\\\\n&=\\begin{bmatrix}\n    \\sqrt{\\lambda_1}\\vec e_1&\n    \\dots&\n    \\sqrt{\\lambda_m}\\vec e_m\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\sqrt{\\lambda_1}\\vec e_1\\\\\n    \\dots\\\\\n    \\sqrt{\\lambda_m}\\vec e_m\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\psi_{1} & 0 & \\dots & 0 \\\\\n    0 & \\psi_{2} & \\dots & 0 \\\\\n    \\vdots & \\vdots & \\ddots& \\vdots \\\\\n    0 & 0 & \\dots & \\psi_{p}\n\\end{bmatrix}\n\\end{aligned}\n\\]","code":""},{"path":"chapter-9-factor-analysis.html","id":"principle-component-solution-of-the-factor-model","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4.1.1 Principle Component Solution of the Factor Model","text":"matrix estimated factor loadings \\(\\{\\tilde{l}_{ij}\\}\\):\\[\\widetilde{\\mathbf{L}}=\\begin{bmatrix}\n    \\sqrt{\\hat\\lambda_1}\\hat{\\vec e_1}&\n    \\dots&\n    \\sqrt{\\hat\\lambda_m}\\hat{\\vec e_m}\n\\end{bmatrix},\\]\\(({\\hat\\lambda_i}\\hat{\\vec e_i})\\) eigenvalue-eigenvector pairs sample covariance matrix \\(\\mathbf{S}\\), \\(\\hat\\lambda_1\\ge \\hat\\lambda_2\\ge\\dots\\hat\\lambda_p\\ge0\\).estimated specific variances provided diagonal elements \\(\\mathbf{S} - \\widetilde{\\mathbf{L}}\\widetilde{\\mathbf{L}}'\\):\\[\\widetilde{\\mathbf{\\Psi}}=\\begin{bmatrix}\n    \\widetilde\\psi_{1} & 0 & \\dots & 0 \\\\\n    0 & \\widetilde\\psi_{2} & \\dots & 0 \\\\\n    \\vdots & \\vdots & \\ddots& \\vdots \\\\\n    0 & 0 & \\dots & \\widetilde\\psi_{p}\n\\end{bmatrix} \\text{ } \\widetilde\\psi_{}=s_{ii} - \\sum_{j=1}^m\\tilde{l}_{ij}^2,\\]Communalities estimated \\(\\tilde{h}_{}^2=\\sum_{j=1}^m\\tilde{l}_{ij}^2\\)Communalities estimated \\(\\tilde{h}_{}^2=\\sum_{j=1}^m\\tilde{l}_{ij}^2\\)residual matrix: \\(\\mathbf{S} - (\\widetilde{\\mathbf{L}}\\widetilde{\\mathbf{L}}' + \\widetilde{\\mathbf{\\Psi}})\\).residual matrix: \\(\\mathbf{S} - (\\widetilde{\\mathbf{L}}\\widetilde{\\mathbf{L}}' + \\widetilde{\\mathbf{\\Psi}})\\).contribution total sample variance \\(j\\)th factor: \\(\\sum_{=1}^p\\tilde{l}_{ij}^2=\\hat\\lambda_i\\)contribution total sample variance \\(j\\)th factor: \\(\\sum_{=1}^p\\tilde{l}_{ij}^2=\\hat\\lambda_i\\)","code":""},{"path":"chapter-9-factor-analysis.html","id":"example-9.3-p.-491","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4.1.2 Example 9.3 (p. 491)","text":"","code":""},{"path":"chapter-9-factor-analysis.html","id":"example-9.4-p.-493-fa-of-stock-price-data","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4.1.3 Example 9.4 (p. 493, FA of stock-price data)","text":"example, first factor might called market factor, second factor might called industry factor.","code":"\nX <- as.matrix(read.table(\"dataset/T8-4.DAT\"))\ncolnames(X) <- c(\"JPM\", \"Citi\", \"WellsF\", \"Shell\", \"Exxon\")\nhead(X)\n#>             JPM       Citi     WellsF      Shell      Exxon\n#> [1,]  0.0130338 -0.0078431 -0.0031889 -0.0447693  0.0052151\n#> [2,]  0.0084862  0.0166886 -0.0062100  0.0119560  0.0134890\n#> [3,] -0.0179153 -0.0086393  0.0100360  0.0000000 -0.0061428\n#> [4,]  0.0215589 -0.0034858  0.0174353 -0.0285917 -0.0069534\n#> [5,]  0.0108225  0.0037167 -0.0101345  0.0291900  0.0409751\n#> [6,]  0.0101713 -0.0121978 -0.0083768  0.0137083  0.0029895\n\n# PCA based on the spectral decomposition\n# cor=T: standardized variables/correlation matrix\nX.pc <- princomp(X, cor=T)\nlibrary(factoextra)\n#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_eig(X.pc)\n\nR <- cor(X)\ned <- eigen(R)\nlambda <- ed$values\nlambda\n#> [1] 2.4372731 1.4070127 0.5005127 0.4000316 0.2551699\nei <- ed$vectors\n\nL.all <- ei %*% diag(sqrt(lambda)) \n\n# m = 2\nm <- 2\nL <- L.all[, 1:m] # estimated factor loadings\nh2 <- rowSums(L^2) # communalities\npsi <- 1 - h2 # estimated specific variances\n\ndf <- data.frame(variable=c(colnames(X)),\n                 F1=L[, 1],\n                 F2=L[, 2],\n                 h2=h2,\n                 psi=psi)\ndf[sapply(df, is.numeric)] <- round(df[sapply(df, is.numeric)], 3)\ndf\n#>   variable     F1     F2    h2   psi\n#> 1      JPM -0.732  0.437 0.727 0.273\n#> 2     Citi -0.831  0.280 0.770 0.230\n#> 3   WellsF -0.726  0.374 0.667 0.333\n#> 4    Shell -0.605 -0.694 0.847 0.153\n#> 5    Exxon -0.563 -0.719 0.834 0.166\n\nResidual <- R - L %*% t(L) - diag(psi)\nResidual\n#>                JPM        Citi       WellsF        Shell\n#> JPM     0.00000000 -0.09884073 -0.184513342 -0.025317756\n#> Citi   -0.09884073  0.00000000 -0.134323655  0.014310342\n#> WellsF -0.18451334 -0.13432365  0.000000000  0.002794963\n#> Shell  -0.02531776  0.01431034  0.002794963  0.000000000\n#> Exxon   0.05580220 -0.05378426  0.005960127 -0.155835955\n#>               Exxon\n#> JPM     0.055802201\n#> Citi   -0.053784258\n#> WellsF  0.005960127\n#> Shell  -0.155835955\n#> Exxon   0.000000000"},{"path":"chapter-9-factor-analysis.html","id":"the-maximum-likelihood-method","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4.2 The Maximum likelihood Method","text":"assume normality, maximize likelihood \\(\\vec \\mu\\) \\(\\mathbf{\\Sigma}\\), subject \\(\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\).assume normality, maximize likelihood \\(\\vec \\mu\\) \\(\\mathbf{\\Sigma}\\), subject \\(\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\).desirable make \\(\\mathbf{L}\\) well defined imposing computationally convenient uniqueness condition:desirable make \\(\\mathbf{L}\\) well defined imposing computationally convenient uniqueness condition:\\[\\mathbf{L}'\\mathbf{\\Psi}^{-1}\\mathbf{L}=\\mathbf{\\Delta}=diag\\{\\delta_1\\ge\\dots\\ge\\delta_m\\}\\]require less parameters restricted FA model unrestricted model.\nnumber parameters restricted FA model (\\(\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\)): \\(p+pm-(m-1)m/2\\)\nnumber parameters unrestricted model (\\(\\mathbf{\\Sigma}\\)): \\(p(p+1)/2\\)\nnumber parameters restricted FA model (\\(\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\)): \\(p+pm-(m-1)m/2\\)number parameters unrestricted model (\\(\\mathbf{\\Sigma}\\)): \\(p(p+1)/2\\)","code":""},{"path":"chapter-9-factor-analysis.html","id":"example-9.5-factor-analysis-of-stock-price-data-using-the-maximum-likelihood-method","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4.2.1 Example 9.5 (Factor analysis of stock-price data using the maximum likelihood method)","text":"ML method suggests first factor might called banking factor, second factor might called oil industry factor.","code":"\nX <- as.matrix(read.table(\"dataset/T8-4.DAT\"))\ncolnames(X) <- c(\"JPM\", \"Citi\", \"WellsF\", \"Shell\", \"Exxon\")\nhead(X)\n#>             JPM       Citi     WellsF      Shell      Exxon\n#> [1,]  0.0130338 -0.0078431 -0.0031889 -0.0447693  0.0052151\n#> [2,]  0.0084862  0.0166886 -0.0062100  0.0119560  0.0134890\n#> [3,] -0.0179153 -0.0086393  0.0100360  0.0000000 -0.0061428\n#> [4,]  0.0215589 -0.0034858  0.0174353 -0.0285917 -0.0069534\n#> [5,]  0.0108225  0.0037167 -0.0101345  0.0291900  0.0409751\n#> [6,]  0.0101713 -0.0121978 -0.0083768  0.0137083  0.0029895\n\nm <- 2\n# Maximum likelihood solution\n# The R function factanal() computes the mle on the basis of the sample correlation matrix R.\nfit.mle <- factanal(X, m, rotation = \"none\") # agrees with textbook (almost); \"varimax\" is the default\nfit.mle\n#> \n#> Call:\n#> factanal(x = X, factors = m, rotation = \"none\")\n#> \n#> Uniquenesses:\n#>    JPM   Citi WellsF  Shell  Exxon \n#>  0.417  0.275  0.542  0.005  0.530 \n#> \n#> Loadings:\n#>        Factor1 Factor2\n#> JPM     0.121   0.754 \n#> Citi    0.328   0.786 \n#> WellsF  0.188   0.650 \n#> Shell   0.997         \n#> Exxon   0.685         \n#> \n#>                Factor1 Factor2\n#> SS loadings      1.622   1.610\n#> Proportion Var   0.324   0.322\n#> Cumulative Var   0.324   0.646\n#> \n#> Test of the hypothesis that 2 factors are sufficient.\n#> The chi square statistic is 1.97 on 1 degree of freedom.\n#> The p-value is 0.16\n\n# check: \ncolSums(fit.mle$loadings^2)\n#>  Factor1  Factor2 \n#> 1.622061 1.609847\n\nL <- fit.mle$loadings\nL\n#> \n#> Loadings:\n#>        Factor1 Factor2\n#> JPM     0.121   0.754 \n#> Citi    0.328   0.786 \n#> WellsF  0.188   0.650 \n#> Shell   0.997         \n#> Exxon   0.685         \n#> \n#>                Factor1 Factor2\n#> SS loadings      1.622   1.610\n#> Proportion Var   0.324   0.322\n#> Cumulative Var   0.324   0.646\nPsi <- diag(fit.mle$uniquenesses)\nround(t(L)%*%solve(Psi)%*%L,4) # diagonal\n#>          Factor1 Factor2\n#> Factor1 200.3689  0.0000\n#> Factor2   0.0000  4.4049\nresids.mle <- R - (L%*%t(L) + Psi) # much smaller than residsual from the PC solution\nresids.mle\n#>                  JPM          Citi        WellsF\n#> JPM     1.055860e-07  7.496780e-06 -2.564223e-03\n#> Citi    7.496780e-06  3.255672e-08  1.608871e-03\n#> WellsF -2.564223e-03  1.608871e-03  5.157370e-08\n#> Shell  -3.325561e-04  2.116218e-04 -9.518792e-06\n#> Exxon   5.198222e-02 -3.307885e-02  5.547153e-04\n#>                Shell         Exxon\n#> JPM    -3.325561e-04  5.198222e-02\n#> Citi    2.116218e-04 -3.307885e-02\n#> WellsF -9.518792e-06  5.547153e-04\n#> Shell  -1.559500e-06  1.218853e-04\n#> Exxon   1.218853e-04  2.670491e-07\nround(resids.mle, 4)\n#>            JPM    Citi  WellsF  Shell   Exxon\n#> JPM     0.0000  0.0000 -0.0026 -3e-04  0.0520\n#> Citi    0.0000  0.0000  0.0016  2e-04 -0.0331\n#> WellsF -0.0026  0.0016  0.0000  0e+00  0.0006\n#> Shell  -0.0003  0.0002  0.0000  0e+00  0.0001\n#> Exxon   0.0520 -0.0331  0.0006  1e-04  0.0000"},{"path":"chapter-9-factor-analysis.html","id":"a-large-sample-test-for-the-number-of-common-factors","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4.3 A Large Sample Test for the Number of Common Factors","text":"result Example 9.5, likelihood ratio test \\(m\\):“Test hypothesis 2 factors sufficient. chi square statistic 1.97 1 degree freedom. p-value 0.16.”","code":""},{"path":"chapter-9-factor-analysis.html","id":"likelihood-ratio-test-for-m","chapter":"2 Chapter 9: Factor Analysis","heading":"2.4.3.1 likelihood ratio test for \\(m\\)","text":"Equation (4-18), Result 5.2, Supplement 9A textbook, likelihood ratio statistic \\(H_0: \\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi}\\):\\[-2\\ln\\Lambda = n\\ln\\bigg(\\frac{|\\hat{\\mathbf{L}} \\hat{\\mathbf{L}}'+\\hat{\\mathbf{\\Psi}}|}{|(n-1)/n\\mathbf{S}|}\\bigg).\\]Bartlett’s correlation:\\[(n-1-(2p+4m+5)/6)\\ln\\bigg(\\frac{|\\hat{\\mathbf{L}} \\hat{\\mathbf{L}}'+\\hat{\\mathbf{\\Psi}}|}{|(n-1)/n\\mathbf{S}|}\\bigg)\\sim \\chi^2_{[(p-m)^2]-(p+m)]/2}\\]","code":""},{"path":"chapter-9-factor-analysis.html","id":"factor-rotation-mathbflmathbflt","chapter":"2 Chapter 9: Factor Analysis","heading":"2.5 9.4 Factor Rotation \\(\\mathbf{L}^*=\\mathbf{LT}\\)","text":"shown FA model invariant orthogonal transformation (loadings solution unique.)shown FA model invariant orthogonal transformation (loadings solution unique.)attempt choose transformation make factors interpreble. loadings particular factor can easily split interpreble groups, means \\(j\\), \\(\\hat{l}_{ij}\\) highly varied.attempt choose transformation make factors interpreble. loadings particular factor can easily split interpreble groups, means \\(j\\), \\(\\hat{l}_{ij}\\) highly varied.","code":""},{"path":"chapter-9-factor-analysis.html","id":"varimax-criterion","chapter":"2 Chapter 9: Factor Analysis","heading":"2.5.1 varimax criterion","text":"Define \\(\\tilde{l}_{ij}^*=\\dfrac{\\hat{l}_{ij}^*}{\\hat h_i}\\). , varimax criterion selects orthogonal transformation \\(\\mathbf{T}\\) maximize\\[V=\\dfrac{1}{p}\\sum_{j=1}^m\\Big[\\sum_{=1}^p\\tilde{l}_{ij}^{*4} -\\dfrac{(\\sum_{=1}^p\\tilde{l}_{ij}^{*2})^2}{p} \\Big].\\]","code":""},{"path":"chapter-9-factor-analysis.html","id":"example-9.9-p.-508-self-study-sas-program","chapter":"2 Chapter 9: Factor Analysis","heading":"2.5.2 Example 9.9 (p. 508, self-study, SAS program)","text":"factor 1: nutritional factor; factor 2: taste factor","code":""},{"path":"chapter-9-factor-analysis.html","id":"example-9.10-rotated-loading-for-the-stock-price-data","chapter":"2 Chapter 9: Factor Analysis","heading":"2.5.3 Example 9.10 (Rotated loading for the stock-price data)","text":"","code":"\nX <- as.matrix(read.table(\"dataset/T8-4.DAT\"))\ncolnames(X) <- c(\"JPM\", \"Citi\", \"WellsF\", \"Shell\", \"Exxon\")\nhead(X)\n#>             JPM       Citi     WellsF      Shell      Exxon\n#> [1,]  0.0130338 -0.0078431 -0.0031889 -0.0447693  0.0052151\n#> [2,]  0.0084862  0.0166886 -0.0062100  0.0119560  0.0134890\n#> [3,] -0.0179153 -0.0086393  0.0100360  0.0000000 -0.0061428\n#> [4,]  0.0215589 -0.0034858  0.0174353 -0.0285917 -0.0069534\n#> [5,]  0.0108225  0.0037167 -0.0101345  0.0291900  0.0409751\n#> [6,]  0.0101713 -0.0121978 -0.0083768  0.0137083  0.0029895\n\nR = cor(X)\n\nm <- 2\n# Maximum likelihood solution\n# The R function factanal() computes the mle on the basis of the sample correlation matrix R.\nfit.mle <- factanal(X, m, rotation = \"none\") # agrees with textbook (almost); \"varimax\" is the default\n\n# Rotations \nfit.mle # mle; no rotation\n#> \n#> Call:\n#> factanal(x = X, factors = m, rotation = \"none\")\n#> \n#> Uniquenesses:\n#>    JPM   Citi WellsF  Shell  Exxon \n#>  0.417  0.275  0.542  0.005  0.530 \n#> \n#> Loadings:\n#>        Factor1 Factor2\n#> JPM     0.121   0.754 \n#> Citi    0.328   0.786 \n#> WellsF  0.188   0.650 \n#> Shell   0.997         \n#> Exxon   0.685         \n#> \n#>                Factor1 Factor2\n#> SS loadings      1.622   1.610\n#> Proportion Var   0.324   0.322\n#> Cumulative Var   0.324   0.646\n#> \n#> Test of the hypothesis that 2 factors are sufficient.\n#> The chi square statistic is 1.97 on 1 degree of freedom.\n#> The p-value is 0.16\nfit.mle.r = factanal(X, m, rotation = \"varimax\")\nfit.mle.r  \n#> \n#> Call:\n#> factanal(x = X, factors = m, rotation = \"varimax\")\n#> \n#> Uniquenesses:\n#>    JPM   Citi WellsF  Shell  Exxon \n#>  0.417  0.275  0.542  0.005  0.530 \n#> \n#> Loadings:\n#>        Factor1 Factor2\n#> JPM    0.763          \n#> Citi   0.819   0.232  \n#> WellsF 0.668   0.108  \n#> Shell  0.113   0.991  \n#> Exxon  0.108   0.677  \n#> \n#>                Factor1 Factor2\n#> SS loadings      1.725   1.507\n#> Proportion Var   0.345   0.301\n#> Cumulative Var   0.345   0.646\n#> \n#> Test of the hypothesis that 2 factors are sufficient.\n#> The chi square statistic is 1.97 on 1 degree of freedom.\n#> The p-value is 0.16\n# Bank stocks (variables 1,2,3) load heavily on factor 1, much less so on factor 2;\n# oil stocks load heavily on factor 2, much less so on factor 1.\n\n# or you can use varimax() function to rotate the loadings\nrotate = varimax(loadings(fit.mle))  # identical to fit.mle.r, but labeling of factors is reversed\nrotate\n#> $loadings\n#> \n#> Loadings:\n#>        Factor1 Factor2\n#> JPM            0.763  \n#> Citi   0.232   0.819  \n#> WellsF 0.108   0.668  \n#> Shell  0.991   0.113  \n#> Exxon  0.677   0.108  \n#> \n#>                Factor1 Factor2\n#> SS loadings      1.507   1.725\n#> Proportion Var   0.301   0.345\n#> Cumulative Var   0.301   0.646\n#> \n#> $rotmat\n#>            [,1]      [,2]\n#> [1,]  0.9927706 0.1200276\n#> [2,] -0.1200276 0.9927706"},{"path":"chapter-9-factor-analysis.html","id":"factor-scores-estimation-of-the-common-factors-vec-f","chapter":"2 Chapter 9: Factor Analysis","heading":"2.6 9.5 Factor Scores (Estimation of the common factors \\(\\vec f\\))","text":"Factor scores estimates values unobserved random factor vectors \\(\\vec F\\), estimates unknown parameters.","code":""},{"path":"chapter-9-factor-analysis.html","id":"the-weighted-least-square-method-bartlett-scores-in-r-when-using-factanal","chapter":"2 Chapter 9: Factor Analysis","heading":"2.6.1 The Weighted Least Square Method (‘Bartlett’ scores in R when using factanal())","text":"Bartlett proposed choosing estimates \\(\\hat{\\vec f}\\) \\(\\vec f\\) minimize sum squares errors, weighted reciprocal variances:\\[\\sum_{=1}^p\\dfrac{\\epsilon_i^2}{\\psi_i}=\\vec \\epsilon'\\mathbf{\\Psi}^{-1}\\vec \\epsilon=(\\vec x - \\vec \\mu - \\mathbf{L}\\vec f)'\\mathbf{\\Psi}^{-1}(\\vec x - \\vec \\mu - \\mathbf{L}\\vec f).\\]Comment: principle component solution, generate factor scores using unweighted (ordinary) least squares procedure (assume specific variances \\(\\psi_i\\) equal nearly equal.)","code":""},{"path":"chapter-9-factor-analysis.html","id":"the-regression-method-regression-scores-in-r-when-using-factanal","chapter":"2 Chapter 9: Factor Analysis","heading":"2.6.2 The Regression Method (‘regression’ scores in R when using factanal())","text":"Regression formula: \\(\\vec F \\sim \\vec x\\)Regression formula: \\(\\vec F \\sim \\vec x\\)normality assumption, \\(\\vec X - \\vec \\mu\\) \\(\\vec F\\) jointly normal:normality assumption, \\(\\vec X - \\vec \\mu\\) \\(\\vec F\\) jointly normal:\\[\n\\begin{bmatrix}\n    \\vec X - \\vec \\mu \\\\\n    \\vec F\n\\end{bmatrix} = N_{p+m} \\Bigg(\\vec 0, \\begin{bmatrix}\n    \\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi} & \\mathbf{L} \\\\\n    \\mathbf{L}' & \\mathbf{}\n\\end{bmatrix}\\Bigg)\n\\]Result 4.6, conditional distribution \\(\\vec F|\\vec x\\) multivariate normal \\[E(\\vec F|\\vec x) = \\mathbf{L}'\\mathbf{\\Sigma}^{-1}(\\vec X - \\vec \\mu)=\\mathbf{L}'(\\mathbf{L}\\mathbf{L}'+\\mathbf{\\Psi})^{-1}(\\vec X - \\vec \\mu)\\]","code":""},{"path":"chapter-9-factor-analysis.html","id":"the-stock-price-data-example-for-factor-scores","chapter":"2 Chapter 9: Factor Analysis","heading":"2.6.2.1 The stock-price data Example for factor scores","text":"","code":"\nX <- as.matrix(read.table(\"dataset/T8-4.DAT\"))\ncolnames(X) <- c(\"JPM\", \"Citi\", \"WellsF\", \"Shell\", \"Exxon\")\nhead(X)\n#>             JPM       Citi     WellsF      Shell      Exxon\n#> [1,]  0.0130338 -0.0078431 -0.0031889 -0.0447693  0.0052151\n#> [2,]  0.0084862  0.0166886 -0.0062100  0.0119560  0.0134890\n#> [3,] -0.0179153 -0.0086393  0.0100360  0.0000000 -0.0061428\n#> [4,]  0.0215589 -0.0034858  0.0174353 -0.0285917 -0.0069534\n#> [5,]  0.0108225  0.0037167 -0.0101345  0.0291900  0.0409751\n#> [6,]  0.0101713 -0.0121978 -0.0083768  0.0137083  0.0029895\n\nm <- 2\n# WLS scores \nfit.ml.WLS.r = factanal(X, m, rotation = \"varimax\", scores = \"Bartlett\") \n\n# Regression scores \nfit.ml.reg.r = factanal(X, m, rotation = \"varimax\", scores = \"regression\")\nfit.ml.reg.r\n#> \n#> Call:\n#> factanal(x = X, factors = m, scores = \"regression\", rotation = \"varimax\")\n#> \n#> Uniquenesses:\n#>    JPM   Citi WellsF  Shell  Exxon \n#>  0.417  0.275  0.542  0.005  0.530 \n#> \n#> Loadings:\n#>        Factor1 Factor2\n#> JPM    0.763          \n#> Citi   0.819   0.232  \n#> WellsF 0.668   0.108  \n#> Shell  0.113   0.991  \n#> Exxon  0.108   0.677  \n#> \n#>                Factor1 Factor2\n#> SS loadings      1.725   1.507\n#> Proportion Var   0.345   0.301\n#> Cumulative Var   0.345   0.646\n#> \n#> Test of the hypothesis that 2 factors are sufficient.\n#> The chi square statistic is 1.97 on 1 degree of freedom.\n#> The p-value is 0.16\n\nout = cbind(fit.ml.WLS.r$scores, fit.ml.reg.r$scores) # use varimax\ncolnames(out) = c(\"wls.r.F1\", \"wls.r.F2\", \"reg.r.F1\", \"reg.r.F2\")\nout[1:10,] # There are n = 103 rows\n#>          wls.r.F1    wls.r.F2    reg.r.F1    reg.r.F2\n#>  [1,]  0.25089936 -1.85367070  0.16535864 -1.83427398\n#>  [2,]  0.44303382  0.24787783  0.36753184  0.25550919\n#>  [3,] -0.48078772 -0.09835680 -0.39519052 -0.10792854\n#>  [4,]  0.79921292 -1.31497895  0.62520403 -1.28789064\n#>  [5,] -0.09859658  0.95881635 -0.06003873  0.94945235\n#>  [6,] -0.47081640  0.41285165 -0.37607023  0.39962913\n#>  [7,]  0.95854964  0.88173946  0.80260447  0.89563925\n#>  [8,]  1.03632812 -0.03557662  0.84651321 -0.01307322\n#>  [9,] -1.07061981 -1.14851602 -0.89995399 -1.16280345\n#> [10,]  0.55016716 -0.97789250  0.42882250 -0.95869583\n\n# Data reduction is accomplished by replacing the standardized data by these two simple factor scores.\nplot(fit.ml.reg.r$scores, main = \"Factor scores: first of 2 regression methods, \\n mle estimates, varimax rotation\")\nabline(h=0)\nabline(v=0)"},{"path":"chapter-9-factor-analysis.html","id":"reference-1","chapter":"2 Chapter 9: Factor Analysis","heading":"2.7 Reference","text":"[1] Johnson, R. ., Wichern, D. W. (2007). Applied multivariate statistical analysis (6th ed.). Upper Saddle River, N.J.: Pearson Prentice Hall.[2] Zelterman, D. (2015). Applied Multivariate Statistics R (1st ed.).[3] Statistics 575: Multivariate Analysis, Douglas Wiens (http://www.mathstat.ualberta.ca/~wiens/stat575/stat575.html)","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"chapter-10-discrimination-and-classification","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3 Chapter 10: Discrimination and Classification","text":"Questions interest:multivariate observations two identified populations, can characterize ?combination measurements can used clearly distinguish groups?rule can used optimally assign new observations two labeled classes?think multivariate terms, use one variable time distinguish groups individuals, , rather, use combination explanatory variables.PCA? objectives PCA (1) data reduction (2) interpretation, although might use principle components identify groups.discriminant analysis begin knowing group membership, try identify linear combinations several variables can used distinguish groups.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"an-introductory-example","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.1 10.1 An Introductory Example","text":"Data three varieties wine cultivars given data set wines (Forina et al. 1988). 178 different wines examined. list 14 variables data set listed Table 10.1:<img src=“images/Table_10_1.png”, width=“400”>top row left column pairwise scatterplots , can identify individual variables demonstrate individual variables differ across three groups wines.Specifically, can perform hypothesis test means variables across three groups. (one-way ANOVA)find one-way analysis variance demonstrates extreme\nlevel statistical significance every variable except Ash variable data set.find one-way analysis variance demonstrates extreme\nlevel statistical significance every variable except Ash variable data set.marginal approach intuitive identifying group means, \nfails identify discriminatory characteristics individuals.marginal approach intuitive identifying group means, \nfails identify discriminatory characteristics individuals.One individual cultivar may high low value specified measurement, histograms three groups may exhibit considerable overlap, making assessment group membership difficult.overlap readily apparent following parallel coordinate plot:One individual cultivar may high low value specified measurement, histograms three groups may exhibit considerable overlap, making assessment group membership difficult.overlap readily apparent following parallel coordinate plot:univariate approach fails include correlations individual\nmeasurements. may combinations variables provide much higher level discriminatory precision three cultivars univariate approach.univariate approach fails include correlations individual\nmeasurements. may combinations variables provide much higher level discriminatory precision three cultivars univariate approach.Considering correlations, number plotted pairs useful identifying discriminating three cultivars aforementioned pairwise scatterplots. include (Abs, Proline), (Alcohol, Hue), (Color, Flav) examples.Considering correlations, number plotted pairs useful identifying discriminating three cultivars aforementioned pairwise scatterplots. include (Abs, Proline), (Alcohol, Hue), (Color, Flav) examples.following two sections describe discriminatory methods based linear combinations variables data set. support vector approach, described Section 10.4, identifies specific observations help define regions high discriminatory value necessarily seek linear combinations achieve goals.following two sections describe discriminatory methods based linear combinations variables data set. support vector approach, described Section 10.4, identifies specific observations help define regions high discriminatory value necessarily seek linear combinations achieve goals.","code":"\nwines <- read.table(\"dataset/Wines.txt\", header = TRUE)\nhead(wines)\n#>   Class Alcohol Malic  Ash Alcal  Mg Phenol Flav Nonf Proan\n#> 1     1   14.23  1.71 2.43  15.6 127   2.80 3.06 0.28  2.29\n#> 2     1   13.20  1.78 2.14  11.2 100   2.65 2.76 0.26  1.28\n#> 3     1   13.16  2.36 2.67  18.6 101   2.80 3.24 0.30  2.81\n#> 4     1   14.37  1.95 2.50  16.8 113   3.85 3.49 0.24  2.18\n#> 5     1   13.24  2.59 2.87  21.0 118   2.80 2.69 0.39  1.82\n#> 6     1   14.20  1.76 2.45  15.2 112   3.27 3.39 0.34  1.97\n#>   Color  Hue  Abs Proline\n#> 1  5.64 1.04 3.92    1065\n#> 2  4.38 1.05 3.40    1050\n#> 3  5.68 1.03 3.17    1185\n#> 4  7.80 0.86 3.45    1480\n#> 5  4.32 1.04 2.93     735\n#> 6  6.75 1.05 2.85    1450\ncolors <- c(\"green\", \"red\" ,\"blue\")[wines[ , 1]]\n\n# The Class variable is jittered to make it easier to view.\n# new dataframe with jittered Class variable\nnewwine <- cbind(jitter(as.integer(wines[ , 1])), wines[ , -1]) # jitter: Add a small amount of noise to a numeric vector\nnames(newwine)[1] <- names(wines)[1] # old name to new variable\n\npairs(newwine, pch = 16, cex = .3, gap = 0, col = colors, xaxt = \"n\", yaxt = \"n\")\nvars <- dim(wines)[2]          # number of variables\nmn <- rep(0, 3)\nfor (i in 2 : vars)            # omit the first, Class variable\n{\n  z <- summary.aov(aov(wines[, i] ~ wines$Class))\n  # print(z)\n  p <- z[[1]]$\"Pr(>F)\"[1]    # capture the p-values\n  # print(p)\n  p <- max(p, .0001)\n  for(j in 1 : 3)\n  {\n    mn[j] <- mean(wines[ wines$Class == j, i])\n  }\n  if(i == 2)univ <- c(p,mn)\n  else univ <- rbind(univ, c(p, mn))\n}\n\nrow.names(univ) <- colnames(wines)[2 : vars]\ncolnames(univ) <- c( \"p-value\", \"Group=1\", \n                     \"Group=2\", \"Group=3\")\nprint(univ, 4)\n#>           p-value  Group=1  Group=2  Group=3\n#> Alcohol 0.0001000   13.745  12.2787  13.1538\n#> Malic   0.0001000    2.011   1.9327   3.3338\n#> Ash     0.5104977    2.456   2.2448   2.4371\n#> Alcal   0.0001000   17.037  20.2380  21.4167\n#> Mg      0.0050754  106.339  94.5493  99.3125\n#> Phenol  0.0001000    2.840   2.2589   1.6787\n#> Flav    0.0001000    2.982   2.0808   0.7815\n#> Nonf    0.0001000    0.290   0.3637   0.4475\n#> Proan   0.0001000    1.899   1.6303   1.1535\n#> Color   0.0003382    5.528   3.0866   7.3963\n#> Hue     0.0001000    1.062   1.0563   0.6827\n#> Abs     0.0001000    3.158   2.7854   1.6835\n#> Proline 0.0001000 1115.712 519.5070 629.8958\n\ntable(wines$Class)      # frequency table for each level\n#> \n#>  1  2  3 \n#> 59 71 48\nlibrary(MASS)\nparcoord(wines[, c(9, 11 : 14, 7, 8)], col = colors)"},{"path":"chapter-10-discrimination-and-classification.html","id":"multinomial-logistic-regression","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.2 10.2 Multinomial Logistic Regression","text":"","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"two-groups","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.2.1 Two groups","text":"use logistic regression distinguish two different groups. classification criteria based probabilistic statement group membership observation.use logistic regression distinguish two different groups. classification criteria based probabilistic statement group membership observation.Denote group membership bivariate response variable, \\(Y = 0\\) \\(1\\), conditional vector explanatory variables \\(\\vec x\\).Denote group membership bivariate response variable, \\(Y = 0\\) \\(1\\), conditional vector explanatory variables \\(\\vec x\\).observed data \\(\\)-th observation: \\((y_i, \\vec x_i)\\).observed data \\(\\)-th observation: \\((y_i, \\vec x_i)\\).logistic regression model:logistic regression model:\\[\\log\\Bigg(\\dfrac{\\Pr(Y=1|\\vec x)}{\\Pr(Y=0|\\vec x)}\\Bigg)=\\vec \\beta'\\vec x\\]simple algebraic manipulation, \\[\\Pr(Y=1|\\vec x)=\\dfrac{1}{1+e^{-\\vec \\beta'\\vec x}}\\]can estimate parameter vector *using maximum likelihood perform statistical inference .","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"more-than-two-groups","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.2.2 More than two groups","text":"commonly used approach selects one category reference baseline compares categories .commonly used approach selects one category reference baseline compares categories .approach performs pairwise comparison categories single baseline, reference category.approach performs pairwise comparison categories single baseline, reference category.approach useful interpretation obvious baseline standard, makes sense categories compared .approach useful interpretation obvious baseline standard, makes sense categories compared .case wine cultivar data, obvious comparison group. set Class 2 reference, since Class 2 observations among three classes.case wine cultivar data, obvious comparison group. set Class 2 reference, since Class 2 observations among three classes.generalization logistic regression model three categories wine data pair simultaneous equations:generalization logistic regression model three categories wine data pair simultaneous equations:\\[\\log\\Bigg(\\dfrac{\\Pr(\\text{Class}=1|\\vec x)}{\\Pr(\\text{Class}=2|\\vec x)}\\Bigg)=\\vec \\beta_1'\\vec x\\]\n\\[\\log\\Bigg(\\dfrac{\\Pr(\\text{Class}=3|\\vec x)}{\\Pr(\\text{Class}=2|\\vec x)}\\Bigg)=\\vec \\beta_3'\\vec x\\]","code":"\nlibrary(nnet)\nwines$Class <- as.factor(wines$Class) # create factor categories\nwines$rClass <- relevel(wines$Class, ref = 2)  # set reference category \n\nwinelogit <- multinom(rClass ~ Alcohol + Ash + Alcal + Abs  + Proline, \n                      data = wines, maxit = 200)\n#> # weights:  21 (12 variable)\n#> initial  value 195.552987 \n#> iter  10 value 38.789908\n#> iter  20 value 20.100843\n#> iter  30 value 17.180767\n#> iter  40 value 16.286691\n#> iter  50 value 13.857030\n#> iter  60 value 13.727827\n#> iter  70 value 13.569620\n#> iter  80 value 13.543034\n#> iter  90 value 13.521774\n#> iter 100 value 13.465942\n#> iter 110 value 13.439976\n#> iter 120 value 13.418083\n#> iter 130 value 13.405035\n#> iter 140 value 13.394287\n#> iter 150 value 13.360739\n#> iter 160 value 13.351303\n#> iter 170 value 13.330023\n#> final  value 13.325689 \n#> converged\n\n# These regression coefficients are the pairwise comparisons between Class=1 and =3 with the reference category. The fitted coefficients represent log-odds ratios of the change on classification probabilities when the independent variable changes by one unit. \nprint(ws <- summary(winelogit), digits = 4)\n#> Call:\n#> multinom(formula = rClass ~ Alcohol + Ash + Alcal + Abs + Proline, \n#>     data = wines, maxit = 200)\n#> \n#> Coefficients:\n#>   (Intercept) Alcohol    Ash   Alcal    Abs  Proline\n#> 1     -124.00   6.213 22.849 -3.3478 10.354 0.029383\n#> 3      -46.46   2.927  6.192  0.3032 -7.483 0.005955\n#> \n#> Std. Errors:\n#>   (Intercept) Alcohol    Ash  Alcal    Abs  Proline\n#> 1      0.1139  1.6304 0.1403 1.0161 0.2612 0.032351\n#> 3      0.3588  0.4539 2.8564 0.2566 1.8766 0.003968\n#> \n#> Residual Deviance: 26.65138 \n#> AIC: 50.65138\n\ntratio <- ws$coefficients / ws$standard.errors\n# Compute two-tailed p values from the t with $edf = effective df\nround(2 * (1 - pt(abs(tratio), df = ws$edf)), digits = 4)\n#>   (Intercept) Alcohol   Ash  Alcal    Abs Proline\n#> 1           0  0.0025 0.000 0.0064 0.0000  0.3816\n#> 3           0  0.0000 0.051 0.2603 0.0018  0.1592\n# Each row is the true class membership and the estimated probability of class membership for this wine.\nround(cbind(wines$Class,ws$fitted.values)[c(1:5, 101:105, 171:175), ], digits = 4)\n#>            2      1      3\n#> 1   1 0.0000 1.0000 0.0000\n#> 2   1 0.0000 1.0000 0.0000\n#> 3   1 0.0000 1.0000 0.0000\n#> 4   1 0.0000 1.0000 0.0000\n#> 5   1 0.0042 0.9944 0.0014\n#> 101 2 1.0000 0.0000 0.0000\n#> 102 2 0.9999 0.0000 0.0001\n#> 103 2 1.0000 0.0000 0.0000\n#> 104 2 1.0000 0.0000 0.0000\n#> 105 2 1.0000 0.0000 0.0000\n#> 171 3 0.7819 0.0000 0.2181\n#> 172 3 0.1745 0.0000 0.8255\n#> 173 3 0.0005 0.0000 0.9995\n#> 174 3 0.0016 0.0000 0.9984\n#> 175 3 0.0004 0.0000 0.9996"},{"path":"chapter-10-discrimination-and-classification.html","id":"graphical-diagnostics","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.2.3 Graphical diagnostics","text":"","code":"\ncolors <- c(\"green\", \"red\", \"blue\")[wines$Class]\nplot(winelogit$fitted.values[, 2 : 3], \n     col = colors, cex = 1.25, cex.lab=1.5,\n     xlab = \"Fitted  probability of  y=1\",  \n     ylab = \"Fitted probability of y=3\", main = \"multinomial logistic\")\ntext( rep(.42, 4), c(1, .92,  .84, .76), pos = rep(4, 4),\n      col = c(\"black\", \"green\", \"red\", \"blue\"),\n      labels = c(\"Observation codes:\", \"y=1 category\", \n                 \"y=2 category (ref)\", \"y=3 category\"))\nlines( c(.4, .4, 1.2), c(1.2, .7, .7))\n\nlines(c(0, 1), c(1, 0), lty = \"dotted\")\nlines(c(0, 0), c(1, 0), lty = \"dotted\")\nlines(c(1, 0), c(0, 0), lty = \"dotted\")\nnmlr <- function(fit, obs){ # normalized  multivariate logistic residuals\n  nmlr <- matrix(0, nrow = dim(fit)[1], \n                 ncol = dim(fit)[2])  # initialize \n  for (i in 1 : (dim(fit)[1])){\n    for (j in 1 : (dim(fit)[2])){\n      p <- fit[i, j]              # fitted p parameter\n      ob <- obs[i, j]\n      res <- (ob - p)  /  (sqrt( p * (1 - p) ) )\n      nmlr[i, j] <- res\n    }\n  }\n  nmlr\n}\n###  Plot of standardized residuals\nobs <- cbind(as.integer(wines$Class == 1),\n             as.integer(wines$Class == 3))\ncolnames(obs) = c(\"1\", \"3\")\nres <- nmlr(winelogit$fitted.values[, 2 : 3], obs)\nplot(res, col = colors, cex = 1.25,\n     xlab = \"Std residual for fitted probability of y=1\",\n     ylab = \"Std residual for y=3\")"},{"path":"chapter-10-discrimination-and-classification.html","id":"linear-discriminant-analysis","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.3 10.3 Linear Discriminant Analysis","text":"Linear discrimination analysis allows us identify linear combination variables can used clearly identify group membership individuals.Linear discrimination analysis allows us identify linear combination variables can used clearly identify group membership individuals.linear discriminant analysis assumes vector explanatory variables \\(\\vec x\\) multivariate normal distributions different means group covariance across groups.linear discriminant analysis assumes vector explanatory variables \\(\\vec x\\) multivariate normal distributions different means group covariance across groups.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"wine-data-example","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.3.1 Wine data example","text":"apply linear discrimination analysis wine data plot resulting groups colors identifying Class number.","code":"\nwines <- read.table(\"dataset/Wines.txt\", header = TRUE)\ncolors <- c(\"green\", \"red\", \"blue\")[wines$Class]\nwines$Class <- as.factor(wines$Class) \n\nlibrary(MASS)\nld <- lda(Class ~ ., data = wines)\n\n# The first and second discriminators are linear combinations of variables that best discriminate between the three cultivars of wines. The linear weights are listed as LD1 and LD2 in the output:\nld\n#> Call:\n#> lda(Class ~ ., data = wines)\n#> \n#> Prior probabilities of groups:\n#>         1         2         3 \n#> 0.3314607 0.3988764 0.2696629 \n#> \n#> Group means:\n#>    Alcohol    Malic      Ash    Alcal       Mg   Phenol\n#> 1 13.74475 2.010678 2.455593 17.03729 106.3390 2.840169\n#> 2 12.27873 1.932676 2.244789 20.23803  94.5493 2.258873\n#> 3 13.15375 3.333750 2.437083 21.41667  99.3125 1.678750\n#>        Flav     Nonf    Proan    Color       Hue      Abs\n#> 1 2.9823729 0.290000 1.899322 5.528305 1.0620339 3.157797\n#> 2 2.0808451 0.363662 1.630282 3.086620 1.0563380 2.785352\n#> 3 0.7814583 0.447500 1.153542 7.396250 0.6827083 1.683542\n#>     Proline\n#> 1 1115.7119\n#> 2  519.5070\n#> 3  629.8958\n#> \n#> Coefficients of linear discriminants:\n#>                  LD1           LD2\n#> Alcohol -0.403274956  0.8718833272\n#> Malic    0.165185223  0.3051811048\n#> Ash     -0.368792093  2.3459219420\n#> Alcal    0.154783909 -0.1463931519\n#> Mg      -0.002162757 -0.0004611477\n#> Phenol   0.617931702 -0.0324979420\n#> Flav    -1.661172871 -0.4916834144\n#> Nonf    -1.495756932 -1.6303752589\n#> Proan    0.134093115 -0.3070371492\n#> Color    0.355006846  0.2530559406\n#> Hue     -0.819785218 -1.5182643908\n#> Abs     -1.157612096  0.0512054337\n#> Proline -0.002690475  0.0028540202\n#> \n#> Proportion of trace:\n#>    LD1    LD2 \n#> 0.6875 0.3125\n\n# The loadings are the linear combinations of the explanatory variables,\nloading <- as.matrix(wines[, 2 : 14])  %*%   ld$scaling # ld$scaling: coefficients\nhead(loading)\n#>            LD1      LD2\n#> [1,] -13.93059 16.62038\n#> [2,] -13.53241 15.81159\n#> [3,] -12.65109 16.07028\n#> [4,] -13.43569 18.64437\n#> [5,] -10.74060 15.09195\n#> [6,] -13.74883 17.85450\n\n# Thie following figure illustrates a clear distinction between the three cultivars.\nplot(loading, col = colors, pch = 16, cex = 2,\n     xlab = \"First linear discriminator\",\n     ylab = \"Second linear discriminator\")\nfor (i in 1 : 3){       # add class number to each centroid\n  centx <- mean(loading[wines[, 1] == i, ] [, 1] )\n  centy <- mean(loading[wines[,1] == i, ] [, 2] )\n  text(centx, centy, i, cex = 2)\n}\n\n# The fitted (posterior) estimated probabilities of group membership can be obtained as predict(ld)$posterior\nldp <- predict(ld)$posterior\n\n# From the following plot, we can see the plot for LDA is much better than the corresponding one for the multinomial logistic regression (under stronger assumption, which needs to be checked before analysis!)\nplot(ldp[, 1], ldp[, 3], col = colors, cex = 1.25,\n     xlab = \"Fitted probability of y=1\", cex.lab=1.5,\n     ylab = \"Fitted probability of y=3\", main = \"LDA\")\ntext( rep(.42, 4), c(1, .92,  .84, .76), pos = rep(4, 4),\n      col = c(\"black\", \"green\", \"red\", \"blue\"), cex=1.25,\n      labels = c(\"Observation codes:\", \"y=1 category\", \n                 \"y=2 category (ref)\", \"y=3 category\"))\nlines( c(.4, .4, 1.2), c(1.2, .7, .7))\n\nlines(c(0, 1), c(1, 0), lty = \"dotted\")\nlines(c(0, 0), c(1, 0), lty = \"dotted\")\nlines(c(1, 0), c(0, 0), lty = \"dotted\")"},{"path":"chapter-10-discrimination-and-classification.html","id":"methodology","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.3.2 Methodology","text":"Suppose \\(K\\) different populations (\\(K = 2, 3, \\dots\\)) population expressible \\(p\\)-dimensional normal population respective means \\(\\vec \\mu_j\\) (\\(j = 1,\\dots,K\\)) covariance matrix \\(\\mathbf{\\Sigma}\\).\\[\\vec X \\sim N_p(\\vec \\mu_j, \\mathbf{\\Sigma}), \\vec X \\text{ Population } j, j = 1,\\dots,K\\]","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"prior-probabilities","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.3.2.1 Prior probabilities:","text":"Let \\(\\pi_j\\) denote probability randomly selected observation sampled Population \\(j\\).\\[\\Pr(\\text{Population } j)=\\pi_j\\]parameters \\(\\pi_j\\) called prior probabilities represent characteristics population known us data observed.can estimated data, frequently case practice. wine data example, prior probabilities observed sample proportions three wine cultivars.means \\(\\vec \\mu_j\\) covariance matrix \\(\\mathbf{\\Sigma}\\) common populations assumed known, , practice, also estimated data.Given \\(p\\)-dimensional observation \\(\\vec x\\), conditional density function \\(\\vec x\\) given sampled Population \\(j\\) \n\\[f(\\vec x|\\text{Population } j)=f(\\vec x|\\vec \\mu_j, \\mathbf{\\Sigma})\\]\nmultivariate normal density function mean \\(\\vec \\mu_j\\) covariance matrix \\(\\mathbf{\\Sigma}\\).joint density observing \\(\\vec x\\) Population \\(j\\) \n\\[f(\\vec x \\text{ sampled Population } j ) = \\pi_jf(\\vec x|\\vec \\mu_j, \\mathbf{\\Sigma})\\]marginal density value \\(\\vec x\\) sampled unspecified population:\n\\[f(\\vec x) =\\sum_{j=1}^K\\pi_jf(\\vec x|\\vec \\mu_j, \\mathbf{\\Sigma})\\]\nsum possible population values \\(j = 1,\\dots,K\\).posterior probability sampling Population \\(k\\) given observation \\(\\vec x\\) \n\\[f(\\text{Population } k| \\vec x) = \\dfrac{\\pi_kf(\\vec x|\\vec \\mu_k, \\mathbf{\\Sigma})}{\\sum_{j=1}^K\\pi_jf(\\vec x|\\vec \\mu_j, \\mathbf{\\Sigma})}\\]maximum posteriori (MAP) estimator \\(\\hat k\\) directs us choose value \\(k\\) makes posterior probability large possible (also called Bayes classifier.) Since denominator every value \\(k\\) MAP estimator \\(k\\) value maximizes numerator.\\[\\begin{aligned}\n\\hat k=&\\underset{k}{argmax} \\ \\pi_kf(\\vec x|\\vec \\mu_k, \\mathbf{\\Sigma})\\\\\n=&\\underset{k}{argmax} \\ \\log\\{\\pi_kf(\\vec x|\\vec \\mu_k, \\mathbf{\\Sigma})\\}\\\\\n=&\\underset{k}{argmax} \\ \\log(\\pi_k)-\\log\\{(2\\pi)^{p/2}|\\mathbf{\\Sigma}^{1/2}|\\}-\\dfrac{1}{2}(\\vec x-\\vec \\mu_k)'\\mathbf{\\Sigma}^{-1}(\\vec x-\\vec \\mu_k)\\\\\n=&\\underset{k}{argmax} \\ \\log(\\pi_k)-\\dfrac{1}{2}(\\vec x-\\vec \\mu_k)'\\mathbf{\\Sigma}^{-1}(\\vec x-\\vec \\mu_k)\\\\\n=&\\underset{k}{argmax} \\ \\log(\\pi_k)-\\dfrac{1}{2}\\vec x'\\mathbf{\\Sigma}^{-1}\\vec x-\\dfrac{1}{2}\\vec \\mu_k'\\mathbf{\\Sigma}^{-1}\\vec \\mu_k+ \\vec x'\\mathbf{\\Sigma}^{-1}\\vec \\mu_k\\\\\n=&\\underset{k}{argmax} \\ \\log(\\pi_k)-\\dfrac{1}{2}\\vec \\mu_k'\\mathbf{\\Sigma}^{-1}\\vec \\mu_k+ \\vec x'\\mathbf{\\Sigma}^{-1}\\vec \\mu_k\n\\end{aligned}\\]Finally, discriminant function LDA, defined \n\\[\\delta_k(\\vec x)=\\log(\\pi_k)-\\dfrac{1}{2}\\vec \\mu_k'\\mathbf{\\Sigma}^{-1}\\vec \\mu_k+ \\vec x'\\mathbf{\\Sigma}^{-1}\\vec \\mu_k\\]\nlinear function \\(\\vec x\\), leading us use name, linear discrimination.practice, parameters (\\(\\mu_k, \\mathbf{\\Sigma}\\)) replaced usual estimators (Lecture 6.)linear discrimination process distinguish two different populations denoted \\(j\\) \\(k\\)?boundary process occurs values \\(\\vec x\\) \\(\\delta_j(\\vec x) = \\delta_k(\\vec x)\\). , solutions equation set hypothetical data values \\(\\vec x\\) provide equal amounts evidence different populations.solutions equation \\(\\delta_j(\\vec x) = \\delta_k(\\vec x)\\) occur values \\(\\vec x\\) \n\n\\[\\log(\\pi_k/\\pi_j)-\\dfrac{1}{2}(\\vec \\mu_k+\\vec \\mu_j)'\\mathbf{\\Sigma}^{-1}(\\vec \\mu_k-\\vec \\mu_j)+ \\vec x'\\mathbf{\\Sigma}^{-1}(\\vec \\mu_k-\\vec \\mu_j)\\]\nsimply,\nConstant + x\n\\[\\text{Constant }+ \\vec x'\\mathbf{\\Sigma}^{-1}(\\vec \\mu_k-\\vec \\mu_j)\\]\nConstant depends \\(j\\) \\(k\\) \\(\\vec x\\). Obviously, linear boundary.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"an-illustrative-example-mixture-of-two-normal-distributions","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.3.3 An illustrative example (mixture of two normal distributions)","text":"Prior probabilities: \\(\\pi_1=0.6, \\pi_2=0.4\\)Popolation means covariance matrix:\\[\\mu_1=\\begin{bmatrix}\n-1\\\\\n-1\n\\end{bmatrix},\n\\mu_2=\\begin{bmatrix}\n1\\\\\n1\n\\end{bmatrix},\n\\mathbf{\\Sigma}=\\begin{bmatrix}\n1 & -0.4\\\\\n-0.4 & 1.2\n\\end{bmatrix}\\]","code":"\nlibrary(MVA)\n#> Loading required package: HSAUR2\n#> Loading required package: tools\nlibrary(mvtnorm)\n\npi1 <- .6        # prior probilities\npi2 <- .4\nmu1 <- c(-1, -1) # mean vectors\nmu2 <- c(1, 1)\ncov <- matrix( c( 1, -.4, -.4, 1.2), 2, 2)\n\ndel <- .1      # how fine the grid\nlim <- 3.2     # normals plotted on +/- lim\nx <- seq(-lim, lim, del)\ny <- x\n\n# bivariate normal density for contours\ncorcon <- function(x, y, mu1, mu2, cov){\n  nx <- length(x)\n  ny <- length(y)\n  z <- matrix(rep(0, nx * ny), nx, ny)  # build lattice\n  for (i in 1 : nx){\n    for (j in 1 : ny){\n      xy <- c( x[i], y[j] )\n      z[i,j] <- pi1 * dmvnorm(xy, mu1, cov) + \n        pi2 * dmvnorm(xy, mu2, cov)\n    }\n  }\n  z\n}\nz <- corcon(x, y, mu1, mu2, cov) \n\n# contour plot\ncontour(x, y, z, drawlabels = FALSE, axes = FALSE, \n        frame = TRUE, col = \"blue\", lwd = 2)\n# boundary\nsinv <- solve(cov)\nconst <- log(pi1 / pi2) -.5 * (mu1 + mu2) %*% sinv %*% (mu1 - mu2)\nline.coef <- sinv %*% (mu1 - mu2)\nabline(a=-1/line.coef[2], b=-line.coef[1]/line.coef[2], col=\"red\")\n\n# perspective plot\n# The front/left normal (with pi1 = 0.6) is a little taller than the second population with pi2 = 0.4. \npersp(x, y, z, r=1,\n      axes = FALSE, xlab=\" \", ylab=\" \", box = FALSE, \n      col = \"green\", shade = .05)"},{"path":"chapter-10-discrimination-and-classification.html","id":"fisher-linear-discriminant-function","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.3.3.1 Fisher linear discriminant function","text":"Ignoring thr constant term boundary function, Fisher linear discriminant function populations \\(j\\) \\(k\\) \n\\[L_{jk}(x)=\\vec x'\\mathbf{S}^{-1}(\\vec{\\bar x}_k-\\vec{\\bar x}_j).\\]Population means \\(\\mu_k\\) replaced sampled within-group means. covariance matrix \\(\\mathbf{\\Sigma}\\) replaced sample covariance matrix \\(\\mathbf{S}\\) assuming covariance groups.fisher discriminator scalar-valued, linear function vector data values, \\(\\vec x\\). generally, discriminating \\(K\\) groups, () \\(K-1\\) linear discriminators.wine data Example, two linear discriminators.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"quadratic-discriminant-analysis-qda","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.3.4 Quadratic Discriminant Analysis (QDA)","text":"Unlike LDA, QDA assumes class covariance matrix:\n\\[\\vec X \\sim N_p(\\vec \\mu_k, \\mathbf{\\Sigma}_k)\\]assumption, Bayes classifier assigns observation \\(\\vec X = \\vec x\\) class \n\\[\\delta_k(\\vec x)=\\log(\\pi_k) - \\dfrac{1}{2}\\log(|\\mathbf{\\Sigma_k}|)-\\dfrac{1}{2}\\vec x'\\mathbf{\\Sigma_k}^{-1}\\vec x+ \\vec x'\\mathbf{\\Sigma_k}^{-1}\\vec \\mu_k-\\dfrac{1}{2}\\vec \\mu_k'\\mathbf{\\Sigma_k}^{-1}\\vec \\mu_k\\]\nlargest.","code":"\nwines <- read.table(\"dataset/Wines.txt\", header = TRUE)\ncolors <- c(\"green\", \"red\", \"blue\")[wines$Class]\nwines$Class <- as.factor(wines$Class) \n\nlibrary(MASS)\nqd <- qda(Class ~ ., data = wines)\nqdp <- predict(qd)$posterior\n\nplot(qdp[, 1], qdp[, 3], col = colors, cex = 1.25,\n     xlab = \"Fitted probability of y=1\", cex.lab=1.5,\n     ylab = \"Fitted probability of y=3\", main = \"QDA\")\ntext( rep(.42, 4), c(1, .92,  .84, .76), pos = rep(4, 4),\n      col = c(\"black\", \"green\", \"red\", \"blue\"), cex=1.25,\n      labels = c(\"Observation codes:\", \"y=1 category\", \n                 \"y=2 category (ref)\", \"y=3 category\"))\nlines( c(.4, .4, 1.2), c(1.2, .7, .7))\n\nlines(c(0, 1), c(1, 0), lty = \"dotted\")\nlines(c(0, 0), c(1, 0), lty = \"dotted\")\nlines(c(1, 0), c(0, 0), lty = \"dotted\")"},{"path":"chapter-10-discrimination-and-classification.html","id":"support-vector-machine","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4 10.4 Support Vector Machine","text":"support vector machine (SVM) group classification algorithms \ninclude variety parametric nonparametric models:\n* Parametric models include straight lines similar regression methods.\n* nonparametric methods contain variety kernel smoothing techniques.programs R include features estimate classification probabilities using cross validation well training samples machine learning.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"example-linear-svm-classifier","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.1 Example (linear SVM classifier)","text":"Line completely classify red blue groups samples.Line B classifies correctly small margins misclassification.Line C, product SVM, provides largest possible margins. margins given dotted lines.Support vectors:\n(1) observations closest margins important determination classification regions\n(2) observations define lie periphery regions different colored points","code":"\nlibrary(kernlab)\nsvmex <- read.table(file = \"dataset/svmex.txt\", header = TRUE, \n                    row.names = 1) # artifical data\nsvmex <- svmex[svmex[, 1] > -2, ]  # omit outlier\ntype <- 1 + (svmex[, 1] + svmex[, 2] > 0)      # known types\nplot(svmex[, 1 : 2], pch = 16, cex = 1.75,\n     col = c(\"red\", \"blue\")[type], xlab = \" \", \n     ylab = \" \", xaxt = \"n\", yaxt = \"n\")\n\n# line A: does not discriminate completely\nlines(c(-.5,.5), c(-2.5, 3.25), type = \"l\", \n      col = \"blue\") \ntext(.25, 2.5, label = \"A\", cex = 1.2)\n\n# line B: is not maximum margins\nlines(c(-.5, .5), c(3.25, -2.5), type = \"l\", col = \"blue\")\ntext(-.5, 2.5, labels = \"B\")\n\nsv <- ksvm(type ~ . , data = svmex, type = \"C-svc\", \n           prob.model = TRUE)\n# Extract a and b from the model    \na <- colSums(coef(sv)[[1]] * svmex[SVindex(sv), ])\nb <- b(sv)\nabline( b / a[1],  -a[2] / a[1], \n        col = \"blue\") # maximum margin line\ntext(-1.4, 1.7, labels = \"C\")\nabline( (b + 6.9) / a[1], -a[2] / a[1], lty = 2) # upper margin\nabline( (b - 6.9) / a[1], -a[2] / a[1], lty = 2) # lower margin\n\n# circle the support vectors\nai <- alphaindex(sv)[[1]]    # indices for support vectors\npoints(svmex[ai, ], pch = 1, cex = 2.00)   # three thick circles\npoints(svmex[ai, ], pch = 1, cex = 2.25)\npoints(svmex[ai, ], pch = 1, cex = 2.50)"},{"path":"chapter-10-discrimination-and-classification.html","id":"example-kernel-svm-classifier","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.2 Example (kernel SVM classifier)","text":"Kernel smoothing makes assumption shape regions. functional form boundary two regions specified, SVM program clearly locates .Kernel smoothing makes assumption shape regions. functional form boundary two regions specified, SVM program clearly locates .kernel smooth produced SVM classifies future observations confidence based intensity plotted color. lighter-colored regions low confidence white represents complete indifference, wavy region across center.kernel smooth produced SVM classifies future observations confidence based intensity plotted color. lighter-colored regions low confidence white represents complete indifference, wavy region across center.groups sine wave plotted circles triangles, respectively. observations identified support vectors. critical estimating boundaries plotted solid.groups sine wave plotted circles triangles, respectively. observations identified support vectors. critical estimating boundaries plotted solid.","code":"\nlibrary(kernlab)\nlibrary(mvtnorm)\nset.seed(123)\nx <- rmvnorm(n = 250, mean = c(0, 0), \n             sigma = matrix(c(1, .1, .1, 1), 2 , 2))\nwavy <- sin(pi * x[, 1]) / 2               # wavy boundary\ninclude <- (abs( wavy - x[, 2]) > .25 )# 2x width of boundary\nx <- x[include, ]                      # include those inside \nwavy <- wavy[include]                 #    the wavy boundary\ngroup <- 1 + (x[, 2] > wavy)          # above or below boundary?\nall <- data.frame(x = x[, c(2, 1)], \n                  group = group)                     # build a data.frame\ncolnames(all) <- c(\"x2\", \"x1\", \"group\")\n\nsv <- ksvm(group ~ . , data = all, type = \"C-svc\")\n\nx1.lim <- range(all$x1)\nx2.lim <- range(all$x2)\nplot(sv, data = all, xlim = x1.lim, ylim = x2.lim)"},{"path":"chapter-10-discrimination-and-classification.html","id":"cross-validation","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.3 Cross Validation","text":"Cross Validation method testing accuracy classification modelCross Validation method testing accuracy classification modelHow well model correctly classify future observations part model building process?well model correctly classify future observations part model building process?\\(K\\)-fold cross validation randomly partitions data set \\(K\\) nearly equally sized sets. (Typically, \\(K\\) integer range 5 10.)\\(K\\)-fold cross validation randomly partitions data set \\(K\\) nearly equally sized sets. (Typically, \\(K\\) integer range 5 10.)One set, called testing data set, omitted.One set, called testing data set, omitted.model fitted using remaining \\(K-1\\) sets, referred training data set.model fitted using remaining \\(K-1\\) sets, referred training data set.training data set fitted, cross validation examines well resulting model classifies observations omitted, testing data set.training data set fitted, cross validation examines well resulting model classifies observations omitted, testing data set.cross-validation process systematically replaces omitted\ndata removes different set. process repeated, separately fitting\ntesting \\(K\\) different models.cross-validation process systematically replaces omitted\ndata removes different set. process repeated, separately fitting\ntesting \\(K\\) different models.process cross validation produces estimate accuracy model also hidden benefit, referred machine learning. process producing \\(K\\)-fold estimate classification error, SVC (support vector classifier) also fit \\(K\\) different models. models may better properties original model fitted entire data. manner, SVM appears learn better models.process cross validation produces estimate accuracy model also hidden benefit, referred machine learning. process producing \\(K\\)-fold estimate classification error, SVC (support vector classifier) also fit \\(K\\) different models. models may better properties original model fitted entire data. manner, SVM appears learn better models.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"application-to-the-wine-data","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.4 Application to the wine data","text":"residual plot useful exhibits distorted horizontal scale, owing small probabilities involved.Pros:\n* extremely flexible\n* able detect wavy boundary without additional specification\n* many options different kernel smoothing functions available SVM method.Cons:\n* SVM produces estimates model can expressed mathematical formula.","code":"\nwines <- read.table(\"dataset/Wines.txt\", header = TRUE)\nlevels(wines$Class) <- 1 : 3     # grouping variable\nclass(wines$Class) <- \"factor\"   # classify the variable type\ncolors <- c(\"green\", \"red\",\" blue\")[wines$Class]\n\nsvw <- ksvm(Class ~ . , data = wines, type = \"C-svc\", \n            prob.model = T)\n\n# The alphaindex() command produces three separate lists of support vectors: one for each of the three cultivars. There may be some duplication when support vectors are on the boundary for more than one of the three group.\nsvi <- ( c(alphaindex(svw)[[1]], alphaindex(svw)[[2]], \n           alphaindex(svw)[[3]]))   # list of all support vectors\nsvi <- svi[ !duplicated(svi)]    # remove duplicates\n\nsvwine <- wines[svi, ]           # subset support vectors\nsvwine[, 1] <- jitter(as.integer(svwine[, 1])) # jitter Class\nsvcol <- colors[svi]             # and their colors\n\npairs(svwine, pch = 16, cex = .3, gap = 0, col = svcol, \n      xaxt = \"n\", yaxt = \"n\") # 69 observations out of 178 are support vectors\n\n# 5-fold CV\n(svw.CV <- ksvm(Class ~ . , data = wines, \n                type = \"C-svc\", cross = 5))\n#> Support Vector Machine object of class \"ksvm\" \n#> \n#> SV type: C-svc  (classification) \n#>  parameter : cost C = 1 \n#> \n#> Gaussian Radial Basis kernel function. \n#>  Hyperparameter : sigma =  0.0615286159790621 \n#> \n#> Number of Support Vectors : 68 \n#> \n#> Objective Function Value : -12.774 -4.4679 -12.7795 \n#> Training error : 0 \n#> Cross validation error : 0.022381\n\n# prob.model = T: produce estimated multinomial prediction probabilities of individual observations belonging to each of the different groups.\nprint(predict(svw, wines[, -1], \n              type = \"probabilities\"), digits = 2)\n#>              1       2       3\n#>   [1,] 0.99179 0.00304 0.00517\n#>   [2,] 0.98183 0.01064 0.00753\n#>   [3,] 0.99254 0.00358 0.00389\n#>   [4,] 0.97070 0.01823 0.01108\n#>   [5,] 0.97536 0.01766 0.00698\n#>   [6,] 0.99636 0.00106 0.00259\n#>   [7,] 0.99633 0.00054 0.00313\n#>   [8,] 0.99375 0.00105 0.00520\n#>   [9,] 0.99287 0.00320 0.00393\n#>  [10,] 0.99494 0.00206 0.00300\n#>  [11,] 0.99445 0.00208 0.00347\n#>  [12,] 0.99033 0.00319 0.00648\n#>  [13,] 0.99667 0.00085 0.00247\n#>  [14,] 0.97139 0.01806 0.01055\n#>  [15,] 0.97454 0.01470 0.01076\n#>  [16,] 0.99315 0.00194 0.00491\n#>  [17,] 0.99236 0.00195 0.00569\n#>  [18,] 0.99169 0.00335 0.00496\n#>  [19,] 0.97091 0.01821 0.01088\n#>  [20,] 0.99531 0.00125 0.00345\n#>  [21,] 0.98287 0.01096 0.00618\n#>  [22,] 0.97463 0.01760 0.00777\n#>  [23,] 0.99660 0.00114 0.00227\n#>  [24,] 0.97647 0.01757 0.00596\n#>  [25,] 0.97589 0.01870 0.00541\n#>  [26,] 0.87598 0.11443 0.00959\n#>  [27,] 0.99553 0.00109 0.00338\n#>  [28,] 0.97540 0.01493 0.00967\n#>  [29,] 0.99504 0.00232 0.00265\n#>  [30,] 0.99552 0.00184 0.00264\n#>  [31,] 0.97369 0.01781 0.00850\n#>  [32,] 0.99398 0.00209 0.00393\n#>  [33,] 0.98856 0.00792 0.00352\n#>  [34,] 0.97230 0.01780 0.00990\n#>  [35,] 0.99326 0.00177 0.00496\n#>  [36,] 0.98953 0.00769 0.00278\n#>  [37,] 0.98211 0.00830 0.00959\n#>  [38,] 0.97270 0.01766 0.00963\n#>  [39,] 0.90030 0.09082 0.00888\n#>  [40,] 0.97118 0.01812 0.01069\n#>  [41,] 0.99081 0.00583 0.00336\n#>  [42,] 0.97244 0.01775 0.00981\n#>  [43,] 0.99461 0.00177 0.00362\n#>  [44,] 0.95655 0.03276 0.01069\n#>  [45,] 0.97768 0.01908 0.00323\n#>  [46,] 0.98714 0.00312 0.00974\n#>  [47,] 0.99708 0.00105 0.00187\n#>  [48,] 0.99415 0.00325 0.00259\n#>  [49,] 0.99584 0.00127 0.00289\n#>  [50,] 0.99339 0.00231 0.00430\n#>  [51,] 0.96385 0.02591 0.01024\n#>  [52,] 0.99495 0.00140 0.00365\n#>  [53,] 0.99067 0.00495 0.00438\n#>  [54,] 0.99642 0.00072 0.00286\n#>  [55,] 0.99369 0.00170 0.00461\n#>  [56,] 0.98562 0.00812 0.00626\n#>  [57,] 0.99524 0.00167 0.00309\n#>  [58,] 0.99732 0.00060 0.00208\n#>  [59,] 0.99211 0.00348 0.00441\n#>  [60,] 0.01343 0.97274 0.01383\n#>  [61,] 0.00312 0.98886 0.00802\n#>  [62,] 0.00329 0.95196 0.04476\n#>  [63,] 0.01480 0.97974 0.00546\n#>  [64,] 0.00240 0.99269 0.00491\n#>  [65,] 0.00211 0.99543 0.00246\n#>  [66,] 0.06241 0.93281 0.00478\n#>  [67,] 0.01650 0.97573 0.00777\n#>  [68,] 0.00320 0.99439 0.00241\n#>  [69,] 0.01184 0.97232 0.01584\n#>  [70,] 0.01389 0.97352 0.01258\n#>  [71,] 0.00751 0.94137 0.05112\n#>  [72,] 0.01665 0.97478 0.00858\n#>  [73,] 0.00744 0.97997 0.01258\n#>  [74,] 0.01669 0.97437 0.00894\n#>  [75,] 0.01635 0.97750 0.00615\n#>  [76,] 0.00213 0.99513 0.00274\n#>  [77,] 0.00405 0.99233 0.00362\n#>  [78,] 0.00490 0.98216 0.01294\n#>  [79,] 0.01601 0.97337 0.01061\n#>  [80,] 0.01603 0.98158 0.00239\n#>  [81,] 0.00068 0.99784 0.00148\n#>  [82,] 0.10655 0.89070 0.00274\n#>  [83,] 0.00178 0.99598 0.00224\n#>  [84,] 0.01098 0.63121 0.35781\n#>  [85,] 0.01131 0.98254 0.00615\n#>  [86,] 0.00695 0.99189 0.00116\n#>  [87,] 0.00072 0.99813 0.00116\n#>  [88,] 0.00279 0.99426 0.00294\n#>  [89,] 0.00213 0.99525 0.00262\n#>  [90,] 0.00158 0.99652 0.00190\n#>  [91,] 0.00120 0.99653 0.00226\n#>  [92,] 0.00131 0.99322 0.00548\n#>  [93,] 0.00224 0.98503 0.01273\n#>  [94,] 0.00341 0.99561 0.00098\n#>  [95,] 0.00238 0.99462 0.00300\n#>  [96,] 0.01639 0.97205 0.01156\n#>  [97,] 0.01163 0.97446 0.01390\n#>  [98,] 0.00212 0.99681 0.00107\n#>  [99,] 0.01621 0.97832 0.00547\n#> [100,] 0.00674 0.98867 0.00459\n#> [101,] 0.01417 0.98321 0.00262\n#> [102,] 0.00340 0.99441 0.00220\n#> [103,] 0.01596 0.98005 0.00399\n#> [104,] 0.00067 0.99810 0.00123\n#> [105,] 0.00951 0.98910 0.00139\n#> [106,] 0.00342 0.99003 0.00655\n#> [107,] 0.00317 0.99565 0.00118\n#> [108,] 0.00236 0.99009 0.00755\n#> [109,] 0.00121 0.99740 0.00138\n#> [110,] 0.01609 0.98074 0.00316\n#> [111,] 0.01658 0.97255 0.01087\n#> [112,] 0.00157 0.99705 0.00139\n#> [113,] 0.01465 0.97192 0.01343\n#> [114,] 0.00081 0.99644 0.00276\n#> [115,] 0.00157 0.99613 0.00230\n#> [116,] 0.00390 0.98605 0.01005\n#> [117,] 0.00106 0.99796 0.00098\n#> [118,] 0.00123 0.99796 0.00081\n#> [119,] 0.00964 0.93923 0.05113\n#> [120,] 0.00550 0.99305 0.00144\n#> [121,] 0.01328 0.98465 0.00207\n#> [122,] 0.01670 0.97311 0.01019\n#> [123,] 0.01207 0.97584 0.01209\n#> [124,] 0.01673 0.97337 0.00990\n#> [125,] 0.01076 0.98275 0.00650\n#> [126,] 0.00131 0.99795 0.00074\n#> [127,] 0.00370 0.99251 0.00379\n#> [128,] 0.00633 0.98155 0.01213\n#> [129,] 0.00070 0.99828 0.00102\n#> [130,] 0.00596 0.98104 0.01300\n#> [131,] 0.01086 0.23991 0.74923\n#> [132,] 0.00419 0.01436 0.98145\n#> [133,] 0.00432 0.01668 0.97900\n#> [134,] 0.00861 0.02351 0.96788\n#> [135,] 0.00898 0.22868 0.76234\n#> [136,] 0.00504 0.02333 0.97163\n#> [137,] 0.00629 0.02247 0.97124\n#> [138,] 0.01062 0.02348 0.96591\n#> [139,] 0.00421 0.02231 0.97348\n#> [140,] 0.01056 0.03316 0.95628\n#> [141,] 0.00703 0.02339 0.96959\n#> [142,] 0.01155 0.02380 0.96466\n#> [143,] 0.00742 0.01659 0.97599\n#> [144,] 0.01180 0.04021 0.94800\n#> [145,] 0.01271 0.02416 0.96313\n#> [146,] 0.00662 0.02345 0.96993\n#> [147,] 0.01086 0.02361 0.96553\n#> [148,] 0.00378 0.00266 0.99356\n#> [149,] 0.00256 0.00112 0.99631\n#> [150,] 0.00469 0.00396 0.99135\n#> [151,] 0.00958 0.01653 0.97389\n#> [152,] 0.00878 0.01312 0.97810\n#> [153,] 0.01105 0.02361 0.96534\n#> [154,] 0.01139 0.01739 0.97122\n#> [155,] 0.00639 0.02333 0.97028\n#> [156,] 0.00795 0.01273 0.97931\n#> [157,] 0.00660 0.00720 0.98620\n#> [158,] 0.01081 0.01433 0.97486\n#> [159,] 0.01221 0.04224 0.94554\n#> [160,] 0.01065 0.02192 0.96744\n#> [161,] 0.00487 0.00862 0.98651\n#> [162,] 0.00748 0.00961 0.98291\n#> [163,] 0.00578 0.01640 0.97781\n#> [164,] 0.00304 0.00561 0.99135\n#> [165,] 0.00467 0.00350 0.99183\n#> [166,] 0.00365 0.00717 0.98917\n#> [167,] 0.00708 0.00630 0.98662\n#> [168,] 0.00591 0.00541 0.98867\n#> [169,] 0.00619 0.00489 0.98892\n#> [170,] 0.01251 0.01622 0.97127\n#> [171,] 0.00316 0.02327 0.97356\n#> [172,] 0.00690 0.00815 0.98495\n#> [173,] 0.00654 0.00354 0.98992\n#> [174,] 0.00723 0.00486 0.98790\n#> [175,] 0.00275 0.00128 0.99596\n#> [176,] 0.01078 0.01043 0.97878\n#> [177,] 0.00805 0.00640 0.98556\n#> [178,] 0.00663 0.00591 0.98746\n\n# Plot fitted probabilities and residuals\n\n# Fitted probabilities:\nprob <- predict(svw, wines[, -1], type = \"probabilities\")\n\nplot(prob[, 1], prob[, 3], col = colors, cex = 1.25, cex.lab = 1.5,\n     xlab = \"Fitted  probability of  y=1\",  \n     ylab = \"Fitted probability of y=3\", main=\"SVM\")\ntext( rep(.42, 4), c(1, .92,  .84, .76), pos = rep(4, 4),\n      col = c(\"black\", \"green\", \"red\", \"blue\"),\n      labels = c(\"Observation codes:\", \"y=1 category\", \n                 \"y=2 category (ref)\", \"y=3 category\"))\nlines( c(.4, .4, 1.2), c(1.2, .7, .7))  #  box around the text\n\nlines(c(0, 1), c(1, 0), lty = \"dotted\")\nlines(c(0, 0), c(1, 0), lty = \"dotted\")\nlines(c(1, 0), c(0, 0), lty = \"dotted\")\n\n# Residual plot\n\nnmlr <- function(fit, obs){ # normalized  multivariate logistic residuals\n  nmlr <- matrix(0,nrow = dim(fit)[1], ncol = dim(fit)[2])  \n  for (i in 1 : (dim(fit)[1])){\n    for (j in 1 : (dim(fit)[2])){\n      p <- fit[i, j]            # fitted p parameter\n      ob <- obs[i, j]\n      res <- (ob - p)  /  (sqrt( p * (1 - p) ) )\n      nmlr[i, j] <- res\n    }\n  }\n  return(nmlr)\n}\n\nobs <- cbind(as.integer(wines$Class == 1),\n             as.integer(wines$Class == 3))\ncolnames(obs) = c(\"1\", \"3\")\nres <- nmlr(prob[, 2 : 3], obs)\nplot(res, col = colors, cex = 1.25,\n     xlab = \"Std residual for fitted probability of y=1\",\n     ylab = \"Std residual for y=3\")"},{"path":"chapter-10-discrimination-and-classification.html","id":"maximal-margin-classifier","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.5 Maximal Margin Classifier","text":"support vector machine generalization simple intuitive classifier called maximal margin classifier.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"what-is-a-hyperplane","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.5.1 What Is a Hyperplane?","text":"\\(p\\)-dimensional space, hyperplane flat subspace dimension \\(p-1\\).two dimensions, hyperplane line, defined equation\n\\[\\beta_0+\\beta_1X_1+\\beta_2X_2=0\\]hyperplane \\(p\\)-dimensional space:\n\\[\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p=0\\]point \\(\\vec X =(X_1,X_2,\\cdots , X_p)^T\\) \\(p\\)-dimensional space (.e. vector length \\(p\\)) satisfies\n\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p=0\\), \\(\\vec X\\) lies hyperplane.point \\(\\vec X =(X_1,X_2,\\cdots , X_p)^T\\) satisfies \\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p<0\\), \\(\\vec X\\) lies one side hyperplane.hand, point \\(\\vec X =(X_1,X_2,\\cdots, X_p)^T\\) satisfies \\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p>0\\), \\(\\vec X\\) lies side hyperplane.Solid line: \\(1+2X_1+3X_2=0\\)Blue: \\(1+2X_1+3X_2>0\\)Purple: \\(1+2X_1+3X_2<0\\)","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"classification-using-a-separating-hyperplane","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.5.2 Classification Using a Separating Hyperplane","text":"Now suppose \\(n\\times p\\) data matrix \\(\\mathbf{X}=\\{x_{ij}, =1,2,\\dots,n,j=1,2,\\dots,p\\}\\) consists \\(n\\) training observations \\(p\\)-dimensional space, observations fall two classes-, \\(y_1,\\dots, y_n \\\\{-1, 1\\}\\) \\(-1\\) represents one class \\(1\\) class.separating hyperplane property \n\\[\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}>0 \\text{ } y_i=1\\]\n\n\\[\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}<0 \\text{ } y_i=-1.\\]\nEquivalently, separating hyperplane property \n\\[y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip})>0\\]\n\\(=1,\\dots,n.\\)","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"the-maximal-margin-classifier","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.5.3 The Maximal Margin Classifier","text":"can compute (perpendicular) distance training observation given separating hyperplane; smallest distance minimal distance observations hyperplane, known margin.maximal margin hyperplane (also known optimal separating hyperplane), separating hyperplane farthest training observations (margin largest). known maximal margin classifier.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"construction-of-the-maximal-margin-classifier","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.5.4 Construction of the Maximal Margin Classifier","text":"maximal margin hyperplane solution optimization problem\\[\n\\begin{aligned}\n& \\underset{\\beta_0,\\dots,\\beta_p}{\\text{maximize}}M\\\\\n& \\text{subject } \\sum_{j=1}^p \\beta_j^2=1,\\\\\n& y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}) \\ge M, \\forall =1,2,\\dots,n\n\\end{aligned}\n\\]Note , \\(\\sum_{j=1}^p \\beta_j^2=1\\), perpendicular distance \\(\\)th observation hyperplane given \n\\[\\dfrac{y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip})}{\\sqrt{y_i^2\\sum_{j=1}^p \\beta_j^2}}=y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}).\\]Hence, \\(M\\) represents margin hyperplane. exactly definition maximal margin hyperplane!","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"support-vector-classifier","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.6 Support Vector Classifier","text":"","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"a-non-separable-case","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.6.1 A non-separable case","text":"use separating hyperplane exactly separate two classes:support vector classifier extension maximal margin classifier non-separable case.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"details-of-the-support-vector-classifier","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.6.2 Details of the Support Vector Classifier","text":"support vector classifier, sometimes called soft margin classifier.Rather seeking largest possible margin every observation correct side hyperplane also correct side margin, instead allow observations incorrect side margin, even incorrect side hyperplane. (margin soft can violated training observations.)solution optimization problem\n\\[\n\\begin{aligned}\n& \\underset{\\beta_0,\\dots,\\beta_p,\\epsilon_1,\\dots,\\epsilon_n}{\\text{maximize}}M\\\\\n& \\text{subject } \\sum_{j=1}^p \\beta_j^2=1,\\\\\n& y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}) \\ge M(1-\\epsilon_i)\\\\\n& \\epsilon_i\\ge0, \\sum_{=1}^n\\epsilon_i\\le C\n\\end{aligned}\n\\]\\(C\\) nonnegative tuning parameter\\(M\\) width margin; seek make quantity large possible.\\(\\epsilon_1,\\dots,\\epsilon_p\\) slack variables allow individual observations wrong side margin hyperplane","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"slack-variable","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.6.2.1 slack variable","text":"slack variable \\(\\epsilon_i\\) tells us \\(\\)th observation located, relative hyperplane relative margin:\n(1) \\(\\epsilon_i=0\\): \\(\\)th observation correct side margin\n(2) \\(\\epsilon_i>0\\): \\(\\)th observation wrong side margin\n(3) \\(\\epsilon_i>1\\): \\(\\)th observation wrong side hyperplane","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"tuning-parameter","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.6.2.2 Tuning parameter","text":"tuning parameter \\(C\\) bounds \\(\\sum_{=1}^n\\epsilon_i\\), determines number severity violations margin (hyperplane) tolerate.can think \\(C\\) budget amount margin can violated n observations.support vector classifier fit using four different values tuning parameter \\(C\\)","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"support-vectot-machine","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.7 Support Vectot Machine","text":"Support vector machine extension support vector classifier order accommodate non-linear class boundaries.first discuss general mechanism converting linear classifier one produces non-linear decision boundaries. introduce support vector machine, automatic way.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"classification-with-non-linear-decision-boundaries","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.7.1 Classification with Non-linear Decision Boundaries","text":"practice, sometimes faced non-linear class boundaries.right panel, support vector classifier seeks linear boundary, consequently performs poorly.address problem possibly non-linear boundaries classes enlarging feature space using quadratic, cubic, even higher-order polynomial functions predictors.instance, rather fitting support vector classifier using \\(p\\) features\n\\[X_1,X_2, \\dots, X_p,\\]\ninstead fit support vector classifier using \\(2p\\) features\n\\[X_1,X_1^2,X_2,X_2^2, \\dots, X_p,X_p^2.\\]non-linear decision boundary led solving following optimization problem\n\\[\n\\begin{aligned}\n& \\underset{\\beta_0,\\beta_{11},\\beta_{12},\\dots,\\beta_{p1},\\beta_{p2},\\epsilon_1,\\dots,\\epsilon_n}{\\text{maximize}}M\\\\\n& \\text{subject } \\sum_{j=1}^p \\sum_{k=1}^2\\beta_{jk}^2=1,\\\\\n& y_i(\\beta_0+\\sum_{j=1}^p\\beta_{j1}x_{ij}+\\sum_{j=1}^p\\beta_{j2}x_{ij}^2) \\ge M(1-\\epsilon_i)\\\\\n& \\epsilon_i\\ge0, \\sum_{=1}^n\\epsilon_i\\le C\n\\end{aligned}\n\\]","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"details-of-the-support-vector-machine","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.7.2 Details of the Support Vector Machine","text":"support vector machine (SVM) extension support vector classifier results enlarging feature space specific way, using kernels.kernel approach describe simply efficient computational approach enlarging feature space.Let \\(\\vec \\beta=(\\beta_1,\\dots,\\beta_p)', \\vec x_i=(x_{i1},\\dots, x_{ip})'\\). , support vector classifier problem :\n\\[\n\\begin{aligned}\n& \\underset{\\beta_0,\\dots,\\beta_p,\\epsilon_1,\\dots,\\epsilon_n}{\\text{maximize}}M\\\\\n& \\text{subject } ||\\vec \\beta||=\\sqrt{\\vec \\beta'\\vec \\beta}=1,\\\\\n& y_i(\\beta_0+\\vec x_i'\\vec\\beta) \\ge M(1-\\epsilon_i)\\\\\n& \\epsilon_i\\ge0, \\sum_{=1}^n\\epsilon_i\\le C\n\\end{aligned}\n\\]equivalently (setting \\(||\\vec \\beta||=1/M\\)),\n\\[\n\\begin{aligned}\n& \\underset{\\beta_0,\\vec \\beta,\\epsilon_1,\\dots,\\epsilon_n}{\\text{minimize}}\\dfrac{1}{2}||\\vec \\beta||^2+C\\sum_{=1}^n\\epsilon_i\\\\\n& \\text{subject } y_i(\\beta_0+\\vec x_i'\\vec\\beta) \\ge 1-\\epsilon_i, \\epsilon_i\\ge0\n\\end{aligned}\n\\]\\(C\\) controls tradeoff maximum margin loss.convex optimization problem, quadratic programming solution using Lagrange multipliers.Lagrange (primal) function:\n\\[L_P=\\dfrac{1}{2}||\\vec \\beta||^2+C\\sum_{=1}^n\\epsilon_i-\\sum_{=1}^n\\alpha_i[y_i(\\beta_0+\\vec x_i'\\vec\\beta) - (1-\\epsilon_i)]-\\sum_{=1}^n\\mu_i\\epsilon_i\\]Taking derivatives respect \\(\\beta_0, \\vec \\beta, \\epsilon_i\\) setting zeo, get Karush-Kuhn-Tucker (KKT) condition 1:\n\\[\n\\begin{aligned}\n&\\vec \\beta=\\sum_{=1}^n\\alpha_iy_ix_i,\\\\\n&0=\\sum_{=1}^n\\alpha_iy_i,\\\\\n&\\alpha_i=C-\\mu_i,\\forall \n\\end{aligned}\n\\]Lagrange (Wolfe) dual objective function:\n\\[L_D=\\sum_{=1}^n\\alpha_i-\\dfrac{1}{2}\\sum_{=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_j\\vec x_i' \\vec x_j\\]\ngives lower bound objective function feasible point., maximize \\(L_D\\) subject KKT condition 1, 2, 3 (http://mypages.iit.edu/~jwang134/posts/KKT-SVs-SVM.html) get unique solution.\\(\\vec \\beta=\\sum_{=1}^n\\alpha_iy_ix_i\\), can represent linear support vector classifier \\[f(\\vec x)=\\beta_0+\\vec x_i'\\vec\\beta=\\beta_0+\\sum_{=1}^n\\alpha_iy_i<\\vec x, \\vec x_i>\\]","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"the-property-of-alpha_i","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.7.3 The property of \\(\\alpha_i\\)","text":"observations \\(\\alpha_i=0\\) contribute calculation \\(\\vec \\beta\\).observations \\(\\alpha_i\\ne 0\\) called support vectors.slack variable \\(\\epsilon_i>0\\), \\(\\alpha_i = C\\)","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"kernel-enlarge-the-feature-space","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.7.4 Kernel (enlarge the feature space)","text":"introduce “bi-vector” function (kernel function \\(K(\\vec x, \\vec x_i)\\)) generalize inner product. , can represent classifier \n\\[f(\\vec x)=\\beta_0+\\sum_{=1}^n\\alpha_iy_iK(\\vec x, \\vec x_i)\\]Three popular choices K SVM literature \\(d\\)th-Degree polynomial: \\(K(\\vec x, \\vec x_i)=(1+<\\vec x, \\vec x_i>)^d\\),(Gaussian) Radial basis: \\(K(\\vec x, \\vec x_i)=\\exp(-\\gamma||\\vec x-\\vec x_i||^2)\\)Neural network: \\(K(\\vec x, \\vec x_i)=\\tanh(\\kappa_1<\\vec x, \\vec x_i>+\\kappa_2)\\)Left: SVM polynomial kernel degree 3 applied non-linear data.Right: SVM radial kernel applied.Solid line: SVMDashed line: margins SVMIn example, either kernel capable capturing decision boundary.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"svms-with-more-than-two-classes-k2-classes","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.7.5 SVMs with More than Two Classes (\\(K>2\\) classes)","text":"","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"one-versus-one-classification","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.7.5.1 One-Versus-One Classification","text":"one-versus-one -pairs approach constructs \\({K \\choose 2}\\) SVMs, compares pair classes.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"one-versus-all-classification","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.7.5.2 One-Versus-All Classification","text":"one-versus-approach alternative procedure applying SVMs one-versusin case \\(K > 2\\) classes. fit \\(K\\) SVMs, time comparing one \\(K\\) classes remaining \\(K-1\\) classes.","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"lab-support-vector-machines","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.8 Lab: Support Vector Machines","text":"textbook “Introduction statistical learning”, use e1071 library R demonstrate support vector classifier SVM.kernlab versus e1071\nhttps://www.thekerneltrip.com/statistics/kernlab-vs-e1071/","code":""},{"path":"chapter-10-discrimination-and-classification.html","id":"support-vector-classifier-1","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.8.1 Support Vector Classifier","text":"","code":"\n## generate the data\nset.seed(1)\nx=matrix(rnorm(20*2), ncol=2)\ny=c(rep(-1,10), rep(1,10))\nx[y==1,]=x[y==1,] + 1\nplot(x, col=(3-y))\n\n# combine x and y into a data frame\ndat=data.frame(x=x, y=as.factor(y))\n\n## load the package\nlibrary(e1071)\n\n## fit the support vector classifier (kernel=\"linear\")\n# A cost argument allows us to specify the cost of a violation to the margin. \n# When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.\n# The argument scale=FALSE tells the svm() function not to scale each feature to have mean zero or standard deviation one\nsvmfit=svm(y~., data=dat, kernel=\"linear\", cost=10,scale=FALSE)\nplot(svmfit, dat)\n# index of support vectors\nsvmfit$index\n#> [1]  1  2  5  7 14 16 17\nsummary(svmfit)\n#> \n#> Call:\n#> svm(formula = y ~ ., data = dat, kernel = \"linear\", \n#>     cost = 10, scale = FALSE)\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  10 \n#> \n#> Number of Support Vectors:  7\n#> \n#>  ( 4 3 )\n#> \n#> \n#> Number of Classes:  2 \n#> \n#> Levels: \n#>  -1 1\n\n# using a smaller cost yields a wider margin and more support vectors\nsvmfit=svm(y~., data=dat, kernel=\"linear\", cost=0.1,scale=FALSE)\nplot(svmfit, dat)\nsvmfit$index\n#>  [1]  1  2  3  4  5  7  9 10 12 13 14 15 16 17 18 20\n\n# The e1071 library includes a built-in function, tune(), to perform ten-fold cross-validation.\nset.seed(1)\ntune.out=tune(svm, y~., data=dat, kernel=\"linear\", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))\nsummary(tune.out)\n#> \n#> Parameter tuning of 'svm':\n#> \n#> - sampling method: 10-fold cross validation \n#> \n#> - best parameters:\n#>  cost\n#>   0.1\n#> \n#> - best performance: 0.05 \n#> \n#> - Detailed performance results:\n#>    cost error dispersion\n#> 1 1e-03  0.55  0.4377975\n#> 2 1e-02  0.55  0.4377975\n#> 3 1e-01  0.05  0.1581139\n#> 4 1e+00  0.15  0.2415229\n#> 5 5e+00  0.15  0.2415229\n#> 6 1e+01  0.15  0.2415229\n#> 7 1e+02  0.15  0.2415229\n# choose the best model with the lowest cross-validation error rate\nbestmod=tune.out$best.model\nsummary(bestmod)\n#> \n#> Call:\n#> best.tune(METHOD = svm, train.x = y ~ ., data = dat, \n#>     ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, \n#>         10, 100)), kernel = \"linear\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  0.1 \n#> \n#> Number of Support Vectors:  16\n#> \n#>  ( 8 8 )\n#> \n#> \n#> Number of Classes:  2 \n#> \n#> Levels: \n#>  -1 1\n\n# generate a test data set.\nxtest=matrix(rnorm(20*2), ncol=2)\nytest=sample(c(-1,1), 20, rep=TRUE)\nxtest[ytest==1,]=xtest[ytest==1,] + 1\ntestdat=data.frame(x=xtest, y=as.factor(ytest))\n\n# The predict() function can be used to predict the class label on a set of test observations\nypred=predict(bestmod,testdat)\ntable(predict=ypred, truth=testdat$y)\n#>        truth\n#> predict -1 1\n#>      -1  9 1\n#>      1   2 8\n\n# try another model with cost 0.01\nsvmfit=svm(y~., data=dat, kernel=\"linear\", cost=.01,scale=FALSE)\nypred=predict(svmfit,testdat)\ntable(predict=ypred, truth=testdat$y)\n#>        truth\n#> predict -1  1\n#>      -1 11  6\n#>      1   0  3"},{"path":"chapter-10-discrimination-and-classification.html","id":"support-vector-machine-1","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.4.8.2 Support Vector Machine","text":"","code":"\n## generate some data with a non-linear class boundary\nset.seed(1)\nx=matrix(rnorm(200*2), ncol=2)\nx[1:100,]=x[1:100,]+2\nx[101:150,]=x[101:150,]-2\ny=c(rep(1,150),rep(2,50))\ndat=data.frame(x=x,y=as.factor(y))\nplot(x, col=y)\n\n## split the dataset into train and test datasets\ntrain=sample(200,100)\n\n## fit the svm model\nsvmfit=svm(y~., data=dat[train,], kernel=\"radial\",  gamma=1, cost=1)\nplot(svmfit, dat[train,])\n\nsummary(svmfit)\n#> \n#> Call:\n#> svm(formula = y ~ ., data = dat[train, ], kernel = \"radial\", \n#>     gamma = 1, cost = 1)\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  radial \n#>        cost:  1 \n#> \n#> Number of Support Vectors:  31\n#> \n#>  ( 16 15 )\n#> \n#> \n#> Number of Classes:  2 \n#> \n#> Levels: \n#>  1 2\n\nsvmfit=svm(y~., data=dat[train,], kernel=\"radial\",gamma=1,cost=1e5)\nplot(svmfit,dat[train,])\n\nset.seed(1)\n# tuning parameters: C and gamma\ntune.out=tune(svm, y~., data=dat[train,], kernel=\"radial\", ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\nsummary(tune.out)\n#> \n#> Parameter tuning of 'svm':\n#> \n#> - sampling method: 10-fold cross validation \n#> \n#> - best parameters:\n#>  cost gamma\n#>     1   0.5\n#> \n#> - best performance: 0.07 \n#> \n#> - Detailed performance results:\n#>     cost gamma error dispersion\n#> 1  1e-01   0.5  0.26 0.15776213\n#> 2  1e+00   0.5  0.07 0.08232726\n#> 3  1e+01   0.5  0.07 0.08232726\n#> 4  1e+02   0.5  0.14 0.15055453\n#> 5  1e+03   0.5  0.11 0.07378648\n#> 6  1e-01   1.0  0.22 0.16193277\n#> 7  1e+00   1.0  0.07 0.08232726\n#> 8  1e+01   1.0  0.09 0.07378648\n#> 9  1e+02   1.0  0.12 0.12292726\n#> 10 1e+03   1.0  0.11 0.11005049\n#> 11 1e-01   2.0  0.27 0.15670212\n#> 12 1e+00   2.0  0.07 0.08232726\n#> 13 1e+01   2.0  0.11 0.07378648\n#> 14 1e+02   2.0  0.12 0.13165612\n#> 15 1e+03   2.0  0.16 0.13498971\n#> 16 1e-01   3.0  0.27 0.15670212\n#> 17 1e+00   3.0  0.07 0.08232726\n#> 18 1e+01   3.0  0.08 0.07888106\n#> 19 1e+02   3.0  0.13 0.14181365\n#> 20 1e+03   3.0  0.15 0.13540064\n#> 21 1e-01   4.0  0.27 0.15670212\n#> 22 1e+00   4.0  0.07 0.08232726\n#> 23 1e+01   4.0  0.09 0.07378648\n#> 24 1e+02   4.0  0.13 0.14181365\n#> 25 1e+03   4.0  0.15 0.13540064\n\n## see the performance of the best svm model selected by tune()\ntable(true=dat[-train,\"y\"], pred=predict(tune.out$best.model,newx=dat[-train,]))\n#>     pred\n#> true  1  2\n#>    1 54 23\n#>    2 17  6"},{"path":"chapter-10-discrimination-and-classification.html","id":"final-project-apply-the-svm-method-to-the-heart-disease-data","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.5 Final Project (Apply the SVM method to the Heart Disease Data)","text":"Download Heart Disease Data (Heart.csv) website: http://faculty.marshall.usc.edu/gareth-james/ISL/data.htmlData Description: data contain binary outcome HD (last column Heart.csv) 303 patients presented chest pain. outcome value Yes indicates presence heart disease based angiographic test, means heart disease. 13 predictors including Age, Sex, Chol (cholesterol measurement), heart lung function measurements.Objective: use 13 predictors Age, Sex, Chol order predict whether individual heart disease.Software: R package “e1071”, …Requirements: please fit SVM Heart Disease Data","code":"\n# load the Heart data\ndat <- read.csv(\"dataset/Heart.csv\")\ndat <- dat[, -1]\ndat <- na.omit(dat)\n\n#..."},{"path":"chapter-10-discrimination-and-classification.html","id":"reference-2","chapter":"3 Chapter 10: Discrimination and Classification","heading":"3.6 Reference","text":"[1] Johnson, R. ., Wichern, D. W. (2007). Applied multivariate statistical analysis (6th ed.). Upper Saddle River, N.J.: Pearson Prentice Hall.[2] Zelterman, D. (2015). Applied Multivariate Statistics R (1st ed.).[3] Statistics 575: Multivariate Analysis, Douglas Wiens (http://www.mathstat.ualberta.ca/~wiens/stat575/stat575.html)[4] James, G., Witten, D., Hastie, T., Tibshirani, R. (2013), Introduction Statistical Learning Applications R[5] figures presentation taken “Introduction Statistical Learning, applications R” (Springer, 2013) permission authors: G. James, D. Witten, T. Hastie R. Tibshirani","code":""}]
