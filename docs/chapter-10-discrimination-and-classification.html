<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Chapter 10: Discrimination and Classification | Lecture notes for STA 6707 Multivariate Methods</title>
<meta name="author" content="Fei Heng">
<meta name="description" content="Questions of interest: If we have multivariate observations from two or more identified populations, how can we characterize them? Is there a combination of measurements that can be used to...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 3 Chapter 10: Discrimination and Classification | Lecture notes for STA 6707 Multivariate Methods">
<meta property="og:type" content="book">
<meta property="og:description" content="Questions of interest: If we have multivariate observations from two or more identified populations, how can we characterize them? Is there a combination of measurements that can be used to...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Chapter 10: Discrimination and Classification | Lecture notes for STA 6707 Multivariate Methods">
<meta name="twitter:description" content="Questions of interest: If we have multivariate observations from two or more identified populations, how can we characterize them? Is there a combination of measurements that can be used to...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.0/transition.js"></script><script src="libs/bs3compat-0.5.0/tabs.js"></script><script src="libs/bs3compat-0.5.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Lecture notes for STA 6707 Multivariate Methods</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">About</a></li>
<li><a class="" href="chapter-8-principle-component-analysis.html"><span class="header-section-number">1</span> Chapter 8: Principle Component Analysis</a></li>
<li><a class="" href="chapter-9-factor-analysis.html"><span class="header-section-number">2</span> Chapter 9: Factor Analysis</a></li>
<li><a class="active" href="chapter-10-discrimination-and-classification.html"><span class="header-section-number">3</span> Chapter 10: Discrimination and Classification</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/rstudio/bookdown-demo">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-10-discrimination-and-classification" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Chapter 10: Discrimination and Classification<a class="anchor" aria-label="anchor" href="#chapter-10-discrimination-and-classification"><i class="fas fa-link"></i></a>
</h1>
<hr>
<p><strong>Questions of interest</strong>:</p>
<ul>
<li>If we have multivariate observations from two or more identified populations, how can we characterize them?</li>
<li>Is there a combination of measurements that can be used to clearly distinguish between these groups?</li>
<li>Is there a rule that can be used to optimally assign new observations to two or more labeled classes?</li>
</ul>
<p>To think in <strong>multivariate</strong> terms, we do not use only one variable at a time to distinguish between groups of individuals, but, rather, we use <strong>a combination of explanatory variables</strong>.</p>
<p>PCA? The objectives of PCA are (1) data reduction and (2) interpretation, although we might use principle components to identify groups.</p>
<p>In discriminant analysis we begin by knowing the group membership, and then try to identify linear combinations of several variables that <strong>can be used to distinguish between the groups</strong>.</p>
<div id="an-introductory-example" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> 10.1 An Introductory Example<a class="anchor" aria-label="anchor" href="#an-introductory-example"><i class="fas fa-link"></i></a>
</h2>
<p>Data on <strong>three varieties of wine cultivars</strong> is given in the data set wines (Forina et al. 1988). There are 178 different wines examined. A list of the 14 variables in this data set are listed in Table 10.1:</p>
<p>&lt;img src=“images/Table_10_1.png”, width=“400”&gt;</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">wines</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"dataset/Wines.txt"</span>, header <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">wines</span><span class="op">)</span></span>
<span><span class="co">#&gt;   Class Alcohol Malic  Ash Alcal  Mg Phenol Flav Nonf Proan</span></span>
<span><span class="co">#&gt; 1     1   14.23  1.71 2.43  15.6 127   2.80 3.06 0.28  2.29</span></span>
<span><span class="co">#&gt; 2     1   13.20  1.78 2.14  11.2 100   2.65 2.76 0.26  1.28</span></span>
<span><span class="co">#&gt; 3     1   13.16  2.36 2.67  18.6 101   2.80 3.24 0.30  2.81</span></span>
<span><span class="co">#&gt; 4     1   14.37  1.95 2.50  16.8 113   3.85 3.49 0.24  2.18</span></span>
<span><span class="co">#&gt; 5     1   13.24  2.59 2.87  21.0 118   2.80 2.69 0.39  1.82</span></span>
<span><span class="co">#&gt; 6     1   14.20  1.76 2.45  15.2 112   3.27 3.39 0.34  1.97</span></span>
<span><span class="co">#&gt;   Color  Hue  Abs Proline</span></span>
<span><span class="co">#&gt; 1  5.64 1.04 3.92    1065</span></span>
<span><span class="co">#&gt; 2  4.38 1.05 3.40    1050</span></span>
<span><span class="co">#&gt; 3  5.68 1.03 3.17    1185</span></span>
<span><span class="co">#&gt; 4  7.80 0.86 3.45    1480</span></span>
<span><span class="co">#&gt; 5  4.32 1.04 2.93     735</span></span>
<span><span class="co">#&gt; 6  6.75 1.05 2.85    1450</span></span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">colors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"green"</span>, <span class="st">"red"</span> ,<span class="st">"blue"</span><span class="op">)</span><span class="op">[</span><span class="va">wines</span><span class="op">[</span> , <span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># The Class variable is jittered to make it easier to view.</span></span>
<span><span class="co"># new dataframe with jittered Class variable</span></span>
<span><span class="va">newwine</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/jitter.html">jitter</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="va">wines</span><span class="op">[</span> , <span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>, <span class="va">wines</span><span class="op">[</span> , <span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="co"># jitter: Add a small amount of noise to a numeric vector</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">newwine</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">wines</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="co"># old name to new variable</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">newwine</span>, pch <span class="op">=</span> <span class="fl">16</span>, cex <span class="op">=</span> <span class="fl">.3</span>, gap <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="va">colors</span>, xaxt <span class="op">=</span> <span class="st">"n"</span>, yaxt <span class="op">=</span> <span class="st">"n"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-3-1.png" width="672"></div>
<p>In the top row and left column in the pairwise scatterplots above, we can identify <strong>individual variables</strong> that demonstrate how individual variables differ across the three groups of wines.</p>
<p>Specifically, we can perform a hypothesis test that the means of each of the variables are the same across the three groups. (<strong>one-way ANOVA</strong>)</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">vars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">wines</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>          <span class="co"># number of variables</span></span>
<span><span class="va">mn</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span> <span class="op">:</span> <span class="va">vars</span><span class="op">)</span>            <span class="co"># omit the first, Class variable</span></span>
<span><span class="op">{</span></span>
<span>  <span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/summary.aov.html">summary.aov</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">wines</span><span class="op">[</span>, <span class="va">i</span><span class="op">]</span> <span class="op">~</span> <span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="co"># print(z)</span></span>
<span>  <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">z</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="st">"Pr(&gt;F)"</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>    <span class="co"># capture the p-values</span></span>
<span>  <span class="co"># print(p)</span></span>
<span>  <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">p</span>, <span class="fl">.0001</span><span class="op">)</span></span>
<span>  <span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span> <span class="op">:</span> <span class="fl">3</span><span class="op">)</span></span>
<span>  <span class="op">{</span></span>
<span>    <span class="va">mn</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">wines</span><span class="op">[</span> <span class="va">wines</span><span class="op">$</span><span class="va">Class</span> <span class="op">==</span> <span class="va">j</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="va">i</span> <span class="op">==</span> <span class="fl">2</span><span class="op">)</span><span class="va">univ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">p</span>,<span class="va">mn</span><span class="op">)</span></span>
<span>  <span class="kw">else</span> <span class="va">univ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">univ</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">p</span>, <span class="va">mn</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/row.names.html">row.names</a></span><span class="op">(</span><span class="va">univ</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">wines</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span> <span class="op">:</span> <span class="va">vars</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">univ</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="st">"p-value"</span>, <span class="st">"Group=1"</span>, </span>
<span>                     <span class="st">"Group=2"</span>, <span class="st">"Group=3"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">univ</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt;           p-value  Group=1  Group=2  Group=3</span></span>
<span><span class="co">#&gt; Alcohol 0.0001000   13.745  12.2787  13.1538</span></span>
<span><span class="co">#&gt; Malic   0.0001000    2.011   1.9327   3.3338</span></span>
<span><span class="co">#&gt; Ash     0.5104977    2.456   2.2448   2.4371</span></span>
<span><span class="co">#&gt; Alcal   0.0001000   17.037  20.2380  21.4167</span></span>
<span><span class="co">#&gt; Mg      0.0050754  106.339  94.5493  99.3125</span></span>
<span><span class="co">#&gt; Phenol  0.0001000    2.840   2.2589   1.6787</span></span>
<span><span class="co">#&gt; Flav    0.0001000    2.982   2.0808   0.7815</span></span>
<span><span class="co">#&gt; Nonf    0.0001000    0.290   0.3637   0.4475</span></span>
<span><span class="co">#&gt; Proan   0.0001000    1.899   1.6303   1.1535</span></span>
<span><span class="co">#&gt; Color   0.0003382    5.528   3.0866   7.3963</span></span>
<span><span class="co">#&gt; Hue     0.0001000    1.062   1.0563   0.6827</span></span>
<span><span class="co">#&gt; Abs     0.0001000    3.158   2.7854   1.6835</span></span>
<span><span class="co">#&gt; Proline 0.0001000 1115.712 519.5070 629.8958</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">)</span>      <span class="co"># frequency table for each level</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  1  2  3 </span></span>
<span><span class="co">#&gt; 59 71 48</span></span></code></pre></div>
<ul>
<li><p>We find that the one-way analysis of variance demonstrates an extreme
level of statistical significance for every variable except for the <em>Ash</em> variable in the data set.</p></li>
<li><p>This marginal approach is intuitive for identifying <strong>group means</strong>, but it
fails to identify discriminatory characteristics for <strong>individuals</strong>.</p></li>
<li><p>One <strong>individual</strong> cultivar may have a high or low value for a specified measurement, but the histograms of the three groups may exhibit considerable <strong>overlap</strong>, making assessment of group membership difficult.This overlap is readily apparent in the following <strong>parallel coordinate plot</strong>:</p></li>
</ul>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/parcoord.html">parcoord</a></span><span class="op">(</span><span class="va">wines</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">9</span>, <span class="fl">11</span> <span class="op">:</span> <span class="fl">14</span>, <span class="fl">7</span>, <span class="fl">8</span><span class="op">)</span><span class="op">]</span>, col <span class="op">=</span> <span class="va">colors</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-5-1.png" width="672"></div>
<ul>
<li><p>The univariate approach fails to include correlations between the individual
measurements. There may be combinations of variables that provide a much higher level of discriminatory precision between the three cultivars than this univariate approach.</p></li>
<li><p>Considering the correlations, a number of plotted pairs are useful in identifying and discriminating between the three cultivars in the aforementioned pairwise scatterplots. These include <strong>(Abs, Proline)</strong>, <strong>(Alcohol, Hue)</strong>, and <strong>(Color, Flav)</strong> as examples.</p></li>
<li><p>The following two sections describe discriminatory methods based on <strong>linear combinations of all variables</strong> in this data set. The <strong>support vector approach</strong>, described in Section 10.4, identifies specific observations that help <strong>define regions of high discriminatory value and do not necessarily seek linear combinations</strong> to achieve these goals.</p></li>
</ul>
</div>
<div id="multinomial-logistic-regression" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> 10.2 Multinomial Logistic Regression<a class="anchor" aria-label="anchor" href="#multinomial-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<div id="two-groups" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Two groups<a class="anchor" aria-label="anchor" href="#two-groups"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>We use <strong>logistic regression</strong> to distinguish between two different groups. This classification criteria is based on a probabilistic statement about the group membership of each observation.</p></li>
<li><p>Denote the group membership as a bivariate response variable, <span class="math inline">\(Y = 0\)</span> or <span class="math inline">\(1\)</span>, conditional on a vector of explanatory variables <span class="math inline">\(\vec x\)</span>.</p></li>
<li><p>The observed data on the <span class="math inline">\(i\)</span>-th observation: <span class="math inline">\((y_i, \vec x_i)\)</span>.</p></li>
<li><p>The logistic regression model:</p></li>
</ul>
<p><span class="math display">\[\log\Bigg(\dfrac{\Pr(Y=1|\vec x)}{\Pr(Y=0|\vec x)}\Bigg)=\vec \beta'\vec x\]</span></p>
<ul>
<li>By simple algebraic manipulation, we have</li>
</ul>
<p><span class="math display">\[\Pr(Y=1|\vec x)=\dfrac{1}{1+e^{-\vec \beta'\vec x}}\]</span></p>
<ul>
<li>We can estimate the parameter vector *using <strong>maximum likelihood</strong> and perform statistical inference on it.</li>
</ul>
</div>
<div id="more-than-two-groups" class="section level3" number="3.2.2">
<h3>
<span class="header-section-number">3.2.2</span> More than two groups<a class="anchor" aria-label="anchor" href="#more-than-two-groups"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>The most commonly used approach selects one category as a <strong>reference or baseline</strong> and compares all other categories to it.</p></li>
<li><p>This approach performs a <strong>pairwise comparison</strong> between each of the categories to the single baseline, reference category.</p></li>
<li><p>This approach has a useful interpretation when there is an obvious baseline standard, and it makes sense that all categories should be compared to it.</p></li>
<li><p>In the case of the wine cultivar data, there is no obvious comparison group. We will set Class 2 as the reference, since Class 2 has the most observations among these three classes.</p></li>
<li><p>The generalization of logistic regression to model the three categories for the wine data is the pair of simultaneous equations:</p></li>
</ul>
<p><span class="math display">\[\log\Bigg(\dfrac{\Pr(\text{Class}=1|\vec x)}{\Pr(\text{Class}=2|\vec x)}\Bigg)=\vec \beta_1'\vec x\]</span>
<span class="math display">\[\log\Bigg(\dfrac{\Pr(\text{Class}=3|\vec x)}{\Pr(\text{Class}=2|\vec x)}\Bigg)=\vec \beta_3'\vec x\]</span></p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">nnet</a></span><span class="op">)</span></span>
<span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">)</span> <span class="co"># create factor categories</span></span>
<span><span class="va">wines</span><span class="op">$</span><span class="va">rClass</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/relevel.html">relevel</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span>, ref <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>  <span class="co"># set reference category </span></span>
<span></span>
<span><span class="va">winelogit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/nnet/man/multinom.html">multinom</a></span><span class="op">(</span><span class="va">rClass</span> <span class="op">~</span> <span class="va">Alcohol</span> <span class="op">+</span> <span class="va">Ash</span> <span class="op">+</span> <span class="va">Alcal</span> <span class="op">+</span> <span class="va">Abs</span>  <span class="op">+</span> <span class="va">Proline</span>, </span>
<span>                      data <span class="op">=</span> <span class="va">wines</span>, maxit <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span>
<span><span class="co">#&gt; # weights:  21 (12 variable)</span></span>
<span><span class="co">#&gt; initial  value 195.552987 </span></span>
<span><span class="co">#&gt; iter  10 value 38.789908</span></span>
<span><span class="co">#&gt; iter  20 value 20.100843</span></span>
<span><span class="co">#&gt; iter  30 value 17.180767</span></span>
<span><span class="co">#&gt; iter  40 value 16.286691</span></span>
<span><span class="co">#&gt; iter  50 value 13.857030</span></span>
<span><span class="co">#&gt; iter  60 value 13.727827</span></span>
<span><span class="co">#&gt; iter  70 value 13.569620</span></span>
<span><span class="co">#&gt; iter  80 value 13.543034</span></span>
<span><span class="co">#&gt; iter  90 value 13.521774</span></span>
<span><span class="co">#&gt; iter 100 value 13.465942</span></span>
<span><span class="co">#&gt; iter 110 value 13.439976</span></span>
<span><span class="co">#&gt; iter 120 value 13.418083</span></span>
<span><span class="co">#&gt; iter 130 value 13.405035</span></span>
<span><span class="co">#&gt; iter 140 value 13.394287</span></span>
<span><span class="co">#&gt; iter 150 value 13.360739</span></span>
<span><span class="co">#&gt; iter 160 value 13.351303</span></span>
<span><span class="co">#&gt; iter 170 value 13.330023</span></span>
<span><span class="co">#&gt; final  value 13.325689 </span></span>
<span><span class="co">#&gt; converged</span></span>
<span></span>
<span><span class="co"># These regression coefficients are the pairwise comparisons between Class=1 and =3 with the reference category. The fitted coefficients represent log-odds ratios of the change on classification probabilities when the independent variable changes by one unit. </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ws</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">winelogit</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; multinom(formula = rClass ~ Alcohol + Ash + Alcal + Abs + Proline, </span></span>
<span><span class="co">#&gt;     data = wines, maxit = 200)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;   (Intercept) Alcohol    Ash   Alcal    Abs  Proline</span></span>
<span><span class="co">#&gt; 1     -124.00   6.213 22.849 -3.3478 10.354 0.029383</span></span>
<span><span class="co">#&gt; 3      -46.46   2.927  6.192  0.3032 -7.483 0.005955</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Std. Errors:</span></span>
<span><span class="co">#&gt;   (Intercept) Alcohol    Ash  Alcal    Abs  Proline</span></span>
<span><span class="co">#&gt; 1      0.1139  1.6304 0.1403 1.0161 0.2612 0.032351</span></span>
<span><span class="co">#&gt; 3      0.3588  0.4539 2.8564 0.2566 1.8766 0.003968</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual Deviance: 26.65138 </span></span>
<span><span class="co">#&gt; AIC: 50.65138</span></span>
<span></span>
<span><span class="va">tratio</span> <span class="op">&lt;-</span> <span class="va">ws</span><span class="op">$</span><span class="va">coefficients</span> <span class="op">/</span> <span class="va">ws</span><span class="op">$</span><span class="va">standard.errors</span></span>
<span><span class="co"># Compute two-tailed p values from the t with $edf = effective df</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">tratio</span><span class="op">)</span>, df <span class="op">=</span> <span class="va">ws</span><span class="op">$</span><span class="va">edf</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt;   (Intercept) Alcohol   Ash  Alcal    Abs Proline</span></span>
<span><span class="co">#&gt; 1           0  0.0025 0.000 0.0064 0.0000  0.3816</span></span>
<span><span class="co">#&gt; 3           0  0.0000 0.051 0.2603 0.0018  0.1592</span></span>
<span><span class="co"># Each row is the true class membership and the estimated probability of class membership for this wine.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span>,<span class="va">ws</span><span class="op">$</span><span class="va">fitted.values</span><span class="op">)</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">101</span><span class="op">:</span><span class="fl">105</span>, <span class="fl">171</span><span class="op">:</span><span class="fl">175</span><span class="op">)</span>, <span class="op">]</span>, digits <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt;            2      1      3</span></span>
<span><span class="co">#&gt; 1   1 0.0000 1.0000 0.0000</span></span>
<span><span class="co">#&gt; 2   1 0.0000 1.0000 0.0000</span></span>
<span><span class="co">#&gt; 3   1 0.0000 1.0000 0.0000</span></span>
<span><span class="co">#&gt; 4   1 0.0000 1.0000 0.0000</span></span>
<span><span class="co">#&gt; 5   1 0.0042 0.9944 0.0014</span></span>
<span><span class="co">#&gt; 101 2 1.0000 0.0000 0.0000</span></span>
<span><span class="co">#&gt; 102 2 0.9999 0.0000 0.0001</span></span>
<span><span class="co">#&gt; 103 2 1.0000 0.0000 0.0000</span></span>
<span><span class="co">#&gt; 104 2 1.0000 0.0000 0.0000</span></span>
<span><span class="co">#&gt; 105 2 1.0000 0.0000 0.0000</span></span>
<span><span class="co">#&gt; 171 3 0.7819 0.0000 0.2181</span></span>
<span><span class="co">#&gt; 172 3 0.1745 0.0000 0.8255</span></span>
<span><span class="co">#&gt; 173 3 0.0005 0.0000 0.9995</span></span>
<span><span class="co">#&gt; 174 3 0.0016 0.0000 0.9984</span></span>
<span><span class="co">#&gt; 175 3 0.0004 0.0000 0.9996</span></span></code></pre></div>
</div>
<div id="graphical-diagnostics" class="section level3" number="3.2.3">
<h3>
<span class="header-section-number">3.2.3</span> Graphical diagnostics<a class="anchor" aria-label="anchor" href="#graphical-diagnostics"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">colors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"green"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">[</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">winelogit</span><span class="op">$</span><span class="va">fitted.values</span><span class="op">[</span>, <span class="fl">2</span> <span class="op">:</span> <span class="fl">3</span><span class="op">]</span>, </span>
<span>     col <span class="op">=</span> <span class="va">colors</span>, cex <span class="op">=</span> <span class="fl">1.25</span>, cex.lab<span class="op">=</span><span class="fl">1.5</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Fitted  probability of  y=1"</span>,  </span>
<span>     ylab <span class="op">=</span> <span class="st">"Fitted probability of y=3"</span>, main <span class="op">=</span> <span class="st">"multinomial logistic"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">.42</span>, <span class="fl">4</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">.92</span>,  <span class="fl">.84</span>, <span class="fl">.76</span><span class="op">)</span>, pos <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>      col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"green"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span>,</span>
<span>      labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Observation codes:"</span>, <span class="st">"y=1 category"</span>, </span>
<span>                 <span class="st">"y=2 category (ref)"</span>, <span class="st">"y=3 category"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">.4</span>, <span class="fl">1.2</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.2</span>, <span class="fl">.7</span>, <span class="fl">.7</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-7-1.png" width="672"></div>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nmlr</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">fit</span>, <span class="va">obs</span><span class="op">)</span><span class="op">{</span> <span class="co"># normalized  multivariate logistic residuals</span></span>
<span>  <span class="va">nmlr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>, nrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, </span>
<span>                 ncol <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>  <span class="co"># initialize </span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span> <span class="op">:</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span> <span class="op">:</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>      <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">fit</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span>              <span class="co"># fitted p parameter</span></span>
<span>      <span class="va">ob</span> <span class="op">&lt;-</span> <span class="va">obs</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span></span>
<span>      <span class="va">res</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">ob</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span>  <span class="op">/</span>  <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span> <span class="va">p</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span> <span class="op">)</span> <span class="op">)</span></span>
<span>      <span class="va">nmlr</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">res</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">nmlr</span></span>
<span><span class="op">}</span></span>
<span><span class="co">###  Plot of standardized residuals</span></span>
<span><span class="va">obs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>             <span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span> <span class="op">==</span> <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">obs</span><span class="op">)</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"1"</span>, <span class="st">"3"</span><span class="op">)</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu">nmlr</span><span class="op">(</span><span class="va">winelogit</span><span class="op">$</span><span class="va">fitted.values</span><span class="op">[</span>, <span class="fl">2</span> <span class="op">:</span> <span class="fl">3</span><span class="op">]</span>, <span class="va">obs</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">res</span>, col <span class="op">=</span> <span class="va">colors</span>, cex <span class="op">=</span> <span class="fl">1.25</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Std residual for fitted probability of y=1"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Std residual for y=3"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-8-1.png" width="672"></div>
</div>
</div>
<div id="linear-discriminant-analysis" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> 10.3 Linear Discriminant Analysis<a class="anchor" aria-label="anchor" href="#linear-discriminant-analysis"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>Linear discrimination analysis allows us to identify a linear combination of variables that can be used to clearly identify the group membership of all individuals.</p></li>
<li><p>linear discriminant analysis assumes that the vector of the explanatory variables <span class="math inline">\(\vec x\)</span> has multivariate normal distributions with different means for each group but the same covariance across all groups.</p></li>
</ul>
<div id="wine-data-example" class="section level3" number="3.3.1">
<h3>
<span class="header-section-number">3.3.1</span> Wine data example<a class="anchor" aria-label="anchor" href="#wine-data-example"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>We apply the linear discrimination analysis to the wine data and plot the resulting groups in colors and identifying Class number.</li>
</ul>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">wines</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"dataset/Wines.txt"</span>, header <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">colors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"green"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">[</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">]</span></span>
<span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">)</span> </span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="va">ld</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">Class</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">wines</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The first and second discriminators are linear combinations of variables that best discriminate between the three cultivars of wines. The linear weights are listed as LD1 and LD2 in the output:</span></span>
<span><span class="va">ld</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lda(Class ~ ., data = wines)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Prior probabilities of groups:</span></span>
<span><span class="co">#&gt;         1         2         3 </span></span>
<span><span class="co">#&gt; 0.3314607 0.3988764 0.2696629 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Group means:</span></span>
<span><span class="co">#&gt;    Alcohol    Malic      Ash    Alcal       Mg   Phenol</span></span>
<span><span class="co">#&gt; 1 13.74475 2.010678 2.455593 17.03729 106.3390 2.840169</span></span>
<span><span class="co">#&gt; 2 12.27873 1.932676 2.244789 20.23803  94.5493 2.258873</span></span>
<span><span class="co">#&gt; 3 13.15375 3.333750 2.437083 21.41667  99.3125 1.678750</span></span>
<span><span class="co">#&gt;        Flav     Nonf    Proan    Color       Hue      Abs</span></span>
<span><span class="co">#&gt; 1 2.9823729 0.290000 1.899322 5.528305 1.0620339 3.157797</span></span>
<span><span class="co">#&gt; 2 2.0808451 0.363662 1.630282 3.086620 1.0563380 2.785352</span></span>
<span><span class="co">#&gt; 3 0.7814583 0.447500 1.153542 7.396250 0.6827083 1.683542</span></span>
<span><span class="co">#&gt;     Proline</span></span>
<span><span class="co">#&gt; 1 1115.7119</span></span>
<span><span class="co">#&gt; 2  519.5070</span></span>
<span><span class="co">#&gt; 3  629.8958</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients of linear discriminants:</span></span>
<span><span class="co">#&gt;                  LD1           LD2</span></span>
<span><span class="co">#&gt; Alcohol -0.403274956  0.8718833272</span></span>
<span><span class="co">#&gt; Malic    0.165185223  0.3051811048</span></span>
<span><span class="co">#&gt; Ash     -0.368792093  2.3459219420</span></span>
<span><span class="co">#&gt; Alcal    0.154783909 -0.1463931519</span></span>
<span><span class="co">#&gt; Mg      -0.002162757 -0.0004611477</span></span>
<span><span class="co">#&gt; Phenol   0.617931702 -0.0324979420</span></span>
<span><span class="co">#&gt; Flav    -1.661172871 -0.4916834144</span></span>
<span><span class="co">#&gt; Nonf    -1.495756932 -1.6303752589</span></span>
<span><span class="co">#&gt; Proan    0.134093115 -0.3070371492</span></span>
<span><span class="co">#&gt; Color    0.355006846  0.2530559406</span></span>
<span><span class="co">#&gt; Hue     -0.819785218 -1.5182643908</span></span>
<span><span class="co">#&gt; Abs     -1.157612096  0.0512054337</span></span>
<span><span class="co">#&gt; Proline -0.002690475  0.0028540202</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Proportion of trace:</span></span>
<span><span class="co">#&gt;    LD1    LD2 </span></span>
<span><span class="co">#&gt; 0.6875 0.3125</span></span>
<span></span>
<span><span class="co"># The loadings are the linear combinations of the explanatory variables,</span></span>
<span><span class="va">loading</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">wines</span><span class="op">[</span>, <span class="fl">2</span> <span class="op">:</span> <span class="fl">14</span><span class="op">]</span><span class="op">)</span>  <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span>   <span class="va">ld</span><span class="op">$</span><span class="va">scaling</span> <span class="co"># ld$scaling: coefficients</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">loading</span><span class="op">)</span></span>
<span><span class="co">#&gt;            LD1      LD2</span></span>
<span><span class="co">#&gt; [1,] -13.93059 16.62038</span></span>
<span><span class="co">#&gt; [2,] -13.53241 15.81159</span></span>
<span><span class="co">#&gt; [3,] -12.65109 16.07028</span></span>
<span><span class="co">#&gt; [4,] -13.43569 18.64437</span></span>
<span><span class="co">#&gt; [5,] -10.74060 15.09195</span></span>
<span><span class="co">#&gt; [6,] -13.74883 17.85450</span></span>
<span></span>
<span><span class="co"># Thie following figure illustrates a clear distinction between the three cultivars.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">loading</span>, col <span class="op">=</span> <span class="va">colors</span>, pch <span class="op">=</span> <span class="fl">16</span>, cex <span class="op">=</span> <span class="fl">2</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"First linear discriminator"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Second linear discriminator"</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span> <span class="op">:</span> <span class="fl">3</span><span class="op">)</span><span class="op">{</span>       <span class="co"># add class number to each centroid</span></span>
<span>  <span class="va">centx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">loading</span><span class="op">[</span><span class="va">wines</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="va">i</span>, <span class="op">]</span> <span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">)</span></span>
<span>  <span class="va">centy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">loading</span><span class="op">[</span><span class="va">wines</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="va">i</span>, <span class="op">]</span> <span class="op">[</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="va">centx</span>, <span class="va">centy</span>, <span class="va">i</span>, cex <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-9-1.png" width="672"></div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># The fitted (posterior) estimated probabilities of group membership can be obtained as predict(ld)$posterior</span></span>
<span><span class="va">ldp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ld</span><span class="op">)</span><span class="op">$</span><span class="va">posterior</span></span>
<span></span>
<span><span class="co"># From the following plot, we can see the plot for LDA is much better than the corresponding one for the multinomial logistic regression (under stronger assumption, which needs to be checked before analysis!)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ldp</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">ldp</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span>, col <span class="op">=</span> <span class="va">colors</span>, cex <span class="op">=</span> <span class="fl">1.25</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Fitted probability of y=1"</span>, cex.lab<span class="op">=</span><span class="fl">1.5</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Fitted probability of y=3"</span>, main <span class="op">=</span> <span class="st">"LDA"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">.42</span>, <span class="fl">4</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">.92</span>,  <span class="fl">.84</span>, <span class="fl">.76</span><span class="op">)</span>, pos <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>      col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"green"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span>, cex<span class="op">=</span><span class="fl">1.25</span>,</span>
<span>      labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Observation codes:"</span>, <span class="st">"y=1 category"</span>, </span>
<span>                 <span class="st">"y=2 category (ref)"</span>, <span class="st">"y=3 category"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">.4</span>, <span class="fl">1.2</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.2</span>, <span class="fl">.7</span>, <span class="fl">.7</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-9-2.png" width="672"></div>
</div>
<div id="methodology" class="section level3" number="3.3.2">
<h3>
<span class="header-section-number">3.3.2</span> Methodology<a class="anchor" aria-label="anchor" href="#methodology"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we have <span class="math inline">\(K\)</span> different populations (for some <span class="math inline">\(K = 2, 3, \dots\)</span>) and each population is expressible as a <span class="math inline">\(p\)</span>-dimensional normal population with respective means <span class="math inline">\(\vec \mu_j\)</span> (<span class="math inline">\(j = 1,\dots,K\)</span>) and all with the same covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span>.</p>
<p><span class="math display">\[\vec X \sim N_p(\vec \mu_j, \mathbf{\Sigma}), \vec X \text{ is from Population } j, j = 1,\dots,K\]</span></p>
<div id="prior-probabilities" class="section level4" number="3.3.2.1">
<h4>
<span class="header-section-number">3.3.2.1</span> Prior probabilities:<a class="anchor" aria-label="anchor" href="#prior-probabilities"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>Let <span class="math inline">\(\pi_j\)</span> denote the probability that a randomly selected observation is sampled from Population <span class="math inline">\(j\)</span>.</li>
</ul>
<p><span class="math display">\[\Pr(\text{Population } j)=\pi_j\]</span></p>
<ul>
<li>The parameters <span class="math inline">\(\pi_j\)</span> are called <strong>prior probabilities</strong> because they represent characteristics of the population that are known to us before any data is observed.</li>
<li>They can be <strong>estimated from the data</strong>, as is frequently the case in practice. In the wine data example, these prior probabilities are the observed sample proportions of the three wine cultivars.</li>
</ul>
<p>The means <span class="math inline">\(\vec \mu_j\)</span> and the covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span> common to all populations will be assumed to be known, but, in practice, these will also be <strong>estimated from the data</strong>.</p>
<p>Given a <span class="math inline">\(p\)</span>-dimensional observation <span class="math inline">\(\vec x\)</span>, the <strong>conditional density</strong> function of <span class="math inline">\(\vec x\)</span> given that it was sampled from Population <span class="math inline">\(j\)</span> is
<span class="math display">\[f(\vec x|\text{Population } j)=f(\vec x|\vec \mu_j, \mathbf{\Sigma})\]</span>
for a multivariate normal density function with mean <span class="math inline">\(\vec \mu_j\)</span> and the covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span>.</p>
<p>The <strong>joint density</strong> of observing <span class="math inline">\(\vec x\)</span> from Population <span class="math inline">\(j\)</span> is
<span class="math display">\[f(\vec x \text{ was sampled from Population } j ) = \pi_jf(\vec x|\vec \mu_j, \mathbf{\Sigma})\]</span></p>
<p>The <strong>marginal density</strong> for a value <span class="math inline">\(\vec x\)</span> sampled from an unspecified population:
<span class="math display">\[f(\vec x) =\sum_{j=1}^K\pi_jf(\vec x|\vec \mu_j, \mathbf{\Sigma})\]</span>
is the sum over all possible population values of <span class="math inline">\(j = 1,\dots,K\)</span>.</p>
<p>The <strong>posterior probability</strong> of sampling from Population <span class="math inline">\(k\)</span> for a given observation <span class="math inline">\(\vec x\)</span> is
<span class="math display">\[f(\text{Population } k| \vec x) = \dfrac{\pi_kf(\vec x|\vec \mu_k, \mathbf{\Sigma})}{\sum_{j=1}^K\pi_jf(\vec x|\vec \mu_j, \mathbf{\Sigma})}\]</span></p>
<p>The <strong>maximum a posteriori (MAP) estimator</strong> <span class="math inline">\(\hat k\)</span> directs us to choose the value of <span class="math inline">\(k\)</span> that makes the <strong>posterior probability</strong> as large as possible (also called the <strong>Bayes classifier</strong>.) Since the denominator is the same for every value of <span class="math inline">\(k\)</span> so the <strong>MAP estimator</strong> of <span class="math inline">\(k\)</span> is the value that maximizes the numerator.</p>
<p><span class="math display">\[\begin{aligned}
\hat k=&amp;\underset{k}{argmax} \ \pi_kf(\vec x|\vec \mu_k, \mathbf{\Sigma})\\
=&amp;\underset{k}{argmax} \ \log\{\pi_kf(\vec x|\vec \mu_k, \mathbf{\Sigma})\}\\
=&amp;\underset{k}{argmax} \ \log(\pi_k)-\log\{(2\pi)^{p/2}|\mathbf{\Sigma}^{1/2}|\}-\dfrac{1}{2}(\vec x-\vec \mu_k)'\mathbf{\Sigma}^{-1}(\vec x-\vec \mu_k)\\
=&amp;\underset{k}{argmax} \ \log(\pi_k)-\dfrac{1}{2}(\vec x-\vec \mu_k)'\mathbf{\Sigma}^{-1}(\vec x-\vec \mu_k)\\
=&amp;\underset{k}{argmax} \ \log(\pi_k)-\dfrac{1}{2}\vec x'\mathbf{\Sigma}^{-1}\vec x-\dfrac{1}{2}\vec \mu_k'\mathbf{\Sigma}^{-1}\vec \mu_k+ \vec x'\mathbf{\Sigma}^{-1}\vec \mu_k\\
=&amp;\underset{k}{argmax} \ \log(\pi_k)-\dfrac{1}{2}\vec \mu_k'\mathbf{\Sigma}^{-1}\vec \mu_k+ \vec x'\mathbf{\Sigma}^{-1}\vec \mu_k
\end{aligned}\]</span></p>
<p>Finally, the discriminant function for LDA, defined by
<span class="math display">\[\delta_k(\vec x)=\log(\pi_k)-\dfrac{1}{2}\vec \mu_k'\mathbf{\Sigma}^{-1}\vec \mu_k+ \vec x'\mathbf{\Sigma}^{-1}\vec \mu_k\]</span>
is a linear function of <span class="math inline">\(\vec x\)</span>, leading us to use the name, <strong>linear discrimination</strong>.</p>
<p>In practice, the parameters (<span class="math inline">\(\mu_k, \mathbf{\Sigma}\)</span>) are replaced by their usual estimators (Lecture 6.)</p>
<p>How does the linear discrimination process distinguish between two different populations denoted <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span>?</p>
<p>The <strong>boundary</strong> of this process occurs for values of <span class="math inline">\(\vec x\)</span> for which <span class="math inline">\(\delta_j(\vec x) = \delta_k(\vec x)\)</span>. That is, the solutions to this equation is a set of hypothetical data values <span class="math inline">\(\vec x\)</span> that would provide equal amounts of evidence for both of these different populations.</p>
<p>The solutions of the equation <span class="math inline">\(\delta_j(\vec x) = \delta_k(\vec x)\)</span> occur for values of <span class="math inline">\(\vec x\)</span> in
which
<span class="math display">\[\log(\pi_k/\pi_j)-\dfrac{1}{2}(\vec \mu_k+\vec \mu_j)'\mathbf{\Sigma}^{-1}(\vec \mu_k-\vec \mu_j)+ \vec x'\mathbf{\Sigma}^{-1}(\vec \mu_k-\vec \mu_j)\]</span>
or more simply,
Constant + x
<span class="math display">\[\text{Constant }+ \vec x'\mathbf{\Sigma}^{-1}(\vec \mu_k-\vec \mu_j)\]</span>
where the Constant depends on <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> but not on <span class="math inline">\(\vec x\)</span>. Obviously, it is a linear boundary.</p>
</div>
</div>
<div id="an-illustrative-example-mixture-of-two-normal-distributions" class="section level3" number="3.3.3">
<h3>
<span class="header-section-number">3.3.3</span> An illustrative example (mixture of two normal distributions)<a class="anchor" aria-label="anchor" href="#an-illustrative-example-mixture-of-two-normal-distributions"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Prior probabilities: <span class="math inline">\(\pi_1=0.6, \pi_2=0.4\)</span>
</li>
<li>Popolation means and covariance matrix:</li>
</ul>
<p><span class="math display">\[\mu_1=\begin{bmatrix}
-1\\
-1
\end{bmatrix},
\mu_2=\begin{bmatrix}
1\\
1
\end{bmatrix},
\mathbf{\Sigma}=\begin{bmatrix}
1 &amp; -0.4\\
-0.4 &amp; 1.2
\end{bmatrix}\]</span></p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://dx.doi.org/10.1007/978-1-4419-9650-3">MVA</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: HSAUR2</span></span>
<span><span class="co">#&gt; Loading required package: tools</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">pi1</span> <span class="op">&lt;-</span> <span class="fl">.6</span>        <span class="co"># prior probilities</span></span>
<span><span class="va">pi2</span> <span class="op">&lt;-</span> <span class="fl">.4</span></span>
<span><span class="va">mu1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="co"># mean vectors</span></span>
<span><span class="va">mu2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">cov</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">1</span>, <span class="op">-</span><span class="fl">.4</span>, <span class="op">-</span><span class="fl">.4</span>, <span class="fl">1.2</span><span class="op">)</span>, <span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">del</span> <span class="op">&lt;-</span> <span class="fl">.1</span>      <span class="co"># how fine the grid</span></span>
<span><span class="va">lim</span> <span class="op">&lt;-</span> <span class="fl">3.2</span>     <span class="co"># normals plotted on +/- lim</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="va">lim</span>, <span class="va">lim</span>, <span class="va">del</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span></span>
<span></span>
<span><span class="co"># bivariate normal density for contours</span></span>
<span><span class="va">corcon</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, <span class="va">mu1</span>, <span class="va">mu2</span>, <span class="va">cov</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">nx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="va">ny</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span>  <span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">nx</span> <span class="op">*</span> <span class="va">ny</span><span class="op">)</span>, <span class="va">nx</span>, <span class="va">ny</span><span class="op">)</span>  <span class="co"># build lattice</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span> <span class="op">:</span> <span class="va">nx</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span> <span class="op">:</span> <span class="va">ny</span><span class="op">)</span><span class="op">{</span></span>
<span>      <span class="va">xy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">y</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">)</span></span>
<span>      <span class="va">z</span><span class="op">[</span><span class="va">i</span>,<span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">pi1</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">dmvnorm</a></span><span class="op">(</span><span class="va">xy</span>, <span class="va">mu1</span>, <span class="va">cov</span><span class="op">)</span> <span class="op">+</span> </span>
<span>        <span class="va">pi2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">dmvnorm</a></span><span class="op">(</span><span class="va">xy</span>, <span class="va">mu2</span>, <span class="va">cov</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">z</span></span>
<span><span class="op">}</span></span>
<span><span class="va">z</span> <span class="op">&lt;-</span> <span class="fu">corcon</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, <span class="va">mu1</span>, <span class="va">mu2</span>, <span class="va">cov</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># contour plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/contour.html">contour</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, <span class="va">z</span>, drawlabels <span class="op">=</span> <span class="cn">FALSE</span>, axes <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>        frame <span class="op">=</span> <span class="cn">TRUE</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co"># boundary</span></span>
<span><span class="va">sinv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">cov</span><span class="op">)</span></span>
<span><span class="va">const</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">pi1</span> <span class="op">/</span> <span class="va">pi2</span><span class="op">)</span> <span class="op">-</span><span class="fl">.5</span> <span class="op">*</span> <span class="op">(</span><span class="va">mu1</span> <span class="op">+</span> <span class="va">mu2</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">sinv</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="va">mu1</span> <span class="op">-</span> <span class="va">mu2</span><span class="op">)</span></span>
<span><span class="va">line.coef</span> <span class="op">&lt;-</span> <span class="va">sinv</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="va">mu1</span> <span class="op">-</span> <span class="va">mu2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>a<span class="op">=</span><span class="op">-</span><span class="fl">1</span><span class="op">/</span><span class="va">line.coef</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, b<span class="op">=</span><span class="op">-</span><span class="va">line.coef</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">/</span><span class="va">line.coef</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-10-1.png" width="672"></div>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># perspective plot</span></span>
<span><span class="co"># The front/left normal (with pi1 = 0.6) is a little taller than the second population with pi2 = 0.4. </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/persp.html">persp</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, <span class="va">z</span>, r<span class="op">=</span><span class="fl">1</span>,</span>
<span>      axes <span class="op">=</span> <span class="cn">FALSE</span>, xlab<span class="op">=</span><span class="st">" "</span>, ylab<span class="op">=</span><span class="st">" "</span>, box <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>      col <span class="op">=</span> <span class="st">"green"</span>, shade <span class="op">=</span> <span class="fl">.05</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-10-2.png" width="672"></div>
<div id="fisher-linear-discriminant-function" class="section level4" number="3.3.3.1">
<h4>
<span class="header-section-number">3.3.3.1</span> Fisher linear discriminant function<a class="anchor" aria-label="anchor" href="#fisher-linear-discriminant-function"><i class="fas fa-link"></i></a>
</h4>
<p>Ignoring thr constant term in the boundary function, the Fisher linear discriminant function between populations <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> is
<span class="math display">\[L_{jk}(x)=\vec x'\mathbf{S}^{-1}(\vec{\bar x}_k-\vec{\bar x}_j).\]</span></p>
<p>Population means <span class="math inline">\(\mu_k\)</span> are replaced by <strong>the sampled within-group means</strong>. The covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span> is replaced by <strong>the sample covariance matrix</strong> <span class="math inline">\(\mathbf{S}\)</span> assuming that the covariance is the same in all groups.</p>
<p>The fisher discriminator is a scalar-valued, linear function of a vector of data values, <span class="math inline">\(\vec x\)</span>. More generally, when discriminating between <span class="math inline">\(K\)</span> groups, there will be (<strong>at most</strong>) <span class="math inline">\(K-1\)</span> linear discriminators.</p>
<p><strong>In the wine data Example, there are two linear discriminators</strong>.</p>
</div>
</div>
<div id="quadratic-discriminant-analysis-qda" class="section level3" number="3.3.4">
<h3>
<span class="header-section-number">3.3.4</span> Quadratic Discriminant Analysis (QDA)<a class="anchor" aria-label="anchor" href="#quadratic-discriminant-analysis-qda"><i class="fas fa-link"></i></a>
</h3>
<p>Unlike LDA, QDA assumes that each class has its own covariance matrix:
<span class="math display">\[\vec X \sim N_p(\vec \mu_k, \mathbf{\Sigma}_k)\]</span></p>
<p>Under this assumption, the Bayes classifier assigns an observation <span class="math inline">\(\vec X = \vec x\)</span> to the class for which
<span class="math display">\[\delta_k(\vec x)=\log(\pi_k) - \dfrac{1}{2}\log(|\mathbf{\Sigma_k}|)-\dfrac{1}{2}\vec x'\mathbf{\Sigma_k}^{-1}\vec x+ \vec x'\mathbf{\Sigma_k}^{-1}\vec \mu_k-\dfrac{1}{2}\vec \mu_k'\mathbf{\Sigma_k}^{-1}\vec \mu_k\]</span>
is largest.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">wines</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"dataset/Wines.txt"</span>, header <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">colors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"green"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">[</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">]</span></span>
<span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">)</span> </span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="va">qd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda</a></span><span class="op">(</span><span class="va">Class</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">wines</span><span class="op">)</span></span>
<span><span class="va">qdp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">qd</span><span class="op">)</span><span class="op">$</span><span class="va">posterior</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">qdp</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">qdp</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span>, col <span class="op">=</span> <span class="va">colors</span>, cex <span class="op">=</span> <span class="fl">1.25</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Fitted probability of y=1"</span>, cex.lab<span class="op">=</span><span class="fl">1.5</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Fitted probability of y=3"</span>, main <span class="op">=</span> <span class="st">"QDA"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">.42</span>, <span class="fl">4</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">.92</span>,  <span class="fl">.84</span>, <span class="fl">.76</span><span class="op">)</span>, pos <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>      col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"green"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span>, cex<span class="op">=</span><span class="fl">1.25</span>,</span>
<span>      labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Observation codes:"</span>, <span class="st">"y=1 category"</span>, </span>
<span>                 <span class="st">"y=2 category (ref)"</span>, <span class="st">"y=3 category"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">.4</span>, <span class="fl">1.2</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.2</span>, <span class="fl">.7</span>, <span class="fl">.7</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-11-1.png" width="672"></div>
</div>
</div>
<div id="support-vector-machine" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> 10.4 Support Vector Machine<a class="anchor" aria-label="anchor" href="#support-vector-machine"><i class="fas fa-link"></i></a>
</h2>
<p>The support vector machine (SVM) is a group of classification algorithms that
include a variety of parametric and nonparametric models:
* Parametric models include straight lines and similar regression methods.
* The nonparametric methods contain a variety of kernel smoothing techniques.</p>
<p>The programs in R include features that estimate the classification probabilities using <strong>cross validation</strong> as well as <strong>training samples</strong> in machine learning.</p>
<div id="example-linear-svm-classifier" class="section level3" number="3.4.1">
<h3>
<span class="header-section-number">3.4.1</span> Example (linear SVM classifier)<a class="anchor" aria-label="anchor" href="#example-linear-svm-classifier"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">kernlab</span><span class="op">)</span></span>
<span><span class="va">svmex</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span>file <span class="op">=</span> <span class="st">"dataset/svmex.txt"</span>, header <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                    row.names <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="co"># artifical data</span></span>
<span><span class="va">svmex</span> <span class="op">&lt;-</span> <span class="va">svmex</span><span class="op">[</span><span class="va">svmex</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="op">-</span><span class="fl">2</span>, <span class="op">]</span>  <span class="co"># omit outlier</span></span>
<span><span class="va">type</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="va">svmex</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">svmex</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span>      <span class="co"># known types</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">svmex</span><span class="op">[</span>, <span class="fl">1</span> <span class="op">:</span> <span class="fl">2</span><span class="op">]</span>, pch <span class="op">=</span> <span class="fl">16</span>, cex <span class="op">=</span> <span class="fl">1.75</span>,</span>
<span>     col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">[</span><span class="va">type</span><span class="op">]</span>, xlab <span class="op">=</span> <span class="st">" "</span>, </span>
<span>     ylab <span class="op">=</span> <span class="st">" "</span>, xaxt <span class="op">=</span> <span class="st">"n"</span>, yaxt <span class="op">=</span> <span class="st">"n"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># line A: does not discriminate completely</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">.5</span>,<span class="fl">.5</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">2.5</span>, <span class="fl">3.25</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"l"</span>, </span>
<span>      col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">.25</span>, <span class="fl">2.5</span>, label <span class="op">=</span> <span class="st">"A"</span>, cex <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># line B: is not maximum margins</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">.5</span>, <span class="fl">.5</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3.25</span>, <span class="op">-</span><span class="fl">2.5</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"l"</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="op">-</span><span class="fl">.5</span>, <span class="fl">2.5</span>, labels <span class="op">=</span> <span class="st">"B"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">sv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html">ksvm</a></span><span class="op">(</span><span class="va">type</span> <span class="op">~</span> <span class="va">.</span> , data <span class="op">=</span> <span class="va">svmex</span>, type <span class="op">=</span> <span class="st">"C-svc"</span>, </span>
<span>           prob.model <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># Extract a and b from the model    </span></span>
<span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">colSums</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">sv</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">*</span> <span class="va">svmex</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm-class.html">SVindex</a></span><span class="op">(</span><span class="va">sv</span><span class="op">)</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm-class.html">b</a></span><span class="op">(</span><span class="va">sv</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span> <span class="va">b</span> <span class="op">/</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,  <span class="op">-</span><span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">/</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, </span>
<span>        col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> <span class="co"># maximum margin line</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.4</span>, <span class="fl">1.7</span>, labels <span class="op">=</span> <span class="st">"C"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span> <span class="op">(</span><span class="va">b</span> <span class="op">+</span> <span class="fl">6.9</span><span class="op">)</span> <span class="op">/</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="op">-</span><span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">/</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="co"># upper margin</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span> <span class="op">(</span><span class="va">b</span> <span class="op">-</span> <span class="fl">6.9</span><span class="op">)</span> <span class="op">/</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="op">-</span><span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">/</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="co"># lower margin</span></span>
<span></span>
<span><span class="co"># circle the support vectors</span></span>
<span><span class="va">ai</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm-class.html">alphaindex</a></span><span class="op">(</span><span class="va">sv</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>    <span class="co"># indices for support vectors</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">svmex</span><span class="op">[</span><span class="va">ai</span>, <span class="op">]</span>, pch <span class="op">=</span> <span class="fl">1</span>, cex <span class="op">=</span> <span class="fl">2.00</span><span class="op">)</span>   <span class="co"># three thick circles</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">svmex</span><span class="op">[</span><span class="va">ai</span>, <span class="op">]</span>, pch <span class="op">=</span> <span class="fl">1</span>, cex <span class="op">=</span> <span class="fl">2.25</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">svmex</span><span class="op">[</span><span class="va">ai</span>, <span class="op">]</span>, pch <span class="op">=</span> <span class="fl">1</span>, cex <span class="op">=</span> <span class="fl">2.50</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-12-1.png" width="672"></div>
<ul>
<li>Line A does not completely classify the red and blue groups of samples.</li>
<li>Line B classifies correctly but has small margins for misclassification.</li>
<li>Line C, the product of a SVM, provides the largest possible <strong>margins</strong>. These margins are given with dotted lines.</li>
</ul>
<p><strong>Support vectors</strong>:
(1) observations that are closest to the margins are most important to the determination of the classification regions
(2) observations that define and lie on the periphery of the regions of different colored points</p>
</div>
<div id="example-kernel-svm-classifier" class="section level3" number="3.4.2">
<h3>
<span class="header-section-number">3.4.2</span> Example (kernel SVM classifier)<a class="anchor" aria-label="anchor" href="#example-kernel-svm-classifier"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">kernlab</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">250</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, </span>
<span>             sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">.1</span>, <span class="fl">.1</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fl">2</span> , <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">wavy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">pi</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span>               <span class="co"># wavy boundary</span></span>
<span><span class="va">include</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span> <span class="va">wavy</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">.25</span> <span class="op">)</span><span class="co"># 2x width of boundary</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">include</span>, <span class="op">]</span>                      <span class="co"># include those inside </span></span>
<span><span class="va">wavy</span> <span class="op">&lt;-</span> <span class="va">wavy</span><span class="op">[</span><span class="va">include</span><span class="op">]</span>                 <span class="co">#    the wavy boundary</span></span>
<span><span class="va">group</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="va">wavy</span><span class="op">)</span>          <span class="co"># above or below boundary?</span></span>
<span><span class="va">all</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span><span class="op">]</span>, </span>
<span>                  group <span class="op">=</span> <span class="va">group</span><span class="op">)</span>                     <span class="co"># build a data.frame</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">all</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x2"</span>, <span class="st">"x1"</span>, <span class="st">"group"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">sv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html">ksvm</a></span><span class="op">(</span><span class="va">group</span> <span class="op">~</span> <span class="va">.</span> , data <span class="op">=</span> <span class="va">all</span>, type <span class="op">=</span> <span class="st">"C-svc"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x1.lim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">all</span><span class="op">$</span><span class="va">x1</span><span class="op">)</span></span>
<span><span class="va">x2.lim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">all</span><span class="op">$</span><span class="va">x2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">sv</span>, data <span class="op">=</span> <span class="va">all</span>, xlim <span class="op">=</span> <span class="va">x1.lim</span>, ylim <span class="op">=</span> <span class="va">x2.lim</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-13-1.png" width="672"></div>
<ul>
<li><p><strong>Kernel smoothing</strong> makes <strong>no assumption</strong> on the shape of the regions. A functional form for the boundary between these two regions is not specified, but the SVM program clearly locates it.</p></li>
<li><p>The kernel smooth produced by SVM classifies future observations with <strong>confidence</strong> based on the intensity of the plotted color. The lighter-colored regions have low confidence and white represents complete indifference, as in the wavy region across the center.</p></li>
<li><p>The groups above and below the sine wave are plotted as <strong>circles</strong> and <strong>triangles</strong>, respectively. Those observations are identified as <strong>support vectors</strong>. These are critical to <strong>estimating the boundaries</strong> plotted in solid.</p></li>
</ul>
</div>
<div id="cross-validation" class="section level3" number="3.4.3">
<h3>
<span class="header-section-number">3.4.3</span> Cross Validation<a class="anchor" aria-label="anchor" href="#cross-validation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Cross Validation is a method of testing the accuracy of a classification model</p></li>
<li><p>How well does the model correctly classify some future observations that were not part of the model building process?</p></li>
<li><p>An <strong><span class="math inline">\(K\)</span>-fold cross validation</strong> randomly partitions a data set into <span class="math inline">\(K\)</span> nearly equally sized sets. (Typically, <span class="math inline">\(K\)</span> is an integer in the range between 5 and 10.)</p></li>
<li><p>One set, called <strong>the testing data set</strong>, is omitted.</p></li>
<li><p>The model is fitted using the remaining <span class="math inline">\(K-1\)</span> sets, which are referred to as <strong>the training data set</strong>.</p></li>
<li><p>Once the training data set is fitted, cross validation examines how well the resulting model classifies the observations in the omitted, testing data set.</p></li>
<li><p>The cross-validation process then systematically replaces the omitted
data and removes a different set. This process is <strong>repeated</strong>, separately fitting
and testing <span class="math inline">\(K\)</span> different models.</p></li>
<li><p>The process of cross validation produces an estimate of the accuracy of the model but also has a hidden benefit, referred to as <strong>machine learning</strong>. In the process of producing an <span class="math inline">\(K\)</span>-fold estimate of the classification error, the SVC (support vector classifier) also had to fit <span class="math inline">\(K\)</span> different models. Some of these models may have better properties than the original model fitted from the entire data. In this manner, the SVM appears to learn about better models.</p></li>
</ul>
</div>
<div id="application-to-the-wine-data" class="section level3" number="3.4.4">
<h3>
<span class="header-section-number">3.4.4</span> Application to the wine data<a class="anchor" aria-label="anchor" href="#application-to-the-wine-data"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">wines</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"dataset/Wines.txt"</span>, header <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/levels.html">levels</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">:</span> <span class="fl">3</span>     <span class="co"># grouping variable</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"factor"</span>   <span class="co"># classify the variable type</span></span>
<span><span class="va">colors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"green"</span>, <span class="st">"red"</span>,<span class="st">" blue"</span><span class="op">)</span><span class="op">[</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span><span class="op">]</span></span>
<span></span>
<span><span class="va">svw</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html">ksvm</a></span><span class="op">(</span><span class="va">Class</span> <span class="op">~</span> <span class="va">.</span> , data <span class="op">=</span> <span class="va">wines</span>, type <span class="op">=</span> <span class="st">"C-svc"</span>, </span>
<span>            prob.model <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The alphaindex() command produces three separate lists of support vectors: one for each of the three cultivars. There may be some duplication when support vectors are on the boundary for more than one of the three group.</span></span>
<span><span class="va">svi</span> <span class="op">&lt;-</span> <span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm-class.html">alphaindex</a></span><span class="op">(</span><span class="va">svw</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm-class.html">alphaindex</a></span><span class="op">(</span><span class="va">svw</span><span class="op">)</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span>, </span>
<span>           <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm-class.html">alphaindex</a></span><span class="op">(</span><span class="va">svw</span><span class="op">)</span><span class="op">[[</span><span class="fl">3</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>   <span class="co"># list of all support vectors</span></span>
<span><span class="va">svi</span> <span class="op">&lt;-</span> <span class="va">svi</span><span class="op">[</span> <span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/duplicated.html">duplicated</a></span><span class="op">(</span><span class="va">svi</span><span class="op">)</span><span class="op">]</span>    <span class="co"># remove duplicates</span></span>
<span></span>
<span><span class="va">svwine</span> <span class="op">&lt;-</span> <span class="va">wines</span><span class="op">[</span><span class="va">svi</span>, <span class="op">]</span>           <span class="co"># subset support vectors</span></span>
<span><span class="va">svwine</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/jitter.html">jitter</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="va">svwine</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co"># jitter Class</span></span>
<span><span class="va">svcol</span> <span class="op">&lt;-</span> <span class="va">colors</span><span class="op">[</span><span class="va">svi</span><span class="op">]</span>             <span class="co"># and their colors</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">svwine</span>, pch <span class="op">=</span> <span class="fl">16</span>, cex <span class="op">=</span> <span class="fl">.3</span>, gap <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="va">svcol</span>, </span>
<span>      xaxt <span class="op">=</span> <span class="st">"n"</span>, yaxt <span class="op">=</span> <span class="st">"n"</span><span class="op">)</span> <span class="co"># 69 observations out of 178 are support vectors</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-14-1.png" width="672"></div>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># 5-fold CV</span></span>
<span><span class="op">(</span><span class="va">svw.CV</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html">ksvm</a></span><span class="op">(</span><span class="va">Class</span> <span class="op">~</span> <span class="va">.</span> , data <span class="op">=</span> <span class="va">wines</span>, </span>
<span>                type <span class="op">=</span> <span class="st">"C-svc"</span>, cross <span class="op">=</span> <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Support Vector Machine object of class "ksvm" </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; SV type: C-svc  (classification) </span></span>
<span><span class="co">#&gt;  parameter : cost C = 1 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Gaussian Radial Basis kernel function. </span></span>
<span><span class="co">#&gt;  Hyperparameter : sigma =  0.0615286159790621 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Support Vectors : 68 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Objective Function Value : -12.774 -4.4679 -12.7795 </span></span>
<span><span class="co">#&gt; Training error : 0 </span></span>
<span><span class="co">#&gt; Cross validation error : 0.022381</span></span>
<span></span>
<span><span class="co"># prob.model = T: produce estimated multinomial prediction probabilities of individual observations belonging to each of the different groups.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">svw</span>, <span class="va">wines</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span>, </span>
<span>              type <span class="op">=</span> <span class="st">"probabilities"</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt;              1       2       3</span></span>
<span><span class="co">#&gt;   [1,] 0.99179 0.00304 0.00517</span></span>
<span><span class="co">#&gt;   [2,] 0.98183 0.01064 0.00753</span></span>
<span><span class="co">#&gt;   [3,] 0.99254 0.00358 0.00389</span></span>
<span><span class="co">#&gt;   [4,] 0.97070 0.01823 0.01108</span></span>
<span><span class="co">#&gt;   [5,] 0.97536 0.01766 0.00698</span></span>
<span><span class="co">#&gt;   [6,] 0.99636 0.00106 0.00259</span></span>
<span><span class="co">#&gt;   [7,] 0.99633 0.00054 0.00313</span></span>
<span><span class="co">#&gt;   [8,] 0.99375 0.00105 0.00520</span></span>
<span><span class="co">#&gt;   [9,] 0.99287 0.00320 0.00393</span></span>
<span><span class="co">#&gt;  [10,] 0.99494 0.00206 0.00300</span></span>
<span><span class="co">#&gt;  [11,] 0.99445 0.00208 0.00347</span></span>
<span><span class="co">#&gt;  [12,] 0.99033 0.00319 0.00648</span></span>
<span><span class="co">#&gt;  [13,] 0.99667 0.00085 0.00247</span></span>
<span><span class="co">#&gt;  [14,] 0.97139 0.01806 0.01055</span></span>
<span><span class="co">#&gt;  [15,] 0.97454 0.01470 0.01076</span></span>
<span><span class="co">#&gt;  [16,] 0.99315 0.00194 0.00491</span></span>
<span><span class="co">#&gt;  [17,] 0.99236 0.00195 0.00569</span></span>
<span><span class="co">#&gt;  [18,] 0.99169 0.00335 0.00496</span></span>
<span><span class="co">#&gt;  [19,] 0.97091 0.01821 0.01088</span></span>
<span><span class="co">#&gt;  [20,] 0.99531 0.00125 0.00345</span></span>
<span><span class="co">#&gt;  [21,] 0.98287 0.01096 0.00618</span></span>
<span><span class="co">#&gt;  [22,] 0.97463 0.01760 0.00777</span></span>
<span><span class="co">#&gt;  [23,] 0.99660 0.00114 0.00227</span></span>
<span><span class="co">#&gt;  [24,] 0.97647 0.01757 0.00596</span></span>
<span><span class="co">#&gt;  [25,] 0.97589 0.01870 0.00541</span></span>
<span><span class="co">#&gt;  [26,] 0.87598 0.11443 0.00959</span></span>
<span><span class="co">#&gt;  [27,] 0.99553 0.00109 0.00338</span></span>
<span><span class="co">#&gt;  [28,] 0.97540 0.01493 0.00967</span></span>
<span><span class="co">#&gt;  [29,] 0.99504 0.00232 0.00265</span></span>
<span><span class="co">#&gt;  [30,] 0.99552 0.00184 0.00264</span></span>
<span><span class="co">#&gt;  [31,] 0.97369 0.01781 0.00850</span></span>
<span><span class="co">#&gt;  [32,] 0.99398 0.00209 0.00393</span></span>
<span><span class="co">#&gt;  [33,] 0.98856 0.00792 0.00352</span></span>
<span><span class="co">#&gt;  [34,] 0.97230 0.01780 0.00990</span></span>
<span><span class="co">#&gt;  [35,] 0.99326 0.00177 0.00496</span></span>
<span><span class="co">#&gt;  [36,] 0.98953 0.00769 0.00278</span></span>
<span><span class="co">#&gt;  [37,] 0.98211 0.00830 0.00959</span></span>
<span><span class="co">#&gt;  [38,] 0.97270 0.01766 0.00963</span></span>
<span><span class="co">#&gt;  [39,] 0.90030 0.09082 0.00888</span></span>
<span><span class="co">#&gt;  [40,] 0.97118 0.01812 0.01069</span></span>
<span><span class="co">#&gt;  [41,] 0.99081 0.00583 0.00336</span></span>
<span><span class="co">#&gt;  [42,] 0.97244 0.01775 0.00981</span></span>
<span><span class="co">#&gt;  [43,] 0.99461 0.00177 0.00362</span></span>
<span><span class="co">#&gt;  [44,] 0.95655 0.03276 0.01069</span></span>
<span><span class="co">#&gt;  [45,] 0.97768 0.01908 0.00323</span></span>
<span><span class="co">#&gt;  [46,] 0.98714 0.00312 0.00974</span></span>
<span><span class="co">#&gt;  [47,] 0.99708 0.00105 0.00187</span></span>
<span><span class="co">#&gt;  [48,] 0.99415 0.00325 0.00259</span></span>
<span><span class="co">#&gt;  [49,] 0.99584 0.00127 0.00289</span></span>
<span><span class="co">#&gt;  [50,] 0.99339 0.00231 0.00430</span></span>
<span><span class="co">#&gt;  [51,] 0.96385 0.02591 0.01024</span></span>
<span><span class="co">#&gt;  [52,] 0.99495 0.00140 0.00365</span></span>
<span><span class="co">#&gt;  [53,] 0.99067 0.00495 0.00438</span></span>
<span><span class="co">#&gt;  [54,] 0.99642 0.00072 0.00286</span></span>
<span><span class="co">#&gt;  [55,] 0.99369 0.00170 0.00461</span></span>
<span><span class="co">#&gt;  [56,] 0.98562 0.00812 0.00626</span></span>
<span><span class="co">#&gt;  [57,] 0.99524 0.00167 0.00309</span></span>
<span><span class="co">#&gt;  [58,] 0.99732 0.00060 0.00208</span></span>
<span><span class="co">#&gt;  [59,] 0.99211 0.00348 0.00441</span></span>
<span><span class="co">#&gt;  [60,] 0.01343 0.97274 0.01383</span></span>
<span><span class="co">#&gt;  [61,] 0.00312 0.98886 0.00802</span></span>
<span><span class="co">#&gt;  [62,] 0.00329 0.95196 0.04476</span></span>
<span><span class="co">#&gt;  [63,] 0.01480 0.97974 0.00546</span></span>
<span><span class="co">#&gt;  [64,] 0.00240 0.99269 0.00491</span></span>
<span><span class="co">#&gt;  [65,] 0.00211 0.99543 0.00246</span></span>
<span><span class="co">#&gt;  [66,] 0.06241 0.93281 0.00478</span></span>
<span><span class="co">#&gt;  [67,] 0.01650 0.97573 0.00777</span></span>
<span><span class="co">#&gt;  [68,] 0.00320 0.99439 0.00241</span></span>
<span><span class="co">#&gt;  [69,] 0.01184 0.97232 0.01584</span></span>
<span><span class="co">#&gt;  [70,] 0.01389 0.97352 0.01258</span></span>
<span><span class="co">#&gt;  [71,] 0.00751 0.94137 0.05112</span></span>
<span><span class="co">#&gt;  [72,] 0.01665 0.97478 0.00858</span></span>
<span><span class="co">#&gt;  [73,] 0.00744 0.97997 0.01258</span></span>
<span><span class="co">#&gt;  [74,] 0.01669 0.97437 0.00894</span></span>
<span><span class="co">#&gt;  [75,] 0.01635 0.97750 0.00615</span></span>
<span><span class="co">#&gt;  [76,] 0.00213 0.99513 0.00274</span></span>
<span><span class="co">#&gt;  [77,] 0.00405 0.99233 0.00362</span></span>
<span><span class="co">#&gt;  [78,] 0.00490 0.98216 0.01294</span></span>
<span><span class="co">#&gt;  [79,] 0.01601 0.97337 0.01061</span></span>
<span><span class="co">#&gt;  [80,] 0.01603 0.98158 0.00239</span></span>
<span><span class="co">#&gt;  [81,] 0.00068 0.99784 0.00148</span></span>
<span><span class="co">#&gt;  [82,] 0.10655 0.89070 0.00274</span></span>
<span><span class="co">#&gt;  [83,] 0.00178 0.99598 0.00224</span></span>
<span><span class="co">#&gt;  [84,] 0.01098 0.63121 0.35781</span></span>
<span><span class="co">#&gt;  [85,] 0.01131 0.98254 0.00615</span></span>
<span><span class="co">#&gt;  [86,] 0.00695 0.99189 0.00116</span></span>
<span><span class="co">#&gt;  [87,] 0.00072 0.99813 0.00116</span></span>
<span><span class="co">#&gt;  [88,] 0.00279 0.99426 0.00294</span></span>
<span><span class="co">#&gt;  [89,] 0.00213 0.99525 0.00262</span></span>
<span><span class="co">#&gt;  [90,] 0.00158 0.99652 0.00190</span></span>
<span><span class="co">#&gt;  [91,] 0.00120 0.99653 0.00226</span></span>
<span><span class="co">#&gt;  [92,] 0.00131 0.99322 0.00548</span></span>
<span><span class="co">#&gt;  [93,] 0.00224 0.98503 0.01273</span></span>
<span><span class="co">#&gt;  [94,] 0.00341 0.99561 0.00098</span></span>
<span><span class="co">#&gt;  [95,] 0.00238 0.99462 0.00300</span></span>
<span><span class="co">#&gt;  [96,] 0.01639 0.97205 0.01156</span></span>
<span><span class="co">#&gt;  [97,] 0.01163 0.97446 0.01390</span></span>
<span><span class="co">#&gt;  [98,] 0.00212 0.99681 0.00107</span></span>
<span><span class="co">#&gt;  [99,] 0.01621 0.97832 0.00547</span></span>
<span><span class="co">#&gt; [100,] 0.00674 0.98867 0.00459</span></span>
<span><span class="co">#&gt; [101,] 0.01417 0.98321 0.00262</span></span>
<span><span class="co">#&gt; [102,] 0.00340 0.99441 0.00220</span></span>
<span><span class="co">#&gt; [103,] 0.01596 0.98005 0.00399</span></span>
<span><span class="co">#&gt; [104,] 0.00067 0.99810 0.00123</span></span>
<span><span class="co">#&gt; [105,] 0.00951 0.98910 0.00139</span></span>
<span><span class="co">#&gt; [106,] 0.00342 0.99003 0.00655</span></span>
<span><span class="co">#&gt; [107,] 0.00317 0.99565 0.00118</span></span>
<span><span class="co">#&gt; [108,] 0.00236 0.99009 0.00755</span></span>
<span><span class="co">#&gt; [109,] 0.00121 0.99740 0.00138</span></span>
<span><span class="co">#&gt; [110,] 0.01609 0.98074 0.00316</span></span>
<span><span class="co">#&gt; [111,] 0.01658 0.97255 0.01087</span></span>
<span><span class="co">#&gt; [112,] 0.00157 0.99705 0.00139</span></span>
<span><span class="co">#&gt; [113,] 0.01465 0.97192 0.01343</span></span>
<span><span class="co">#&gt; [114,] 0.00081 0.99644 0.00276</span></span>
<span><span class="co">#&gt; [115,] 0.00157 0.99613 0.00230</span></span>
<span><span class="co">#&gt; [116,] 0.00390 0.98605 0.01005</span></span>
<span><span class="co">#&gt; [117,] 0.00106 0.99796 0.00098</span></span>
<span><span class="co">#&gt; [118,] 0.00123 0.99796 0.00081</span></span>
<span><span class="co">#&gt; [119,] 0.00964 0.93923 0.05113</span></span>
<span><span class="co">#&gt; [120,] 0.00550 0.99305 0.00144</span></span>
<span><span class="co">#&gt; [121,] 0.01328 0.98465 0.00207</span></span>
<span><span class="co">#&gt; [122,] 0.01670 0.97311 0.01019</span></span>
<span><span class="co">#&gt; [123,] 0.01207 0.97584 0.01209</span></span>
<span><span class="co">#&gt; [124,] 0.01673 0.97337 0.00990</span></span>
<span><span class="co">#&gt; [125,] 0.01076 0.98275 0.00650</span></span>
<span><span class="co">#&gt; [126,] 0.00131 0.99795 0.00074</span></span>
<span><span class="co">#&gt; [127,] 0.00370 0.99251 0.00379</span></span>
<span><span class="co">#&gt; [128,] 0.00633 0.98155 0.01213</span></span>
<span><span class="co">#&gt; [129,] 0.00070 0.99828 0.00102</span></span>
<span><span class="co">#&gt; [130,] 0.00596 0.98104 0.01300</span></span>
<span><span class="co">#&gt; [131,] 0.01086 0.23991 0.74923</span></span>
<span><span class="co">#&gt; [132,] 0.00419 0.01436 0.98145</span></span>
<span><span class="co">#&gt; [133,] 0.00432 0.01668 0.97900</span></span>
<span><span class="co">#&gt; [134,] 0.00861 0.02351 0.96788</span></span>
<span><span class="co">#&gt; [135,] 0.00898 0.22868 0.76234</span></span>
<span><span class="co">#&gt; [136,] 0.00504 0.02333 0.97163</span></span>
<span><span class="co">#&gt; [137,] 0.00629 0.02247 0.97124</span></span>
<span><span class="co">#&gt; [138,] 0.01062 0.02348 0.96591</span></span>
<span><span class="co">#&gt; [139,] 0.00421 0.02231 0.97348</span></span>
<span><span class="co">#&gt; [140,] 0.01056 0.03316 0.95628</span></span>
<span><span class="co">#&gt; [141,] 0.00703 0.02339 0.96959</span></span>
<span><span class="co">#&gt; [142,] 0.01155 0.02380 0.96466</span></span>
<span><span class="co">#&gt; [143,] 0.00742 0.01659 0.97599</span></span>
<span><span class="co">#&gt; [144,] 0.01180 0.04021 0.94800</span></span>
<span><span class="co">#&gt; [145,] 0.01271 0.02416 0.96313</span></span>
<span><span class="co">#&gt; [146,] 0.00662 0.02345 0.96993</span></span>
<span><span class="co">#&gt; [147,] 0.01086 0.02361 0.96553</span></span>
<span><span class="co">#&gt; [148,] 0.00378 0.00266 0.99356</span></span>
<span><span class="co">#&gt; [149,] 0.00256 0.00112 0.99631</span></span>
<span><span class="co">#&gt; [150,] 0.00469 0.00396 0.99135</span></span>
<span><span class="co">#&gt; [151,] 0.00958 0.01653 0.97389</span></span>
<span><span class="co">#&gt; [152,] 0.00878 0.01312 0.97810</span></span>
<span><span class="co">#&gt; [153,] 0.01105 0.02361 0.96534</span></span>
<span><span class="co">#&gt; [154,] 0.01139 0.01739 0.97122</span></span>
<span><span class="co">#&gt; [155,] 0.00639 0.02333 0.97028</span></span>
<span><span class="co">#&gt; [156,] 0.00795 0.01273 0.97931</span></span>
<span><span class="co">#&gt; [157,] 0.00660 0.00720 0.98620</span></span>
<span><span class="co">#&gt; [158,] 0.01081 0.01433 0.97486</span></span>
<span><span class="co">#&gt; [159,] 0.01221 0.04224 0.94554</span></span>
<span><span class="co">#&gt; [160,] 0.01065 0.02192 0.96744</span></span>
<span><span class="co">#&gt; [161,] 0.00487 0.00862 0.98651</span></span>
<span><span class="co">#&gt; [162,] 0.00748 0.00961 0.98291</span></span>
<span><span class="co">#&gt; [163,] 0.00578 0.01640 0.97781</span></span>
<span><span class="co">#&gt; [164,] 0.00304 0.00561 0.99135</span></span>
<span><span class="co">#&gt; [165,] 0.00467 0.00350 0.99183</span></span>
<span><span class="co">#&gt; [166,] 0.00365 0.00717 0.98917</span></span>
<span><span class="co">#&gt; [167,] 0.00708 0.00630 0.98662</span></span>
<span><span class="co">#&gt; [168,] 0.00591 0.00541 0.98867</span></span>
<span><span class="co">#&gt; [169,] 0.00619 0.00489 0.98892</span></span>
<span><span class="co">#&gt; [170,] 0.01251 0.01622 0.97127</span></span>
<span><span class="co">#&gt; [171,] 0.00316 0.02327 0.97356</span></span>
<span><span class="co">#&gt; [172,] 0.00690 0.00815 0.98495</span></span>
<span><span class="co">#&gt; [173,] 0.00654 0.00354 0.98992</span></span>
<span><span class="co">#&gt; [174,] 0.00723 0.00486 0.98790</span></span>
<span><span class="co">#&gt; [175,] 0.00275 0.00128 0.99596</span></span>
<span><span class="co">#&gt; [176,] 0.01078 0.01043 0.97878</span></span>
<span><span class="co">#&gt; [177,] 0.00805 0.00640 0.98556</span></span>
<span><span class="co">#&gt; [178,] 0.00663 0.00591 0.98746</span></span>
<span></span>
<span><span class="co"># Plot fitted probabilities and residuals</span></span>
<span></span>
<span><span class="co"># Fitted probabilities:</span></span>
<span><span class="va">prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">svw</span>, <span class="va">wines</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span>, type <span class="op">=</span> <span class="st">"probabilities"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">prob</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">prob</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span>, col <span class="op">=</span> <span class="va">colors</span>, cex <span class="op">=</span> <span class="fl">1.25</span>, cex.lab <span class="op">=</span> <span class="fl">1.5</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Fitted  probability of  y=1"</span>,  </span>
<span>     ylab <span class="op">=</span> <span class="st">"Fitted probability of y=3"</span>, main<span class="op">=</span><span class="st">"SVM"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">.42</span>, <span class="fl">4</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">.92</span>,  <span class="fl">.84</span>, <span class="fl">.76</span><span class="op">)</span>, pos <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>      col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"green"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span>,</span>
<span>      labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Observation codes:"</span>, <span class="st">"y=1 category"</span>, </span>
<span>                 <span class="st">"y=2 category (ref)"</span>, <span class="st">"y=3 category"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">.4</span>, <span class="fl">1.2</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.2</span>, <span class="fl">.7</span>, <span class="fl">.7</span><span class="op">)</span><span class="op">)</span>  <span class="co">#  box around the text</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dotted"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-14-2.png" width="672"></div>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Residual plot</span></span>
<span></span>
<span><span class="va">nmlr</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">fit</span>, <span class="va">obs</span><span class="op">)</span><span class="op">{</span> <span class="co"># normalized  multivariate logistic residuals</span></span>
<span>  <span class="va">nmlr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>,nrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, ncol <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>  </span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span> <span class="op">:</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span> <span class="op">:</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>      <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">fit</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span>            <span class="co"># fitted p parameter</span></span>
<span>      <span class="va">ob</span> <span class="op">&lt;-</span> <span class="va">obs</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span></span>
<span>      <span class="va">res</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">ob</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span>  <span class="op">/</span>  <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span> <span class="va">p</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span> <span class="op">)</span> <span class="op">)</span></span>
<span>      <span class="va">nmlr</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">res</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">nmlr</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">obs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>             <span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">Class</span> <span class="op">==</span> <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">obs</span><span class="op">)</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"1"</span>, <span class="st">"3"</span><span class="op">)</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu">nmlr</span><span class="op">(</span><span class="va">prob</span><span class="op">[</span>, <span class="fl">2</span> <span class="op">:</span> <span class="fl">3</span><span class="op">]</span>, <span class="va">obs</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">res</span>, col <span class="op">=</span> <span class="va">colors</span>, cex <span class="op">=</span> <span class="fl">1.25</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Std residual for fitted probability of y=1"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Std residual for y=3"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-14-3.png" width="672"></div>
<p>The residual plot is not useful and exhibits a distorted horizontal scale, owing to the small probabilities involved.</p>
<p><strong>Pros</strong>:
* extremely flexible
* able to detect the wavy boundary without any additional specification
* there are many options and different kernel smoothing functions available for SVM method.</p>
<p><strong>Cons</strong>:
* SVM produces estimates but no model that can be expressed as a mathematical formula.</p>
</div>
<div id="maximal-margin-classifier" class="section level3" number="3.4.5">
<h3>
<span class="header-section-number">3.4.5</span> Maximal Margin Classifier<a class="anchor" aria-label="anchor" href="#maximal-margin-classifier"><i class="fas fa-link"></i></a>
</h3>
<p>The support vector machine is a generalization of a simple and intuitive classifier called the <strong>maximal margin classifier</strong>.</p>
<div id="what-is-a-hyperplane" class="section level4" number="3.4.5.1">
<h4>
<span class="header-section-number">3.4.5.1</span> What Is a Hyperplane?<a class="anchor" aria-label="anchor" href="#what-is-a-hyperplane"><i class="fas fa-link"></i></a>
</h4>
<p>In a <span class="math inline">\(p\)</span>-dimensional space, a <strong>hyperplane</strong> is a flat subspace of dimension <span class="math inline">\(p-1\)</span>.</p>
<p>In two dimensions, a hyperplane is a line, defined by the equation
<span class="math display">\[\beta_0+\beta_1X_1+\beta_2X_2=0\]</span></p>
<p>A hyperplane for a <span class="math inline">\(p\)</span>-dimensional space:
<span class="math display">\[\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p=0\]</span></p>
<p>If a point <span class="math inline">\(\vec X =(X_1,X_2,\cdots , X_p)^T\)</span> in <span class="math inline">\(p\)</span>-dimensional space (i.e. a vector of length <span class="math inline">\(p\)</span>) satisfies
<span class="math inline">\(\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p=0\)</span>, then <span class="math inline">\(\vec X\)</span> lies on the hyperplane.</p>
<p>If a point <span class="math inline">\(\vec X =(X_1,X_2,\cdots , X_p)^T\)</span> satisfies <span class="math inline">\(\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p&lt;0\)</span>, then <span class="math inline">\(\vec X\)</span> lies to one side of the hyperplane.</p>
<p>On the other hand, if a point <span class="math inline">\(\vec X =(X_1,X_2,\cdots, X_p)^T\)</span> satisfies <span class="math inline">\(\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p&gt;0\)</span>, then <span class="math inline">\(\vec X\)</span> lies on the other side of the hyperplane.</p>
<div class="inline-figure"><img src="images/9.1-1.jpg" width="400"></div>
<ul>
<li>Solid line: <span class="math inline">\(1+2X_1+3X_2=0\)</span>
</li>
<li>Blue: <span class="math inline">\(1+2X_1+3X_2&gt;0\)</span>
</li>
<li>Purple: <span class="math inline">\(1+2X_1+3X_2&lt;0\)</span>
</li>
</ul>
</div>
<div id="classification-using-a-separating-hyperplane" class="section level4" number="3.4.5.2">
<h4>
<span class="header-section-number">3.4.5.2</span> Classification Using a Separating Hyperplane<a class="anchor" aria-label="anchor" href="#classification-using-a-separating-hyperplane"><i class="fas fa-link"></i></a>
</h4>
<p>Now suppose that we have a <span class="math inline">\(n\times p\)</span> data matrix <span class="math inline">\(\mathbf{X}=\{x_{ij}, i=1,2,\dots,n,j=1,2,\dots,p\}\)</span> that consists of <span class="math inline">\(n\)</span> training observations in <span class="math inline">\(p\)</span>-dimensional space, and that these observations fall into two classes-that is, <span class="math inline">\(y_1,\dots, y_n \in \{-1, 1\}\)</span> where <span class="math inline">\(-1\)</span> represents one class and <span class="math inline">\(1\)</span> the other class.</p>
<p>A <strong>separating hyperplane</strong> has the property that
<span class="math display">\[\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}&gt;0 \text{ if } y_i=1\]</span>
and
<span class="math display">\[\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}&lt;0 \text{ if } y_i=-1.\]</span>
Equivalently, a separating hyperplane has the property that
<span class="math display">\[y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip})&gt;0\]</span>
for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
</div>
<div id="the-maximal-margin-classifier" class="section level4" number="3.4.5.3">
<h4>
<span class="header-section-number">3.4.5.3</span> The Maximal Margin Classifier<a class="anchor" aria-label="anchor" href="#the-maximal-margin-classifier"><i class="fas fa-link"></i></a>
</h4>
<p>We can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the <strong>margin</strong>.</p>
<p>The <strong>maximal margin hyperplane</strong> (also known as the <strong>optimal separating hyperplane</strong>), which is the separating hyperplane that is farthest from the training observations (the margin is the largest). This is known as the <strong>maximal margin classifier</strong>.</p>
<div class="inline-figure"><img src="images/9.3-1.jpg" width="400"></div>
</div>
<div id="construction-of-the-maximal-margin-classifier" class="section level4" number="3.4.5.4">
<h4>
<span class="header-section-number">3.4.5.4</span> Construction of the Maximal Margin Classifier<a class="anchor" aria-label="anchor" href="#construction-of-the-maximal-margin-classifier"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>maximal margin hyperplane</strong> is the solution to the <strong>optimization</strong> problem</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\beta_0,\dots,\beta_p}{\text{maximize}}M\\
&amp; \text{subject to } \sum_{j=1}^p \beta_j^2=1,\\
&amp; y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}) \ge M, \forall i=1,2,\dots,n
\end{aligned}
\]</span></p>
<p>Note that, under <span class="math inline">\(\sum_{j=1}^p \beta_j^2=1\)</span>, the perpendicular distance from the <span class="math inline">\(i\)</span>th observation to the hyperplane is given by
<span class="math display">\[\dfrac{y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip})}{\sqrt{y_i^2\sum_{j=1}^p \beta_j^2}}=y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}).\]</span></p>
<p>Hence, <span class="math inline">\(M\)</span> represents the margin of the hyperplane. This is exactly the definition of the maximal margin hyperplane!</p>
</div>
</div>
<div id="support-vector-classifier" class="section level3" number="3.4.6">
<h3>
<span class="header-section-number">3.4.6</span> Support Vector Classifier<a class="anchor" aria-label="anchor" href="#support-vector-classifier"><i class="fas fa-link"></i></a>
</h3>
<div id="a-non-separable-case" class="section level4" number="3.4.6.1">
<h4>
<span class="header-section-number">3.4.6.1</span> A non-separable case<a class="anchor" aria-label="anchor" href="#a-non-separable-case"><i class="fas fa-link"></i></a>
</h4>
<p>We cannot use a separating hyperplane to exactly separate the two classes:</p>
<div class="inline-figure"><img src="images/9.2-1.jpg" width="600"></div>
<p>The <strong>support vector classifier</strong> is an extension of the maximal margin classifier to the non-separable case.</p>
</div>
<div id="details-of-the-support-vector-classifier" class="section level4" number="3.4.6.2">
<h4>
<span class="header-section-number">3.4.6.2</span> Details of the Support Vector Classifier<a class="anchor" aria-label="anchor" href="#details-of-the-support-vector-classifier"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>support vector classifier</strong>, sometimes called a <strong>soft margin classifier</strong>.</p>
<p>Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane. (<strong>The margin is soft because it can be violated by some of the training observations</strong>.)</p>
<div class="inline-figure"><img src="images/9.6-1.jpg" width="600"></div>
<p>It is the solution to the optimization problem
<span class="math display">\[
\begin{aligned}
&amp; \underset{\beta_0,\dots,\beta_p,\epsilon_1,\dots,\epsilon_n}{\text{maximize}}M\\
&amp; \text{subject to } \sum_{j=1}^p \beta_j^2=1,\\
&amp; y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}) \ge M(1-\epsilon_i)\\
&amp; \epsilon_i\ge0, \sum_{i=1}^n\epsilon_i\le C
\end{aligned}
\]</span></p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(C\)</span> is a nonnegative tuning parameter</li>
<li>
<span class="math inline">\(M\)</span> is the width of the margin; we seek to make this quantity as large as possible.</li>
<li>
<span class="math inline">\(\epsilon_1,\dots,\epsilon_p\)</span> are <strong>slack variables</strong> that allow individual observations to be on the wrong side of the margin or the hyperplane</li>
</ol>
<div id="slack-variable" class="section level5" number="3.4.6.2.1">
<h5>
<span class="header-section-number">3.4.6.2.1</span> slack variable<a class="anchor" aria-label="anchor" href="#slack-variable"><i class="fas fa-link"></i></a>
</h5>
<p>The slack variable <span class="math inline">\(\epsilon_i\)</span> tells us where the <span class="math inline">\(i\)</span>th observation is located, relative to the hyperplane and relative to the margin:
(1) <span class="math inline">\(\epsilon_i=0\)</span>: the <span class="math inline">\(i\)</span>th observation is on the correct side of the margin
(2) <span class="math inline">\(\epsilon_i&gt;0\)</span>: the <span class="math inline">\(i\)</span>th observation is on the wrong side of the margin
(3) <span class="math inline">\(\epsilon_i&gt;1\)</span>: the <span class="math inline">\(i\)</span>th observation is on the wrong side of the hyperplane</p>
</div>
<div id="tuning-parameter" class="section level5" number="3.4.6.2.2">
<h5>
<span class="header-section-number">3.4.6.2.2</span> Tuning parameter<a class="anchor" aria-label="anchor" href="#tuning-parameter"><i class="fas fa-link"></i></a>
</h5>
<p>The tuning parameter <span class="math inline">\(C\)</span> bounds <span class="math inline">\(\sum_{i=1}^n\epsilon_i\)</span>, and so it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate.</p>
<p>We can think of <span class="math inline">\(C\)</span> as a <strong>budget</strong> for the amount that the margin can be violated by the n observations.</p>
<p>A support vector classifier was fit using four different values of the tuning parameter <span class="math inline">\(C\)</span></p>
<div class="inline-figure"><img src="images/9.7-1.jpg" width="600"></div>
</div>
</div>
</div>
<div id="support-vectot-machine" class="section level3" number="3.4.7">
<h3>
<span class="header-section-number">3.4.7</span> Support Vectot Machine<a class="anchor" aria-label="anchor" href="#support-vectot-machine"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Support vector machine</strong> is a further extension of the support vector classifier in order to accommodate <strong>non-linear class boundaries</strong>.</p>
<p>We first discuss a general mechanism for converting a linear classifier into one that produces non-linear decision boundaries. We then introduce the support vector machine, which does this in an <strong>automatic</strong> way.</p>
<div id="classification-with-non-linear-decision-boundaries" class="section level4" number="3.4.7.1">
<h4>
<span class="header-section-number">3.4.7.1</span> Classification with Non-linear Decision Boundaries<a class="anchor" aria-label="anchor" href="#classification-with-non-linear-decision-boundaries"><i class="fas fa-link"></i></a>
</h4>
<p>In practice, we are sometimes faced with non-linear class boundaries.</p>
<div class="inline-figure"><img src="images/9.8-1.jpg" width="600"></div>
<p>In the right panel, the <strong>support vector classifier</strong> seeks a <strong>linear</strong> boundary, and consequently performs very <strong>poorly</strong>.</p>
<p>We could address the problem of possibly non-linear boundaries between classes by enlarging the feature space using quadratic, cubic, and even higher-order polynomial functions of the predictors.</p>
<p>For instance, rather than fitting a support vector classifier using <span class="math inline">\(p\)</span> features
<span class="math display">\[X_1,X_2, \dots, X_p,\]</span>
we could instead fit a support vector classifier using <span class="math inline">\(2p\)</span> features
<span class="math display">\[X_1,X_1^2,X_2,X_2^2, \dots, X_p,X_p^2.\]</span></p>
<p>A non-linear decision boundary is led by solving the following optimization problem
<span class="math display">\[
\begin{aligned}
&amp; \underset{\beta_0,\beta_{11},\beta_{12},\dots,\beta_{p1},\beta_{p2},\epsilon_1,\dots,\epsilon_n}{\text{maximize}}M\\
&amp; \text{subject to } \sum_{j=1}^p \sum_{k=1}^2\beta_{jk}^2=1,\\
&amp; y_i(\beta_0+\sum_{j=1}^p\beta_{j1}x_{ij}+\sum_{j=1}^p\beta_{j2}x_{ij}^2) \ge M(1-\epsilon_i)\\
&amp; \epsilon_i\ge0, \sum_{i=1}^n\epsilon_i\le C
\end{aligned}
\]</span></p>
</div>
<div id="details-of-the-support-vector-machine" class="section level4" number="3.4.7.2">
<h4>
<span class="header-section-number">3.4.7.2</span> Details of the Support Vector Machine<a class="anchor" aria-label="anchor" href="#details-of-the-support-vector-machine"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>support vector machine (SVM)</strong> is an extension of the support vector classifier that results from <strong>enlarging the feature space</strong> in a specific way, using <strong>kernels</strong>.</p>
<p>The kernel approach that we describe here is simply an efficient computational approach for enlarging the feature space.</p>
<p>Let <span class="math inline">\(\vec \beta=(\beta_1,\dots,\beta_p)', \vec x_i=(x_{i1},\dots, x_{ip})'\)</span>. Then, the support vector classifier problem is:
<span class="math display">\[
\begin{aligned}
&amp; \underset{\beta_0,\dots,\beta_p,\epsilon_1,\dots,\epsilon_n}{\text{maximize}}M\\
&amp; \text{subject to } ||\vec \beta||=\sqrt{\vec \beta'\vec \beta}=1,\\
&amp; y_i(\beta_0+\vec x_i'\vec\beta) \ge M(1-\epsilon_i)\\
&amp; \epsilon_i\ge0, \sum_{i=1}^n\epsilon_i\le C
\end{aligned}
\]</span></p>
<p>Or equivalently (by setting <span class="math inline">\(||\vec \beta||=1/M\)</span>),
<span class="math display">\[
\begin{aligned}
&amp; \underset{\beta_0,\vec \beta,\epsilon_1,\dots,\epsilon_n}{\text{minimize}}\dfrac{1}{2}||\vec \beta||^2+C\sum_{i=1}^n\epsilon_i\\
&amp; \text{subject to } y_i(\beta_0+\vec x_i'\vec\beta) \ge 1-\epsilon_i, \epsilon_i\ge0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> controls the tradeoff between maximum margin and loss.</p>
<p>It is a convex optimization problem, which has a quadratic programming solution using Lagrange multipliers.</p>
<p>The Lagrange (primal) function:
<span class="math display">\[L_P=\dfrac{1}{2}||\vec \beta||^2+C\sum_{i=1}^n\epsilon_i-\sum_{i=1}^n\alpha_i[y_i(\beta_0+\vec x_i'\vec\beta) - (1-\epsilon_i)]-\sum_{i=1}^n\mu_i\epsilon_i\]</span></p>
<p>Taking the derivatives with respect to <span class="math inline">\(\beta_0, \vec \beta, \epsilon_i\)</span> and setting them to zeo, we get the Karush-Kuhn-Tucker (KKT) condition 1:
<span class="math display">\[
\begin{aligned}
&amp;\vec \beta=\sum_{i=1}^n\alpha_iy_ix_i,\\
&amp;0=\sum_{i=1}^n\alpha_iy_i,\\
&amp;\alpha_i=C-\mu_i,\forall i
\end{aligned}
\]</span></p>
<p>The Lagrange (Wolfe) dual objective function:
<span class="math display">\[L_D=\sum_{i=1}^n\alpha_i-\dfrac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec x_i' \vec x_j\]</span>
which gives a lower bound on the objective function for any feasible point.</p>
<p>Then, we maximize <span class="math inline">\(L_D\)</span> subject to KKT condition 1, 2, and 3 (<a href="http://mypages.iit.edu/~jwang134/posts/KKT-SVs-SVM.html" class="uri">http://mypages.iit.edu/~jwang134/posts/KKT-SVs-SVM.html</a>) to get the unique solution.</p>
<p>From <span class="math inline">\(\vec \beta=\sum_{i=1}^n\alpha_iy_ix_i\)</span>, we can represent the linear support vector classifier as</p>
<p><span class="math display">\[f(\vec x)=\beta_0+\vec x_i'\vec\beta=\beta_0+\sum_{i=1}^n\alpha_iy_i&lt;\vec x, \vec x_i&gt;\]</span></p>
</div>
<div id="the-property-of-alpha_i" class="section level4" number="3.4.7.3">
<h4>
<span class="header-section-number">3.4.7.3</span> The property of <span class="math inline">\(\alpha_i\)</span><a class="anchor" aria-label="anchor" href="#the-property-of-alpha_i"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>The observations with <span class="math inline">\(\alpha_i=0\)</span> do not contribute to the calculation of <span class="math inline">\(\vec \beta\)</span>.</li>
<li>The observations with <span class="math inline">\(\alpha_i\ne 0\)</span> are called the support vectors.</li>
<li>If the slack variable <span class="math inline">\(\epsilon_i&gt;0\)</span>, <span class="math inline">\(\alpha_i = C\)</span>
</li>
</ul>
</div>
<div id="kernel-enlarge-the-feature-space" class="section level4" number="3.4.7.4">
<h4>
<span class="header-section-number">3.4.7.4</span> Kernel (enlarge the feature space)<a class="anchor" aria-label="anchor" href="#kernel-enlarge-the-feature-space"><i class="fas fa-link"></i></a>
</h4>
<p>We introduce a “bi-vector” function (<strong>kernel function <span class="math inline">\(K(\vec x, \vec x_i)\)</span></strong>) to generalize the inner product. Then, we can represent the classifier as
<span class="math display">\[f(\vec x)=\beta_0+\sum_{i=1}^n\alpha_iy_iK(\vec x, \vec x_i)\]</span></p>
<p>Three popular choices for K in the SVM literature are</p>
<ul>
<li>
<span class="math inline">\(d\)</span>th-Degree polynomial: <span class="math inline">\(K(\vec x, \vec x_i)=(1+&lt;\vec x, \vec x_i&gt;)^d\)</span>,</li>
<li>(Gaussian) Radial basis: <span class="math inline">\(K(\vec x, \vec x_i)=\exp(-\gamma||\vec x-\vec x_i||^2)\)</span>
</li>
<li>Neural network: <span class="math inline">\(K(\vec x, \vec x_i)=\tanh(\kappa_1&lt;\vec x, \vec x_i&gt;+\kappa_2)\)</span>
</li>
</ul>
<div class="inline-figure"><img src="images/9.9-1.jpg" width="600"></div>
<ul>
<li>Left: An SVM with a polynomial kernel of degree 3 is applied to the non-linear data.</li>
<li>Right: An SVM with a radial kernel is applied.</li>
<li>Solid line: SVM</li>
<li>Dashed line: margins of the SVM</li>
</ul>
<p>In this example, either kernel is capable of capturing the decision boundary.</p>
</div>
<div id="svms-with-more-than-two-classes-k2-classes" class="section level4" number="3.4.7.5">
<h4>
<span class="header-section-number">3.4.7.5</span> SVMs with More than Two Classes (<span class="math inline">\(K&gt;2\)</span> classes)<a class="anchor" aria-label="anchor" href="#svms-with-more-than-two-classes-k2-classes"><i class="fas fa-link"></i></a>
</h4>
<div id="one-versus-one-classification" class="section level5" number="3.4.7.5.1">
<h5>
<span class="header-section-number">3.4.7.5.1</span> One-Versus-One Classification<a class="anchor" aria-label="anchor" href="#one-versus-one-classification"><i class="fas fa-link"></i></a>
</h5>
<p>A <strong>one-versus-one</strong> or <strong>all-pairs</strong> approach constructs <strong><span class="math inline">\({K \choose 2}\)</span></strong> SVMs, each of which compares a pair of classes.</p>
</div>
<div id="one-versus-all-classification" class="section level5" number="3.4.7.5.2">
<h5>
<span class="header-section-number">3.4.7.5.2</span> One-Versus-All Classification<a class="anchor" aria-label="anchor" href="#one-versus-all-classification"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>one-versus-all</strong> approach is an alternative procedure for applying SVMs one-versusin the case of <span class="math inline">\(K &gt; 2\)</span> classes. We fit <strong><span class="math inline">\(K\)</span></strong> SVMs, each time comparing one of all the <span class="math inline">\(K\)</span> classes to the remaining <span class="math inline">\(K-1\)</span> classes.</p>
</div>
</div>
</div>
<div id="lab-support-vector-machines" class="section level3" number="3.4.8">
<h3>
<span class="header-section-number">3.4.8</span> Lab: Support Vector Machines<a class="anchor" aria-label="anchor" href="#lab-support-vector-machines"><i class="fas fa-link"></i></a>
</h3>
<p>In the textbook “Introduction to statistical learning”, they use the <strong>e1071</strong> library in R to demonstrate the support vector classifier and the SVM.</p>
<p><strong>kernlab</strong> versus <strong>e1071</strong>
<a href="https://www.thekerneltrip.com/statistics/kernlab-vs-e1071/" class="uri">https://www.thekerneltrip.com/statistics/kernlab-vs-e1071/</a></p>
<div id="support-vector-classifier-1" class="section level4" number="3.4.8.1">
<h4>
<span class="header-section-number">3.4.8.1</span> Support Vector Classifier<a class="anchor" aria-label="anchor" href="#support-vector-classifier-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## generate the data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">x</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">20</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">y</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>,<span class="fl">10</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">x</span><span class="op">[</span><span class="va">y</span><span class="op">==</span><span class="fl">1</span>,<span class="op">]</span><span class="op">=</span><span class="va">x</span><span class="op">[</span><span class="va">y</span><span class="op">==</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>, col<span class="op">=</span><span class="op">(</span><span class="fl">3</span><span class="op">-</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-15-1.png" width="672"></div>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># combine x and y into a data frame</span></span>
<span><span class="va">dat</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">x</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## load the package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">e1071</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## fit the support vector classifier (kernel="linear")</span></span>
<span><span class="co"># A cost argument allows us to specify the cost of a violation to the margin. </span></span>
<span><span class="co"># When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.</span></span>
<span><span class="co"># The argument scale=FALSE tells the svm() function not to scale each feature to have mean zero or standard deviation one</span></span>
<span><span class="va">svmfit</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">dat</span>, kernel<span class="op">=</span><span class="st">"linear"</span>, cost<span class="op">=</span><span class="fl">10</span>,scale<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">svmfit</span>, <span class="va">dat</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-15-2.png" width="672"></div>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># index of support vectors</span></span>
<span><span class="va">svmfit</span><span class="op">$</span><span class="va">index</span></span>
<span><span class="co">#&gt; [1]  1  2  5  7 14 16 17</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">svmfit</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; svm(formula = y ~ ., data = dat, kernel = "linear", </span></span>
<span><span class="co">#&gt;     cost = 10, scale = FALSE)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Parameters:</span></span>
<span><span class="co">#&gt;    SVM-Type:  C-classification </span></span>
<span><span class="co">#&gt;  SVM-Kernel:  linear </span></span>
<span><span class="co">#&gt;        cost:  10 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Support Vectors:  7</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  ( 4 3 )</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Classes:  2 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Levels: </span></span>
<span><span class="co">#&gt;  -1 1</span></span>
<span></span>
<span><span class="co"># using a smaller cost yields a wider margin and more support vectors</span></span>
<span><span class="va">svmfit</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">dat</span>, kernel<span class="op">=</span><span class="st">"linear"</span>, cost<span class="op">=</span><span class="fl">0.1</span>,scale<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">svmfit</span>, <span class="va">dat</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-15-3.png" width="672"></div>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">svmfit</span><span class="op">$</span><span class="va">index</span></span>
<span><span class="co">#&gt;  [1]  1  2  3  4  5  7  9 10 12 13 14 15 16 17 18 20</span></span>
<span></span>
<span><span class="co"># The e1071 library includes a built-in function, tune(), to perform ten-fold cross-validation.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">tune.out</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/tune.html">tune</a></span><span class="op">(</span><span class="va">svm</span>, <span class="va">y</span><span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">dat</span>, kernel<span class="op">=</span><span class="st">"linear"</span>, ranges<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>cost<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">1</span>,<span class="fl">5</span>,<span class="fl">10</span>,<span class="fl">100</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">tune.out</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Parameter tuning of 'svm':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; - sampling method: 10-fold cross validation </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; - best parameters:</span></span>
<span><span class="co">#&gt;  cost</span></span>
<span><span class="co">#&gt;   0.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; - best performance: 0.05 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; - Detailed performance results:</span></span>
<span><span class="co">#&gt;    cost error dispersion</span></span>
<span><span class="co">#&gt; 1 1e-03  0.55  0.4377975</span></span>
<span><span class="co">#&gt; 2 1e-02  0.55  0.4377975</span></span>
<span><span class="co">#&gt; 3 1e-01  0.05  0.1581139</span></span>
<span><span class="co">#&gt; 4 1e+00  0.15  0.2415229</span></span>
<span><span class="co">#&gt; 5 5e+00  0.15  0.2415229</span></span>
<span><span class="co">#&gt; 6 1e+01  0.15  0.2415229</span></span>
<span><span class="co">#&gt; 7 1e+02  0.15  0.2415229</span></span>
<span><span class="co"># choose the best model with the lowest cross-validation error rate</span></span>
<span><span class="va">bestmod</span><span class="op">=</span><span class="va">tune.out</span><span class="op">$</span><span class="va">best.model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">bestmod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; best.tune(METHOD = svm, train.x = y ~ ., data = dat, </span></span>
<span><span class="co">#&gt;     ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, </span></span>
<span><span class="co">#&gt;         10, 100)), kernel = "linear")</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Parameters:</span></span>
<span><span class="co">#&gt;    SVM-Type:  C-classification </span></span>
<span><span class="co">#&gt;  SVM-Kernel:  linear </span></span>
<span><span class="co">#&gt;        cost:  0.1 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Support Vectors:  16</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  ( 8 8 )</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Classes:  2 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Levels: </span></span>
<span><span class="co">#&gt;  -1 1</span></span>
<span></span>
<span><span class="co"># generate a test data set.</span></span>
<span><span class="va">xtest</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">20</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">ytest</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>, <span class="fl">20</span>, rep<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">xtest</span><span class="op">[</span><span class="va">ytest</span><span class="op">==</span><span class="fl">1</span>,<span class="op">]</span><span class="op">=</span><span class="va">xtest</span><span class="op">[</span><span class="va">ytest</span><span class="op">==</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="va">testdat</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">xtest</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">ytest</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The predict() function can be used to predict the class label on a set of test observations</span></span>
<span><span class="va">ypred</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">bestmod</span>,<span class="va">testdat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>predict<span class="op">=</span><span class="va">ypred</span>, truth<span class="op">=</span><span class="va">testdat</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="co">#&gt;        truth</span></span>
<span><span class="co">#&gt; predict -1 1</span></span>
<span><span class="co">#&gt;      -1  9 1</span></span>
<span><span class="co">#&gt;      1   2 8</span></span>
<span></span>
<span><span class="co"># try another model with cost 0.01</span></span>
<span><span class="va">svmfit</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">dat</span>, kernel<span class="op">=</span><span class="st">"linear"</span>, cost<span class="op">=</span><span class="fl">.01</span>,scale<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">ypred</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">svmfit</span>,<span class="va">testdat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>predict<span class="op">=</span><span class="va">ypred</span>, truth<span class="op">=</span><span class="va">testdat</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="co">#&gt;        truth</span></span>
<span><span class="co">#&gt; predict -1  1</span></span>
<span><span class="co">#&gt;      -1 11  6</span></span>
<span><span class="co">#&gt;      1   0  3</span></span></code></pre></div>
</div>
<div id="support-vector-machine-1" class="section level4" number="3.4.8.2">
<h4>
<span class="header-section-number">3.4.8.2</span> Support Vector Machine<a class="anchor" aria-label="anchor" href="#support-vector-machine-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## generate some data with a non-linear class boundary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">x</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">200</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span>,<span class="op">]</span><span class="op">=</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span>,<span class="op">]</span><span class="op">+</span><span class="fl">2</span></span>
<span><span class="va">x</span><span class="op">[</span><span class="fl">101</span><span class="op">:</span><span class="fl">150</span>,<span class="op">]</span><span class="op">=</span><span class="va">x</span><span class="op">[</span><span class="fl">101</span><span class="op">:</span><span class="fl">150</span>,<span class="op">]</span><span class="op">-</span><span class="fl">2</span></span>
<span><span class="va">y</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">150</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">50</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dat</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">x</span>,y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>, col<span class="op">=</span><span class="va">y</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-16-1.png" width="672"></div>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co">## split the dataset into train and test datasets</span></span>
<span><span class="va">train</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">200</span>,<span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## fit the svm model</span></span>
<span><span class="va">svmfit</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">dat</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span>, kernel<span class="op">=</span><span class="st">"radial"</span>,  gamma<span class="op">=</span><span class="fl">1</span>, cost<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">svmfit</span>, <span class="va">dat</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-16-2.png" width="672"></div>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">svmfit</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; svm(formula = y ~ ., data = dat[train, ], kernel = "radial", </span></span>
<span><span class="co">#&gt;     gamma = 1, cost = 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Parameters:</span></span>
<span><span class="co">#&gt;    SVM-Type:  C-classification </span></span>
<span><span class="co">#&gt;  SVM-Kernel:  radial </span></span>
<span><span class="co">#&gt;        cost:  1 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Support Vectors:  31</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  ( 16 15 )</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Classes:  2 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Levels: </span></span>
<span><span class="co">#&gt;  1 2</span></span>
<span></span>
<span><span class="va">svmfit</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">dat</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span>, kernel<span class="op">=</span><span class="st">"radial"</span>,gamma<span class="op">=</span><span class="fl">1</span>,cost<span class="op">=</span><span class="fl">1e5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">svmfit</span>,<span class="va">dat</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-classification_files/figure-html/unnamed-chunk-16-3.png" width="672"></div>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co"># tuning parameters: C and gamma</span></span>
<span><span class="va">tune.out</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/tune.html">tune</a></span><span class="op">(</span><span class="va">svm</span>, <span class="va">y</span><span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">dat</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span>, kernel<span class="op">=</span><span class="st">"radial"</span>, ranges<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>cost<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>,<span class="fl">1</span>,<span class="fl">10</span>,<span class="fl">100</span>,<span class="fl">1000</span><span class="op">)</span>,gamma<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>,<span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">tune.out</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Parameter tuning of 'svm':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; - sampling method: 10-fold cross validation </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; - best parameters:</span></span>
<span><span class="co">#&gt;  cost gamma</span></span>
<span><span class="co">#&gt;     1   0.5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; - best performance: 0.07 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; - Detailed performance results:</span></span>
<span><span class="co">#&gt;     cost gamma error dispersion</span></span>
<span><span class="co">#&gt; 1  1e-01   0.5  0.26 0.15776213</span></span>
<span><span class="co">#&gt; 2  1e+00   0.5  0.07 0.08232726</span></span>
<span><span class="co">#&gt; 3  1e+01   0.5  0.07 0.08232726</span></span>
<span><span class="co">#&gt; 4  1e+02   0.5  0.14 0.15055453</span></span>
<span><span class="co">#&gt; 5  1e+03   0.5  0.11 0.07378648</span></span>
<span><span class="co">#&gt; 6  1e-01   1.0  0.22 0.16193277</span></span>
<span><span class="co">#&gt; 7  1e+00   1.0  0.07 0.08232726</span></span>
<span><span class="co">#&gt; 8  1e+01   1.0  0.09 0.07378648</span></span>
<span><span class="co">#&gt; 9  1e+02   1.0  0.12 0.12292726</span></span>
<span><span class="co">#&gt; 10 1e+03   1.0  0.11 0.11005049</span></span>
<span><span class="co">#&gt; 11 1e-01   2.0  0.27 0.15670212</span></span>
<span><span class="co">#&gt; 12 1e+00   2.0  0.07 0.08232726</span></span>
<span><span class="co">#&gt; 13 1e+01   2.0  0.11 0.07378648</span></span>
<span><span class="co">#&gt; 14 1e+02   2.0  0.12 0.13165612</span></span>
<span><span class="co">#&gt; 15 1e+03   2.0  0.16 0.13498971</span></span>
<span><span class="co">#&gt; 16 1e-01   3.0  0.27 0.15670212</span></span>
<span><span class="co">#&gt; 17 1e+00   3.0  0.07 0.08232726</span></span>
<span><span class="co">#&gt; 18 1e+01   3.0  0.08 0.07888106</span></span>
<span><span class="co">#&gt; 19 1e+02   3.0  0.13 0.14181365</span></span>
<span><span class="co">#&gt; 20 1e+03   3.0  0.15 0.13540064</span></span>
<span><span class="co">#&gt; 21 1e-01   4.0  0.27 0.15670212</span></span>
<span><span class="co">#&gt; 22 1e+00   4.0  0.07 0.08232726</span></span>
<span><span class="co">#&gt; 23 1e+01   4.0  0.09 0.07378648</span></span>
<span><span class="co">#&gt; 24 1e+02   4.0  0.13 0.14181365</span></span>
<span><span class="co">#&gt; 25 1e+03   4.0  0.15 0.13540064</span></span>
<span></span>
<span><span class="co">## see the performance of the best svm model selected by tune()</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>true<span class="op">=</span><span class="va">dat</span><span class="op">[</span><span class="op">-</span><span class="va">train</span>,<span class="st">"y"</span><span class="op">]</span>, pred<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tune.out</span><span class="op">$</span><span class="va">best.model</span>,newx<span class="op">=</span><span class="va">dat</span><span class="op">[</span><span class="op">-</span><span class="va">train</span>,<span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;     pred</span></span>
<span><span class="co">#&gt; true  1  2</span></span>
<span><span class="co">#&gt;    1 54 23</span></span>
<span><span class="co">#&gt;    2 17  6</span></span></code></pre></div>
</div>
</div>
</div>
<div id="final-project-apply-the-svm-method-to-the-heart-disease-data" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Final Project (Apply the SVM method to the Heart Disease Data)<a class="anchor" aria-label="anchor" href="#final-project-apply-the-svm-method-to-the-heart-disease-data"><i class="fas fa-link"></i></a>
</h2>
<p>Download the Heart Disease Data (Heart.csv) from the website: <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/data.html" class="uri">http://faculty.marshall.usc.edu/gareth-james/ISL/data.html</a></p>
<p><strong>Data Description</strong>: These data contain a binary outcome HD (the last column in Heart.csv) for 303 patients who presented with chest pain. An outcome value of Yes indicates the presence of heart disease based on an angiographic test, while No means no heart disease. There are 13 predictors including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements.</p>
<p><strong>Objective</strong>: use 13 predictors such as Age, Sex, and Chol in order to predict whether an individual has heart disease.</p>
<p><strong>Software</strong>: R package “e1071”, …</p>
<p><strong>Requirements</strong>: please fit the SVM for the Heart Disease Data</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># load the Heart data</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="st">"dataset/Heart.csv"</span><span class="op">)</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="va">dat</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/na.fail.html">na.omit</a></span><span class="op">(</span><span class="va">dat</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#...</span></span></code></pre></div>
</div>
<div id="reference-2" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> Reference<a class="anchor" aria-label="anchor" href="#reference-2"><i class="fas fa-link"></i></a>
</h2>
<p>[1] Johnson, R. A., and Wichern, D. W. (2007). Applied multivariate statistical analysis (6th ed.). Upper Saddle River, N.J.: Pearson Prentice Hall.</p>
<p>[2] Zelterman, D. (2015). Applied Multivariate Statistics with R (1st ed.).</p>
<p>[3] Statistics 575: Multivariate Analysis, Douglas Wiens (<a href="http://www.mathstat.ualberta.ca/~wiens/stat575/stat575.html" class="uri">http://www.mathstat.ualberta.ca/~wiens/stat575/stat575.html</a>)</p>
<p>[4] James, G., Witten, D., Hastie, T., Tibshirani, R. (2013), An Introduction to Statistical Learning with Applications in R</p>
<p>[5] Some of the figures in this presentation are taken from “An Introduction to Statistical Learning, with applications in R” (Springer, 2013) with permission from the authors: G. James, D. Witten, T. Hastie and R. Tibshirani</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="chapter-9-factor-analysis.html"><span class="header-section-number">2</span> Chapter 9: Factor Analysis</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-10-discrimination-and-classification"><span class="header-section-number">3</span> Chapter 10: Discrimination and Classification</a></li>
<li><a class="nav-link" href="#an-introductory-example"><span class="header-section-number">3.1</span> 10.1 An Introductory Example</a></li>
<li>
<a class="nav-link" href="#multinomial-logistic-regression"><span class="header-section-number">3.2</span> 10.2 Multinomial Logistic Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#two-groups"><span class="header-section-number">3.2.1</span> Two groups</a></li>
<li><a class="nav-link" href="#more-than-two-groups"><span class="header-section-number">3.2.2</span> More than two groups</a></li>
<li><a class="nav-link" href="#graphical-diagnostics"><span class="header-section-number">3.2.3</span> Graphical diagnostics</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#linear-discriminant-analysis"><span class="header-section-number">3.3</span> 10.3 Linear Discriminant Analysis</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#wine-data-example"><span class="header-section-number">3.3.1</span> Wine data example</a></li>
<li><a class="nav-link" href="#methodology"><span class="header-section-number">3.3.2</span> Methodology</a></li>
<li><a class="nav-link" href="#an-illustrative-example-mixture-of-two-normal-distributions"><span class="header-section-number">3.3.3</span> An illustrative example (mixture of two normal distributions)</a></li>
<li><a class="nav-link" href="#quadratic-discriminant-analysis-qda"><span class="header-section-number">3.3.4</span> Quadratic Discriminant Analysis (QDA)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#support-vector-machine"><span class="header-section-number">3.4</span> 10.4 Support Vector Machine</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-linear-svm-classifier"><span class="header-section-number">3.4.1</span> Example (linear SVM classifier)</a></li>
<li><a class="nav-link" href="#example-kernel-svm-classifier"><span class="header-section-number">3.4.2</span> Example (kernel SVM classifier)</a></li>
<li><a class="nav-link" href="#cross-validation"><span class="header-section-number">3.4.3</span> Cross Validation</a></li>
<li><a class="nav-link" href="#application-to-the-wine-data"><span class="header-section-number">3.4.4</span> Application to the wine data</a></li>
<li><a class="nav-link" href="#maximal-margin-classifier"><span class="header-section-number">3.4.5</span> Maximal Margin Classifier</a></li>
<li><a class="nav-link" href="#support-vector-classifier"><span class="header-section-number">3.4.6</span> Support Vector Classifier</a></li>
<li><a class="nav-link" href="#support-vectot-machine"><span class="header-section-number">3.4.7</span> Support Vectot Machine</a></li>
<li><a class="nav-link" href="#lab-support-vector-machines"><span class="header-section-number">3.4.8</span> Lab: Support Vector Machines</a></li>
</ul>
</li>
<li><a class="nav-link" href="#final-project-apply-the-svm-method-to-the-heart-disease-data"><span class="header-section-number">3.5</span> Final Project (Apply the SVM method to the Heart Disease Data)</a></li>
<li><a class="nav-link" href="#reference-2"><span class="header-section-number">3.6</span> Reference</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/rstudio/bookdown-demo/blob/master/10-classification.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/rstudio/bookdown-demo/edit/master/10-classification.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Lecture notes for STA 6707 Multivariate Methods</strong>" was written by Fei Heng. It was last built on 2023-08-15.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
